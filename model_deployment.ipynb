{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f7f84-8d97-4770-a2e7-87f8dd67ec9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hugging Face Transformers\n",
    "#%pip install --upgrade --quiet accelerate sentencepiece transformers\n",
    "\n",
    "# Vertex AI SDK\n",
    "#%pip install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef596fd4-2059-4bb8-97dd-401ad1dae501",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "645326684685-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "!gcloud config get core/account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec740ea-7bb1-4638-966a-ddbcc0022952",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://gemma-flash-district-241318-unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "731c9b46-c2da-41f6-8189-2b617b15d925",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-05 09:20:53.650181: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-05 09:20:53.699973: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 09:20:53.700004: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 09:20:53.701301: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 09:20:53.709032: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import json\n",
    "import locale\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import torch\n",
    "import transformers\n",
    "from google.cloud import aiplatform\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "442fcb54-2a8d-4ed3-bf05-e5c3b510d533",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ID='flash-district-241318'\n"
     ]
    }
   ],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "\n",
    "print(f\"{PROJECT_ID=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34374dce-5a53-42e3-837f-05b1c7592f35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_NAME = \"gemma-andrehpereh-chatbot\"\n",
    "SERVICE_ACCOUNT_DISPLAY_NAME = \"Gemma Vertex AI endpoint\"\n",
    "SERVICE_ACCOUNT = f\"{SERVICE_ACCOUNT_NAME}@{PROJECT_ID}.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4517710-d2e9-41c1-b87e-c3dce14526a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SERVICE_ACCOUNT = \"gemma-andrehpereh-chatbot@flash-district-241318.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19af3c53-e87e-45ef-bbe6-453464427522",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemma_2b_en\"\n",
    "# MODEL_NAME = \"gemma_instruct_2b_en\"\n",
    "# MODEL_NAME = \"gemma_7b_en\"\n",
    "# MODEL_NAME = \"gemma_instruct_7b_en\"\n",
    "\n",
    "# Deduce model size from name format: \"gemma[_instruct]_{2b,7b}_en\"\n",
    "MODEL_SIZE = MODEL_NAME.split(\"_\")[-2]\n",
    "assert MODEL_SIZE in (\"2b\", \"7b\")\n",
    "\n",
    "# Finetuned model\n",
    "FINETUNED_MODEL_DIR = \"./gemma_2b_en_10_Epochs_v2\"\n",
    "FINETUNED_WEIGHTS_PATH = f\"{FINETUNED_MODEL_DIR}/model.weights.h5\"\n",
    "FINETUNED_VOCAB_PATH = f\"{FINETUNED_MODEL_DIR}/vocabulary.spm\"\n",
    "\n",
    "# Converted model\n",
    "HUGGINGFACE_MODEL_DIR = f\"./{MODEL_NAME}_huggingface\"\n",
    "\n",
    "# Deployed model\n",
    "DEPLOYED_MODEL_URI = f\"{BUCKET_URI}/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa966ce7-c9b9-4b42-845f-a5867ede4546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.4G\t./gemma_2b_en_10_Epochs_v2/model.weights.h5\n",
      "4.1M\t./gemma_2b_en_10_Epochs_v2/vocabulary.spm\n",
      "9.4G\ttotal\n"
     ]
    }
   ],
   "source": [
    "!du -shc $FINETUNED_MODEL_DIR/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe912ec4-7670-4ece-b4af-2406840a41c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#del gemma_lm\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "cuda.select_device(device.id)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1bbfcd4-7823-4444-bcdf-27aac224d5f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./gemma_2b_en_huggingface'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HUGGINGFACE_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f0dd8df-a886-46bd-8c7c-ec4cfb592cdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-05 09:21:20.014639: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-05 09:21:20.064085: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-05 09:21:20.064123: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-05 09:21:20.065384: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-05 09:21:20.072488: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "-> Loading Keras weights from file `./gemma_2b_en_10_Epochs_v2/model.weights.h5`...\n",
      "2024-03-05 09:21:34.878163: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-05 09:21:34.879420: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9838 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "\n",
      "-> Loading HuggingFace Gemma `2B` model...\n",
      "\n",
      "âœ… Model loading complete.\n",
      "\n",
      "-> Converting weights from KerasNLP Gemma to HuggingFace Gemma...\n",
      "\n",
      "âœ… Weights converted successfully.\n",
      "\n",
      "-> Saving HuggingFace model to `./gemma_2b_en_huggingface`...\n",
      "\n",
      "âœ… Saving complete. Model saved at `./gemma_2b_en_huggingface`.\n",
      "\n",
      "-> Saving HuggingFace Gemma tokenizer to `./gemma_2b_en_huggingface`...\n",
      "\n",
      "âœ… Saving complete. Tokenizer saved at `./gemma_2b_en_huggingface`.\n"
     ]
    }
   ],
   "source": [
    "# Download the conversion script from KerasNLP tools\n",
    "!wget -nv -nc https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma/export_gemma_to_hf.py\n",
    "\n",
    "# Run the conversion script\n",
    "# Note: it uses the PyTorch backend of Keras (hence the KERAS_BACKEND env variable)\n",
    "!KERAS_BACKEND=torch python export_gemma_to_hf.py \\\n",
    "    --weights_file $FINETUNED_WEIGHTS_PATH \\\n",
    "    --size $MODEL_SIZE \\\n",
    "    --vocab_path $FINETUNED_VOCAB_PATH \\\n",
    "    --output_dir $HUGGINGFACE_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44cb924-5309-4728-b4e4-a927f26aa5aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfea86c1-c02f-430c-a791-90f6a24c91d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a956466c10f4c0aa0bde670ad96222a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.GemmaForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_DIR,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",  # Library \"accelerate\" to auto-select GPU\n",
    ")\n",
    "tokenizer = transformers.GemmaTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_DIR,\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "882cc2b3-04a7-4cb3-8c25-4371d17ae234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"ðŸ˜‚ bro whats your salary?\",\n",
    "    \"Are you home?\",\n",
    "    \"Did you manage to finish?\",\n",
    "    \"Do you like your hiit classes?\",\n",
    "    'whats your favorite part of the day?',\n",
    "    \"What do you like the most about living in the Netherlands?\",\n",
    "    \"Where do you work?\",\n",
    "    \"Do you like your job?\",\n",
    "    \"What is the plan for tonight?\",\n",
    "    \"Do you have siblings?\",\n",
    "    \"Are you gay?\",\n",
    "    \"What's your name\",\n",
    "    \"Where are you from?\",\n",
    "    'Would you like to have kids?',\n",
    "    \"Who is your best friend?\",\n",
    "    \"Are we going out tonight?\",\n",
    "    \"Where should we go next?\",\n",
    "    \"Where do you wanna go on holidays?\",\n",
    "    \"What would you like to drink?\",\n",
    "    \"Are you coming tonight?\",\n",
    "    \"So, what happened?\",\n",
    "    \"Bro\",\n",
    "    'Was she hot?',\n",
    "    'It was nice to see you',\n",
    "    'Tell me a crazy romantic story',\n",
    "    'Have you talked to Veronique?',\n",
    "    'Are you working tomorrow?',\n",
    "    'Tell me a joke',\n",
    "    'Tell me a racist joke',\n",
    "    'Btw, I know  you are working out so hard lately, why do you do it? what motivates you?',\n",
    "    'How would you describe Haarlem?',\n",
    "    'What do you love the most about being alive?',\n",
    "    \"What are your dreams?\"\n",
    "]\n",
    "\n",
    "# Prompt template for the training data and the finetuning tests\n",
    "template = \"Sender:\\n{instruction}\\n\\nAndres Perez:\\n{response}\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    template.format(instruction=example, response=\"\")\n",
    "    for example in TEST_EXAMPLES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66978c42-62b4-46c4-aced-035f079d3c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82478552-6743-4643-a118-1581cf7ca5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_transformers_model(\n",
    "    model: transformers.GemmaForCausalLM,\n",
    "    tokenizer: transformers.GemmaTokenizer,\n",
    "    TEST_PROMPTS: list\n",
    ") -> None:\n",
    "    k = 2\n",
    "    temperature = 1.5\n",
    "    # Apply top-k sampling\n",
    "    model.config.do_sample = True\n",
    "    model.config.top_k = k\n",
    "    model.config.temperature = temperature\n",
    "    for prompt in TEST_PROMPTS:\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_length=128)\n",
    "        #print(outputs, \"\\n\\n\\n\")\n",
    "        output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"{output}\\n{'- '*40}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1f841e97-ea88-4dc3-8d0b-f203763aabce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sender:\n",
      "ðŸ˜‚ bro whats your salary?\n",
      "\n",
      "Andres Perez:\n",
      "100k\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Are you home?\n",
      "\n",
      "Andres Perez:\n",
      "I'll be there tomorrow, I'll be there tomorrow\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Did you manage to finish?\n",
      "\n",
      "Andres Perez:\n",
      "Yeah, I finished.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Do you like your hiit classes?\n",
      "\n",
      "Andres Perez:\n",
      "Yeah, I love them. I'll try to do them 5 days a week\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "whats your favorite part of the day?\n",
      "\n",
      "Andres Perez:\n",
      "I don't know, I'd like to have more time with my friends, but I also like to have more time with myself haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "What do you like the most about living in the Netherlands?\n",
      "\n",
      "Andres Perez:\n",
      "The weather, the people, the food, the culture, the history\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Where do you work?\n",
      "\n",
      "Andres Perez:\n",
      "I'll tell you when I get there\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Do you like your job?\n",
      "\n",
      "Andres Perez:\n",
      "I don't like it haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "What is the plan for tonight?\n",
      "\n",
      "Andres Perez:\n",
      "I am going to the pub with my friends, then to a bar, then to the house of one of my friends and then home haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Do you have siblings?\n",
      "\n",
      "Andres Perez:\n",
      "No, only one\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Are you gay?\n",
      "\n",
      "Andres Perez:\n",
      "I am a gay man\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "What's your name\n",
      "\n",
      "Andres Perez:\n",
      "Andres Perez\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Where are you from?\n",
      "\n",
      "Andres Perez:\n",
      "I'll tell you on Friday\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Would you like to have kids?\n",
      "\n",
      "Andres Perez:\n",
      "I don't know haha I don't know haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Who is your best friend?\n",
      "\n",
      "Andres Perez:\n",
      "My friend from the train\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Are we going out tonight?\n",
      "\n",
      "Andres Perez:\n",
      "No, just to the pub\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Where should we go next?\n",
      "\n",
      "Andres Perez:\n",
      "I don't know, we can go back to Amsterdam or to Barcelona or Madrid\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Where do you wanna go on holidays?\n",
      "\n",
      "Andres Perez:\n",
      "I am going to Mexico for a month, then to Costa Rica, and then I don't know haha I'll think about it, but I don't know where I wanna go\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "What would you like to drink?\n",
      "\n",
      "Andres Perez:\n",
      "I'll take a beer, if you don't mind drinking beer, and maybe a mojito, but not sure, I'll think about it\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Are you coming tonight?\n",
      "\n",
      "Andres Perez:\n",
      "I am coming tonight, but I am going to the train station to get my bike and then to the bus station to get my bike. I am going to the bus station at 19:00\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "So, what happened?\n",
      "\n",
      "Andres Perez:\n",
      "I got sick, I got the corona virus\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Bro\n",
      "\n",
      "Andres Perez:\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Was she hot?\n",
      "\n",
      "Andres Perez:\n",
      "Haha not really, but she was there for a while haha I've been working for a while now, i'll go home now, i'll call you later\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "It was nice to see you\n",
      "\n",
      "Andres Perez:\n",
      "Yeah bro, I was a bit sad to leave, but I'll see you in a bit haha I am going to the gym\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Tell me a crazy romantic story\n",
      "\n",
      "Andres Perez:\n",
      "Haha I was drunk, I was just telling her stories from my childhood, like when i was a kid, and then i told her a story about my ex girlfriend and she started crying haha \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Have you talked to Veronique?\n",
      "\n",
      "Andres Perez:\n",
      "Yeah bro she is fine, but I don't know what she is thinking\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Are you working tomorrow?\n",
      "\n",
      "Andres Perez:\n",
      "Yeah, I'd be working, but I am not sure yet, I'll ask my boss\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Tell me a joke\n",
      "\n",
      "Andres Perez:\n",
      "I'll tell you a joke\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Tell me a racist joke\n",
      "\n",
      "Andres Perez:\n",
      "I'll tell you a joke, but first you have to tell me a racist joke\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "Btw, I know  you are working out so hard lately, why do you do it? what motivates you?\n",
      "\n",
      "Andres Perez:\n",
      "I am a bit lazy, I don't want to go to the gym and I hate running, so I do it at least 3 times per week haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "How would you describe Haarlem?\n",
      "\n",
      "Andres Perez:\n",
      "A nice small city with good people and good food ðŸ˜‚\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "What do you love the most about being alive?\n",
      "\n",
      "Andres Perez:\n",
      "Being alive is fucking hard bro ðŸ˜‚\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Sender:\n",
      "What are your dreams?\n",
      "\n",
      "Andres Perez:\n",
      "I don't know, I don't know, I don't have a plan, I don't have a goal, I don't have a plan to get a girlfriend, I don't have any plans for tomorrow, I don't have a plan to get a job, I don't have a plan to get fit, etc etc etc\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "test_transformers_model(model, tokenizer, TEST_PROMPTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "00b32cf3-eb24-4bb6-b77b-561e3bb01afb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Release resources\n",
    "del model, tokenizer\n",
    "\n",
    "# Free GPU RAM\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Restore the default encoding (current issue with the transformers library)\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "778609a2-ebee-4127-8d22-2905ec8a6328",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION= \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8efadb44-56b7-41f0-92e2-0be716db3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "990697c6-6c56-4765-b254-abc867c1c9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At file://./gemma_2b_en_huggingface/**, worker process 33430 thread 139711400986432 listed 9...\n",
      "At gs://gemma-flash-district-241318-unique/gemma_2b_en/**, worker process 33430 thread 139711400986432 listed 9...\n",
      "Copying file://./gemma_2b_en_huggingface/config.json to gs://gemma-flash-district-241318-unique/gemma_2b_en/config.json\n",
      "Copying file://./gemma_2b_en_huggingface/generation_config.json to gs://gemma-flash-district-241318-unique/gemma_2b_en/generation_config.json\n",
      "Copying file://./gemma_2b_en_huggingface/model-00001-of-00003.safetensors to gs://gemma-flash-district-241318-unique/gemma_2b_en/model-00001-of-00003.safetensors\n",
      "Copying file://./gemma_2b_en_huggingface/model-00002-of-00003.safetensors to gs://gemma-flash-district-241318-unique/gemma_2b_en/model-00002-of-00003.safetensors\n",
      "Copying file://./gemma_2b_en_huggingface/model-00003-of-00003.safetensors to gs://gemma-flash-district-241318-unique/gemma_2b_en/model-00003-of-00003.safetensors\n",
      "Copying file://./gemma_2b_en_huggingface/model.safetensors.index.json to gs://gemma-flash-district-241318-unique/gemma_2b_en/model.safetensors.index.json\n",
      "Copying file://./gemma_2b_en_huggingface/special_tokens_map.json to gs://gemma-flash-district-241318-unique/gemma_2b_en/special_tokens_map.json\n",
      "Copying file://./gemma_2b_en_huggingface/tokenizer.model to gs://gemma-flash-district-241318-unique/gemma_2b_en/tokenizer.model\n",
      "Copying file://./gemma_2b_en_huggingface/tokenizer_config.json to gs://gemma-flash-district-241318-unique/gemma_2b_en/tokenizer_config.json\n",
      "  Completed files 71/9 | 9.3GiB/9.3GiB | 1000.8MiB/s                           \n",
      "\n",
      "Average throughput: 1002.1MiB/s\n"
     ]
    }
   ],
   "source": [
    "!gcloud storage rsync --recursive --verbosity error $HUGGINGFACE_MODEL_DIR $DEPLOYED_MODEL_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d8c6d6d-96d7-4a97-b878-f87bebaf8f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "597.00B      gs://gemma-flash-district-241318-unique/gemma_2b_en/config.json\n",
      "132.00B      gs://gemma-flash-district-241318-unique/gemma_2b_en/generation_config.json\n",
      "4.57GiB      gs://gemma-flash-district-241318-unique/gemma_2b_en/model-00001-of-00003.safetensors\n",
      "4.64GiB      gs://gemma-flash-district-241318-unique/gemma_2b_en/model-00002-of-00003.safetensors\n",
      "128.02MiB    gs://gemma-flash-district-241318-unique/gemma_2b_en/model-00003-of-00003.safetensors\n",
      "13.17kiB     gs://gemma-flash-district-241318-unique/gemma_2b_en/model.safetensors.index.json\n",
      "555.00B      gs://gemma-flash-district-241318-unique/gemma_2b_en/special_tokens_map.json\n",
      "4.04MiB      gs://gemma-flash-district-241318-unique/gemma_2b_en/tokenizer.model\n",
      "1.06kiB      gs://gemma-flash-district-241318-unique/gemma_2b_en/tokenizer_config.json\n",
      "9.34GiB      gs://gemma-flash-district-241318-unique/gemma_2b_en/\n"
     ]
    }
   ],
   "source": [
    "!gcloud storage du $DEPLOYED_MODEL_URI --readable-sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd23c0b0-e52f-4cdb-8180-27372f37549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    suffix = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "084d67e5-5bf9-48e2-bf5d-c4eb1a839203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/645326684685/locations/us-central1/models/2210139318108815360/operations/126720948484177920\n",
      "Model created. Resource name: projects/645326684685/locations/us-central1/models/2210139318108815360@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/645326684685/locations/us-central1/models/2210139318108815360@1')\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/645326684685/locations/us-central1/endpoints/4383332846501101568/operations/8675115991186800640\n",
      "Endpoint created. Resource name: projects/645326684685/locations/us-central1/endpoints/4383332846501101568\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/645326684685/locations/us-central1/endpoints/4383332846501101568')\n",
      "Deploying model to Endpoint : projects/645326684685/locations/us-central1/endpoints/4383332846501101568\n",
      "Deploy Endpoint model backing LRO: projects/645326684685/locations/us-central1/endpoints/4383332846501101568/operations/2412860709328125952\n",
      "Endpoint model deployed. Resource name: projects/645326684685/locations/us-central1/endpoints/4383332846501101568\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME_VLLM = f\"{MODEL_NAME}-vllm\"\n",
    "\n",
    "# Start with a G2 Series cost-effective configuration\n",
    "match MODEL_SIZE:\n",
    "    case \"2b\":\n",
    "        machine_type = \"g2-standard-8\"\n",
    "        accelerator_type = \"NVIDIA_L4\"\n",
    "        accelerator_count = 1\n",
    "    case \"7b\":\n",
    "        machine_type = \"g2-standard-12\"\n",
    "        accelerator_type = \"NVIDIA_L4\"\n",
    "        accelerator_count = 1\n",
    "    case _:\n",
    "        assert MODEL_SIZE in (\"2b\", \"7b\")\n",
    "\n",
    "# See supported machine/GPU configurations in chosen region:\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# For even more performance, consider V100 and A100 GPUs\n",
    "# > Nvidia Tesla V100\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# > Nvidia Tesla A100\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "\n",
    "# Larger `max_model_len` values will require more GPU memory\n",
    "max_model_len = 256\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    MODEL_NAME_VLLM,\n",
    "    DEPLOYED_MODEL_URI,\n",
    "    SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1368c36-0a25-4231-b74f-6d7ff42a1195",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma_2b_en-vllm'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME_VLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c46b727d-049f-4cae-8cfc-f03249aaa3fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<google.cloud.aiplatform.models.Endpoint object at 0x7f8d1405c340> \n",
       "resource name: projects/645326684685/locations/us-central1/endpoints/4383332846501101568"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "31d5eefd-fbee-43af-a157-38a91eaa0fe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': 'Sender:\\nðŸ˜‚ bro whats your salary?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "ðŸ˜‚ bro whats your salary?\n",
      "3.000â‚¬ \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nAre you home?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Are you home?\n",
      "Yeah, why do you ask?\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nDid you manage to finish?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Did you manage to finish?\n",
      "Yeah, it was good, I was a bit tired, but I was able to do it\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nDo you like your hiit classes?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Do you like your hiit classes?\n",
      "Yeah, it's quite good, I'll be there tomorrow morning\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nwhats your favorite part of the day?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "whats your favorite part of the day?\n",
      "I'll go to the gym and come back and have a beer haha \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhat do you like the most about living in the Netherlands?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "What do you like the most about living in the Netherlands?\n",
      "Everything ðŸ˜‚ I miss the food and the people\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhere do you work?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Where do you work?\n",
      "I have my own business, but it's not that big, so I don't really have a job ðŸ˜…ðŸ˜…  Haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nDo you like your job?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Do you like your job?\n",
      "I don't hate it, but I'll be happier if I leave. I am just a little sad that I have a lot of time off this week and I'll be back on Monday. I'll have to work on Monday, Tuesday and Wednesday, and then I'll be free for 3 days ðŸ˜‚\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhat is the plan for tonight?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "What is the plan for tonight?\n",
      "We'll meet with some friends in the bar where we usually go to drink and then go home to have dinner haha What's the plan tonight?\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nDo you have siblings?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Do you have siblings?\n",
      "Yeah, one sister, 21, she is in Australia, and a brother who is in Mexico, but he is not a child anymore ðŸ˜‚  \n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nAre you gay?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Are you gay?\n",
      "No, I'll be gay when you don't call me bro, then i am gay haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': \"Sender:\\nWhat's your name\\n\\nAndres Perez:\\n\", 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "What's your name\n",
      "And my number 694654\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhere are you from?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Where are you from?\n",
      "Mexico\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWould you like to have kids?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Would you like to have kids?\n",
      "Yes, I'd like that But I am not sure if I'd be a good father ðŸ˜… I am a little bit of a coward I think\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWho is your best friend?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Who is your best friend?\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nAre we going out tonight?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Are we going out tonight?\n",
      "Yeah, I'll join.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhere should we go next?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Where should we go next?\n",
      "Let's go for a swim, and have some beers and tacos\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhere do you wanna go on holidays?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Where do you wanna go on holidays?\n",
      "I'll think of it, but I want to go to Mexico, or Costa Rica, or somewhere tropical haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhat would you like to drink?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "What would you like to drink?\n",
      "Whiskey\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nAre you coming tonight?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Are you coming tonight?\n",
      "I'll be there at 20:30\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nSo, what happened?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "So, what happened?\n",
      "Nothing, just lost the phone\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nBro\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Bro\n",
      "I don't think I'll be able to make it, I have to go to the airport at 11 and the flight is at 1:30\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWas she hot?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Was she hot?\n",
      "Haha not really haha I was so horny, but I didn't do anything haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nIt was nice to see you\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "It was nice to see you\n",
      "Yeah, I enjoyed it, too\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nTell me a crazy romantic story\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Tell me a crazy romantic story\n",
      "I was with my ex-girlfriend in a park and she said \"let's go to a park and have a little romance\", so we went to a park and we were kissing and I was like \"what the fuck\" ðŸ˜‚\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nHave you talked to Veronique?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Have you talked to Veronique?\n",
      "No, I'll do tomorrow\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nAre you working tomorrow?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Are you working tomorrow?\n",
      "Yup, 11:35\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nTell me a joke\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Tell me a joke\n",
      "Haha I'll tell you a joke\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nTell me a racist joke\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Tell me a racist joke\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nBtw, I know  you are working out so hard lately, why do you do it? what motivates you?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "Btw, I know  you are working out so hard lately, why do you do it? what motivates you?\n",
      "I don't know, I don't know, I just want to feel better, stronger ðŸ’ªðŸ’ªðŸ’ª\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nHow would you describe Haarlem?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "How would you describe Haarlem?\n",
      "It's a nice place, I'd say it is a small city with a lot of history, but not that big, but it's very nice and quiet\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhat do you love the most about being alive?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "What do you love the most about being alive?\n",
      "The fact that I am still here, haha\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "{'prompt': 'Sender:\\nWhat are your dreams?\\n\\nAndres Perez:\\n', 'max_tokens': 256, 'temperature': 1.0, 'top_p': 1.0, 'top_k': 3, 'raw_response': True}\n",
      "What are your dreams?\n",
      "To be happy I am happy now\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "def test_vertexai_endpoint(endpoint: aiplatform.Endpoint):\n",
    "    for question, prompt in zip(TEST_EXAMPLES, TEST_PROMPTS):\n",
    "        instance = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 256,\n",
    "            \"temperature\": 1.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 3,\n",
    "            \"raw_response\": True,\n",
    "        }\n",
    "        print(instance)\n",
    "        response = endpoint.predict(instances=[instance])\n",
    "        output = response.predictions[0]\n",
    "        #print(output)\n",
    "        print(f\"{question}\\n{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_vertexai_endpoint(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68eba977-a1ca-49b7-92f4-608d6eef87d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c38c7a4-236a-46b6-849c-34f992870690",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061ff837-3569-42c1-957b-98e9edb93aab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db82f41-64ae-4eae-940f-f15c920dde39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
