{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f178cfef-2782-4a77-b4f8-7d372a7c61f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 16:57:38.716483: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-15 16:57:38.776618: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import keras_nlp\n",
    "import os\n",
    "from util import get_model_paths_and_config, upload2bs\n",
    "from config import Config\n",
    "from components.data_preparation.data_ingestion import process_whatsapp_chat\n",
    "from components.fine_tunning.trainer import finetune_gemma\n",
    "from components.fine_tunning.conversion_function import convert_checkpoints\n",
    "from numba import cuda\n",
    "from google.cloud import aiplatform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d508aa69-1810-4fc2-9a3d-735e55e4ccf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_paths_and_config = get_model_paths_and_config(Config.MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69027026-fb4f-4de3-9e5c-84f7ec881d29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input_data/andrehpereh', 'able-analyst-416817-chatbot-v1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.TRAIN_DATA_DIR, Config.BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b737395-4d42-4a7e-becf-9fbc07a2580d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Bucket: able-analyst-416817-chatbot-v1>\n",
      "input_data/andrehpereh\n",
      "<google.api_core.page_iterator.HTTPIterator object at 0x7f1dac862560>\n",
      "input_data/andrehpereh/\n",
      "\n",
      "input_data/andrehpereh/WhatsApp Chat with Anki.txt\n",
      "WhatsApp Chat with Anki.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Anki.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Ilse Flatmate.txt\n",
      "WhatsApp Chat with Ilse Flatmate.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Ilse Flatmate.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Michael.txt\n",
      "WhatsApp Chat with Michael.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Michael.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Mike Haarlem.txt\n",
      "WhatsApp Chat with Mike Haarlem.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Mike Haarlem.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Rosa Rosa Rosa.txt\n",
      "WhatsApp Chat with Rosa Rosa Rosa.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Rosa Rosa Rosa.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Ruben Ewald Puijker.txt\n",
      "WhatsApp Chat with Ruben Ewald Puijker.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Ruben Ewald Puijker.txt\n"
     ]
    }
   ],
   "source": [
    "data = process_whatsapp_chat(Config.BUCKET_NAME, Config.TRAIN_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04ebef-6bad-4c2d-aced-34083491afce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67913c94-353d-43f3-8026-827aafdbbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_weights_path = finetune_gemma(data=data[:50], model_paths=model_paths_and_config, model_name=Config.MODEL_NAME, rank_lora=Config.SEQUENCE_LENGTH, sequence_length=Config.SEQUENCE_LENGTH, epochs=Config.EPOCHS, batch_size=Config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d2d38-4f7b-4eca-bf72-31027d060ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = cuda.get_current_device()\n",
    "cuda.select_device(device.id)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c2ed7-d7fc-4c1b-a73f-e44940701a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = convert_checkpoints(\n",
    "    weights_file=finetuned_weights_path,\n",
    "    size=model_paths_and_config['model_size'],\n",
    "    output_dir=model_paths_and_config['huggingface_model_dir'],\n",
    "    vocab_path=model_paths_and_config['finetuned_vocab_path'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202aae46-9fdc-40f6-9d3e-d4f7cb50278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=model_paths_and_config['huggingface_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad8c09-c59c-4424-8165-24e5716740e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = upload2bs(local_directory = output_dir, bucket_name = Config.BUCKET_NAME, destination_subfolder = model_paths_and_config['deployed_model_blob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea5a28-8c3c-44d7-87aa-1952de6c3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=Config.PROJECT_ID, location=Config.REGION, staging_bucket=Config.BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61b32639-f0ac-459a-924b-db590770a837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    suffix = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c07dfde0-7192-48af-9730-d3b692cf878a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma_2b_en-vllm'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up artificially since the model was already in a bucket\n",
    "model_paths_and_config['deployed_model_uri'] = 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240314162107'\n",
    "model_paths_and_config['model_name_vllm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "251fc433-3657-434e-a64c-6b351bf143ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/24796876098/locations/us-central1/models/6563293868962349056/operations/1798728489633841152\n",
      "Model created. Resource name: projects/24796876098/locations/us-central1/models/6563293868962349056@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/24796876098/locations/us-central1/models/6563293868962349056@1')\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/24796876098/locations/us-central1/endpoints/2459943961893011456/operations/8683465682488131584\n",
      "Endpoint created. Resource name: projects/24796876098/locations/us-central1/endpoints/2459943961893011456\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/24796876098/locations/us-central1/endpoints/2459943961893011456')\n",
      "Deploying model to Endpoint : projects/24796876098/locations/us-central1/endpoints/2459943961893011456\n",
      "Deploy Endpoint model backing LRO: projects/24796876098/locations/us-central1/endpoints/2459943961893011456/operations/7799071305663250432\n",
      "Endpoint model deployed. Resource name: projects/24796876098/locations/us-central1/endpoints/2459943961893011456\n"
     ]
    }
   ],
   "source": [
    "max_model_len = 2048\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=model_paths_and_config['model_name_vllm'],\n",
    "    model_uri=model_paths_and_config['deployed_model_uri'],\n",
    "    service_account=Config.SERVICE_ACCOUNT,\n",
    "    machine_type=model_paths_and_config['machine_type'],\n",
    "    accelerator_type=model_paths_and_config['accelerator_type'],\n",
    "    accelerator_count=model_paths_and_config['accelerator_count'],\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69dc40ff-d622-423b-8c43-f066e93907e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88520f0-174a-4cb7-97c4-41a1fbcaca81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b0e2a926-dac3-4dfd-a184-515685b4b657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f6f329e-9925-42fe-8362-3ca5c97f1cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the plan for tonight?\n",
      "I don’t know, I’m not sure. I’ll let you know when I know.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What would you like to drink?\n",
      "I’ll take a coffee, please.\n",
      "\n",
      "Andres Perez:\n",
      "I’ll take a coffee, please.\n",
      "\n",
      "Andres Perez:\n",
      "I’ll take a coffee, please.\n",
      "\n",
      "Andres Perez:\n",
      "I’ll take a coffee, please.\n",
      "\n",
      "Andres Perez:\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Are you coming tonight?\n",
      "I’m not sure. I’ll ask my mom.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"What is the plan for tonight?\",\n",
    "    \"What would you like to drink?\",\n",
    "    \"Are you coming tonight?\"\n",
    "]\n",
    "\n",
    "# Prompt template for the training data and the finetuning tests\n",
    "PROMPT_TEMPLATE = \"Sender:\\n{instruction}\\n\\nAndres Perez:\\n{response}\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    PROMPT_TEMPLATE.format(instruction=example, response=\"\")\n",
    "    for example in TEST_EXAMPLES\n",
    "]\n",
    "\n",
    "def test_vertexai_endpoint(endpoint: aiplatform.Endpoint):\n",
    "    for question, prompt in zip(TEST_EXAMPLES, TEST_PROMPTS):\n",
    "        instance = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 56,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 1,\n",
    "            \"raw_response\": True,\n",
    "        }\n",
    "        response = endpoint.predict(instances=[instance])\n",
    "        output = response.predictions[0]\n",
    "        print(f\"{question}\\n{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_vertexai_endpoint(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "518bd694-aa5b-45f4-934a-894c33b2bf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "SERVICE_ACCOUNT = 'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'\n",
    "from datetime import datetime\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "GCP_REGION='us-central1'\n",
    "IMAGE_NAME=\"gemma-chatbot\"\n",
    "TAG_NAME = 'latest'\n",
    "KAGGLE_USERNAME='andrehpereh1'\n",
    "KAGGLE_KEY='5859e39806d9456749dcbac685f04bc9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60d98f42-6a85-4324-81a7-dd1c6b5bab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-data-preparation,TAG_NAME=latest\n"
     ]
    }
   ],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-data-preparation\",\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfd38dea-1f7f-486d-bf3e-1b3a1f69582c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Runs the data_preparation component image. (Development, when tested should be moved to the main cloudbuild in the project folder)\n",
    "# Pay attention to the \".\" after summit. Might need some changes when move to the master pipeline.\n",
    "#!gcloud builds submit . --timeout=15m --config \"components/data_preparation/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "# DO not forget the tag\n",
    "#!docker run gcr.io/able-analyst-416817/gemma-chatbot-data-preparation:latest data_ingestion.py --bucket-name 'able-analyst-416817-chatbot-v1' --directory 'input_data/andrehpereh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a0d0f14-b406-4a5a-bc6b-fa4e8da5cc62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250f9bf-5bf0-4bd2-8f5f-3f10daf9dc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8ef60c94-45b0-4cf2-a7d1-cf96b3f6ad6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-fine-tunning,_KAGGLE_USERNAME=andrehpereh1,_KAGGLE_KEY=5859e39806d9456749dcbac685f04bc9,TAG_NAME=latest\n",
      "2024-03-17 15:01:48.885597: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-17 15:01:48.944732: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-17 15:01:50.087724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "3\n",
      "trainer.py <class 'str'>\n",
      "[\"Sender: FoooodddAndres Perez: Coming :)\", \"Sender: Can I maybe borrow your iron? Andres Perez: It's not my iron But yeah haha Or is it?\"] <class 'str'>\n",
      "{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"} <class 'str'>\n",
      "This is type param1 <class 'list'>\n",
      "<class 'dict'>\n",
      "Traceback (most recent call last):\n",
      "  File \"/trainer/trainer.py\", line 50, in <module>\n",
      "    experimental_fnc(data=param1, model_paths=param2)\n",
      "NameError: name 'experimental_fnc' is not defined\n"
     ]
    }
   ],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_KAGGLE_USERNAME={},\\\n",
    "_KAGGLE_KEY={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-fine-tunning\",\n",
    "           KAGGLE_USERNAME,\n",
    "           KAGGLE_KEY,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "\n",
    "\n",
    "# Builds image\n",
    "!gcloud builds submit . --config \"components/fine_tunning/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "\n",
    "\n",
    "data = '\"[\\\\\"Sender: FoooodddAndres Perez: Coming :)\\\\\", \\\\\"Sender: Can I maybe borrow your iron? Andres Perez: It\\'s not my iron But yeah haha Or is it?\\\\\"]\"'\n",
    "model_paths = \"\"\"{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"}\"\"\"\n",
    "model_paths_json = json.dumps(model_paths)\n",
    "\n",
    "# Runs the model trainning component image.  (Development, when tested should be moved to the main cloudbuild in the project folder)\n",
    "\n",
    "!docker run gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest {data} {model_paths_json}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d2d84c44-c65c-4ae1-9a4c-6d293f139495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"[\\\\\"Sender: FoooodddAndres Perez: Coming :)\\\\\", \\\\\"Sender: Can I maybe borrow your iron? Andres Perez: It\\'s not my iron But yeah haha Or is it?\\\\\"]\"'"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e6404c6c-d998-4253-bbf3-3f83b29abe94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-experimental,TAG_NAME=latest\n",
      "3\n",
      "experimental.py <class 'str'>\n",
      "\"[\\\"Sender: FoooodddAndres Perez: Coming :)\\\", \\\"Sender: Can I maybe borrow your iron? Andres Perez: It's not my iron But yeah haha Or is it?\\\"]\" <class 'str'>\n",
      "{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"} <class 'str'>\n",
      "This is type param1 <class 'str'>\n",
      "<class 'dict'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "Jalo todo bien\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-experimental\",\n",
    "           TAG_NAME,\n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "data = '\"[\\\\\"Sender: FoooodddAndres Perez: Coming :)\\\\\", \\\\\"Sender: Can I maybe borrow your iron? Andres Perez: It\\'s not my iron But yeah haha Or is it?\\\\\"]\"'\n",
    "model_paths = \"\"\"{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"}\"\"\"\n",
    "data_json = json.dumps(data)\n",
    "model_paths_json = json.dumps(model_paths)\n",
    "#!gcloud builds submit . --timeout=15m --config \"components/experimental/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "!docker run gcr.io/able-analyst-416817/gemma-chatbot-experimental:latest experimental.py {data_json} {model_paths_json}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff6c518-79e1-4ec6-a832-39ae6f364ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-fine-tunning,_GCP_REGION=us-central1,TAG_NAME=latest\n"
     ]
    }
   ],
   "source": [
    "#Re-runs the image to restart the website, service account might be needed with this one.  (Development, when tested should be moved to the main cloudbuild in the project folder)\n",
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_GCP_REGION={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-running-app\",\n",
    "           GCP_REGION,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "!gcloud builds submit . --timeout=15m --config \"components/app_flask/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "!gcloud builds submit . --timeout=15m --config \"cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1cae7-c300-4cd1-aa97-c7fef2efaa5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05aa3f-9a40-41db-8553-b65d7dee4352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_GCP_REGION={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-running-app\",\n",
    "           GCP_REGION,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "!gcloud builds submit . --timeout=15m --config \"components/app_flask/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "!gcloud builds submit . --timeout=15m --config \"cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb10f3c-b135-470b-a74b-0c825e0f0761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0aa1979-e2f1-41ce-a3f4-bab8465866e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/24796876098/locations/us-central1/pipelineJobs/whatsapp-chat-20240317213054\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/24796876098/locations/us-central1/pipelineJobs/whatsapp-chat-20240317213054')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/whatsapp-chat-20240317213054?project=24796876098\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/whatsapp-chat-20240317213054 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/whatsapp-chat-20240317213054 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/whatsapp-chat-20240317213054 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/whatsapp-chat-20240317213054 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/24796876098/locations/us-central1/pipelineJobs/whatsapp-chat-20240317213054\n"
     ]
    }
   ],
   "source": [
    "import kfp.dsl as dsl\n",
    "from typing import List\n",
    "from kfp import compiler\n",
    "\n",
    "@dsl.component(\n",
    "  base_image ='gcr.io/able-analyst-416817/gemma-chatbot-data-preparation:latest'\n",
    ")\n",
    "def process_whatsapp_chat_op(\n",
    "  bucket_name: str,\n",
    "  directory: str,\n",
    "  output_path: dsl.OutputPath('Dataset')\n",
    "):\n",
    "    print(\"Inside process_whatsapp_chat_op:\")\n",
    "    print(f\"Received bucket_name: {bucket_name}\")\n",
    "    print(f\"Received directory: {directory}\")\n",
    "    import data_ingestion\n",
    "    formatted_messages = data_ingestion.process_whatsapp_chat(bucket_name, directory)\n",
    "    output_path = \"/tmp/formatted_messages.txt\"\n",
    "    with open(output_path, 'w') as f:\n",
    "        for item in formatted_messages:\n",
    "            f.write(f\"{item}\\n\")\n",
    "\n",
    "\n",
    "@dsl.component\n",
    "def consume_dataset(dataset: dsl.InputPath('Dataset')):\n",
    "    print(dataset)\n",
    "\n",
    "@dsl.pipeline(name=\"whatsapp-chat\")\n",
    "def pipeline(\n",
    "    bucket_name: str,\n",
    "    directory: str\n",
    "):\n",
    "    create_dataset_op = process_whatsapp_chat_op(\n",
    "        bucket_name=bucket_name,\n",
    "        directory=directory\n",
    "    )\n",
    "    consume_dataset(dataset=create_dataset_op.outputs['output_path'])\n",
    "    \n",
    "    \n",
    "    model_train_evaluate_op = gcc_aip.CustomContainerTrainingJobRunOp(\n",
    "        # Vertex AI Python SDK authentication parameters.     \n",
    "        project=project,\n",
    "        location=location,\n",
    "        staging_bucket=staging_bucket,\n",
    "        display_name=display_name,  # Added from pipeline definition\n",
    "        container_uri=container_uri,\n",
    "        model_serving_container_image_uri=model_serving_container_image_uri,\n",
    "\n",
    "        # WorkerPool arguments.\n",
    "        replica_count=1,\n",
    "        machine_type=\"e2-standard-4\",\n",
    "\n",
    "        # Additional Arguments \n",
    "        base_output_dir=base_output_dir \n",
    "        # ... other arguments specific to your training code\n",
    "    )\n",
    "    \n",
    "    \n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pipeline, package_path=\"test-whatsapp.json\"\n",
    ")\n",
    "\n",
    "from google.cloud import aiplatform as vertexai\n",
    "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
    "    display_name=\"test-whatsapp\",\n",
    "    template_path=\"test-whatsapp.json\",\n",
    "    parameter_values={\n",
    "        \"bucket_name\": \"able-analyst-416817-chatbot-v1\",\n",
    "        \"directory\": \"input_data/andrehpereh\"\n",
    "    },\n",
    "    enable_caching=True\n",
    ")\n",
    "vertex_pipelines_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53b974a3-cab2-425e-bfc5-dcb01d45b5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11402c19-d1b8-4b8c-a959-932ca09fcf18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5cf7ac36-bc0e-4e00-928f-1c7247b9685e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09398956-e97e-411b-a1fe-9e05ff61e6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdfdea-c57b-4a26-ac5f-2775ff291bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed69c43-8407-460b-bd2d-1ac0ed39a610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m118"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
