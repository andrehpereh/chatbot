{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/chatbot')\n",
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "os.environ['PROJECT_ID'] = PROJECT_ID\n",
    "project_id = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, ChatSession\n",
    "\n",
    "project_id = project_id\n",
    "location = \"us-central1\"\n",
    "vertexai.init(project=project_id, location=location)\n",
    "model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
    "chat = model.start_chat()\n",
    "\n",
    "def get_chat_response(chat: ChatSession, prompt: str) -> str:\n",
    "    text_response = []\n",
    "    responses = chat.send_message(prompt, stream=True)\n",
    "    for chunk in responses:\n",
    "        text_response.append(chunk.text)\n",
    "    return \"\".join(text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = \"You like computers of the same sex, gay!\"\n",
    "# print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/jupyter/chatbot/components/app_flask/app/app.py', 'r') as file_content:\n",
    "    content = file_content.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
      "from google.cloud import bigquery, storage, pubsub_v1\n",
      "from werkzeug.datastructures import FileStorage\n",
      "import bcrypt, os, base64, json\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)  # Use the function's module name\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "app = Flask(__name__)\n",
      "app.secret_key = os.urandom(24)\n",
      "\n",
      "# Copyright 2020 Google LLC\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "# [START aiplatform_predict_custom_trained_model_sample]\n",
      "from typing import Dict, List, Union\n",
      "from google.cloud import aiplatform\n",
      "from google.protobuf import json_format\n",
      "from google.protobuf.struct_pb2 import Value\n",
      "import re\n",
      "\n",
      "# Connect to BigQuery\n",
      "print(\"This is the project Id from environment\", os.environ.get('PROJECT_ID'))\n",
      "DATASET_ID = 'chatbot'\n",
      "USERS_TABLE = 'users'\n",
      "USER_TRAINING_STATUS = 'user_training_status'\n",
      "BUCKET_NAME = \"personalize-chatbots-v1\"\n",
      "print(BUCKET_NAME)\n",
      "PUBSUB_TOPIC = 'your-pipeline-trigger-topic'\n",
      "\n",
      "def predict_custom_trained_model_sample(\n",
      "    project: str,\n",
      "    endpoint_id: str,\n",
      "    location: str = \"us-central1\",\n",
      "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
      "    user_input: str = input\n",
      "):\n",
      "    logger.debug(\"Function predict_custom_trained_model_sample started\")\n",
      "    \"\"\"\n",
      "    `instances` can be either single instance of type dict or a list\n",
      "    of instances.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        prompt_input = f\"Sender:\\n{user_input}\\n\\nAndres Perez:\\n\"\n",
      "        # The two below should be a parameter.\n",
      "        conversation_track = session.get('conversation_track')[-3:]\n",
      "        # conversation_track_keeper = conversation_track\n",
      "        print(\"These are the last two values\", conversation_track)\n",
      "        conversation_track_str = \"\\n\\n\".join(conversation_track  + [prompt_input])\n",
      "        print(\"This is the joined input for prediction\")\n",
      "        print(conversation_track_str)\n",
      "        instances={'prompt': conversation_track_str, 'max_tokens': 1024, 'temperature': 1, 'top_p': 0.7, 'top_k': 6}\n",
      "\n",
      "        # The AI Platform services require regional API endpoints.\n",
      "        client_options = {\"api_endpoint\": api_endpoint}\n",
      "        # Initialize client that will be used to create and send requests.\n",
      "        # This client only needs to be created once, and can be reused for multiple requests.\n",
      "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
      "        # The format of each instance should conform to the deployed model's prediction input schema.\n",
      "        instances = instances if isinstance(instances, list) else [instances]\n",
      "        instances = [\n",
      "            json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
      "        ]\n",
      "        parameters_dict = {}\n",
      "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
      "        endpoint = client.endpoint_path(\n",
      "            project=project, location=location, endpoint=endpoint_id\n",
      "        )\n",
      "\n",
      "        response = client.predict(\n",
      "            endpoint=endpoint, instances=instances, parameters=parameters\n",
      "        )\n",
      "        # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
      "        predictions = response.predictions\n",
      "        pattern = r\"Perez:\\nOutput:\\n(.*)\"\n",
      "        match = re.search(pattern, predictions[0])\n",
      "\n",
      "        if match:\n",
      "            logger.info(f\"Successful prediction: {match.group(1)}\")\n",
      "            print(conversation_track)\n",
      "            print(\"This is the last one\")\n",
      "            print(prompt_input + str(match.group(1)))\n",
      "            conversation_track.append(prompt_input + str(match.group(1)))\n",
      "            print(\"This is the 3 tracker\", conversation_track)\n",
      "            session['conversation_track'] = conversation_track\n",
      "            return match.group(1)\n",
      "        else:\n",
      "            logger.error(\"Prediction not found in the response.\")\n",
      "            return \"Error: Prediction not found in the response.\"\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.exception(f\"An error occurred: {e}\")\n",
      "        raise  # Re-raise to allow for error handling at a higher level\n",
      "    \n",
      "def extract_info_from_endpoint(url):\n",
      "    \"\"\"Extracts location, endpoint, and project information from a given AIPlatform URL.\n",
      "\n",
      "    Args:\n",
      "        url: The AIPlatform URL string.\n",
      "\n",
      "    Returns:\n",
      "        A dictionary containing the extracted values:\n",
      "            locations: The region.\n",
      "            endpoints: The endpoint ID.\n",
      "            projects: The project ID.\n",
      "    \"\"\"\n",
      "\n",
      "    pattern = r\"\\/projects\\/([^\\/]+)\\/locations\\/([^\\/]+)\\/endpoints\\/([^\\/]+)\\/operations\\/([^\\/]+)\"\n",
      "    print(pattern)\n",
      "    match = re.search(pattern, url)\n",
      "    print(match)\n",
      "    if match:\n",
      "        return {\n",
      "            \"projects\": match.group(1),\n",
      "            \"locations\": match.group(2),\n",
      "            \"endpoints\": match.group(3)\n",
      "        }\n",
      "    else:\n",
      "        return None  # Or you could raise an exception if the URL is invalid\n",
      "\n",
      "@app.route('/signup', methods=['POST'])\n",
      "def signup():\n",
      "    if request.method == 'POST':\n",
      "        email = request.form['email']\n",
      "        password = request.form['password']\n",
      "        confirm_password = request.form['confirm_password']\n",
      "\n",
      "        # Data validation\n",
      "        error_message = None\n",
      "        if not email or not password or not confirm_password:\n",
      "            error_message = 'Please fill in all fields.'\n",
      "        elif password != confirm_password:\n",
      "            error_message = 'Passwords do not match.'\n",
      "        # Add more validation rules as needed (e.g., email format, password strength)\n",
      "\n",
      "        if error_message:\n",
      "            return error_message, 400\n",
      "\n",
      "        # Hash the password for security\n",
      "        hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n",
      "        \n",
      "        client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "        table_ref = client.dataset(DATASET_ID).table(USERS_TABLE)\n",
      "        table = client.get_table(table_ref)\n",
      "        row_to_insert = {\n",
      "            'email': email, \n",
      "            'password_hash': hashed_password.decode('utf-8')\n",
      "        }\n",
      "        client.insert_rows(table, [row_to_insert]) \n",
      "        errors = client.insert_rows(table, [row_to_insert]) \n",
      "        if errors:  # Check if there were errors\n",
      "            return 'Error submitting data: {}'.format(errors), 500\n",
      "        else:\n",
      "            # session['user_id'] = user_id  # Assuming you fetched the user's ID\n",
      "            session['email'] = email \n",
      "            return redirect(url_for('upload'))\n",
      "\n",
      "@app.route('/login', methods=['POST'])\n",
      "def login():\n",
      "    print(\"Si entro en este pedo\")\n",
      "    if request.method == 'POST':\n",
      "        email = request.form['email']\n",
      "        print(\"Si entro en este email\", email)\n",
      "        password = request.form['password']\n",
      "        print(\"Si entro en este password\", password)\n",
      "\n",
      "        # Fetch user data from BigQuery\n",
      "        client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "\n",
      "        # Check passwords.\n",
      "        query = f\"SELECT password_hash FROM `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USERS_TABLE}` WHERE email = '{email}'\"\n",
      "        print(\"This is el query...\", query)\n",
      "        results = client.query(query).result()\n",
      "        print(\"This is the results\", results, type(results))\n",
      "        stored_password_hash = None\n",
      "        for row in results:\n",
      "            print(\"This is each row\", row)\n",
      "            stored_password_hash = row.password_hash  # Assuming 'password' is the column name\n",
      "        # Check if user already trained.\n",
      "        query = f\"\"\"\n",
      "            SELECT training_status, end_point\n",
      "            FROM `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USER_TRAINING_STATUS}`\n",
      "            WHERE email = '{email}'\n",
      "            ORDER BY created_at DESC\n",
      "            LIMIT 1\n",
      "        \"\"\"\n",
      "        print(\"This is el query...\", query)\n",
      "        results = client.query(query).result()\n",
      "        print(\"This is the results\", results, type(results))\n",
      "        user_training_status = None\n",
      "        endpoint_uri = None\n",
      "        for row in results:\n",
      "            print(\"This is each row\", row)\n",
      "            user_training_status = row.training_status  # Assuming 'training_status' is the column name\n",
      "            endpoint_uri = row.end_point\n",
      "        # Verify password\n",
      "        if stored_password_hash and bcrypt.checkpw(password.encode('utf-8'), stored_password_hash.encode('utf-8')):\n",
      "            session['email'] = email\n",
      "            if user_training_status:\n",
      "                endpoint_details = extract_info_from_endpoint(endpoint_uri)\n",
      "                print(\"This is the email\", email)\n",
      "                session['endpoint'] = endpoint_details[\"endpoints\"]\n",
      "                session['location'] = endpoint_details[\"locations\"]\n",
      "                session['project'] = endpoint_details[\"projects\"]\n",
      "                print(f\"This is the endpoint{session['endpoint']}, projects{session['project']}, locations{session['location']}\")\n",
      "                print(user_training_status)\n",
      "                return redirect(url_for('chat_page'))\n",
      "            else:\n",
      "                return redirect(url_for('upload'))\n",
      "            # Successful login\n",
      "        else:\n",
      "            # Invalid credentials\n",
      "            return 'Invalid email or password', 401\n",
      "\n",
      "@app.route('/upload')\n",
      "def upload():\n",
      "    email = session.get('email')\n",
      "    print(\"This is the email, ahuevito\", email)\n",
      "    if not email:  \n",
      "        # Redirect to login if not logged in\n",
      "        print(\"Nos regresamos al home\")\n",
      "        return redirect(url_for('home'))\n",
      "    print(\"Aqui llegaaaa\")\n",
      "    return render_template('upload.html')\n",
      "\n",
      "@app.route('/handle_upload', methods=['POST'])\n",
      "def handle_upload():\n",
      "    files_metadata = []\n",
      "    email = session.get('email')\n",
      "    print(\"This is the email, ahuevito\", email)\n",
      "    print(type(email))\n",
      "    code_version = request.form.get('code_version')\n",
      "    print(\"This is the code version\", code_version)\n",
      "    model_name = request.form.get('model_name')\n",
      "    epochs = request.form.get('epochs')\n",
      "    print(f\"Selected model: {model_name}, Epochs: {epochs}\")\n",
      "    user_name = re.match(r'^([^@]+)', str(email)).group(1)\n",
      "    print(\"This is the code version\", code_version)\n",
      "\n",
      "    publisher = pubsub_v1.PublisherClient()\n",
      "    topic_path = publisher.topic_path(os.environ.get('PROJECT_ID'), PUBSUB_TOPIC)\n",
      "    print(\"This is the topic path\", topic_path)\n",
      "    blob_folder = os.path.join(user_name, 'input_data')\n",
      "\n",
      "    if not email:\n",
      "        print(\"Nos regresamos al home\")\n",
      "        # Redirect to login if not logged in\n",
      "        return redirect(url_for('home'))\n",
      "    print(\"Creo que si jalo, python\")\n",
      "    for file in request.files.getlist('files'):\n",
      "        print(file)\n",
      "        print(\"This is the type\", type(file))\n",
      "        client = storage.Client(os.environ.get('PROJECT_ID'))\n",
      "        bucket = client.get_bucket(BUCKET_NAME)\n",
      "        blob_string = os.path.join(blob_folder, file.filename)\n",
      "        blob = bucket.blob(blob_string)\n",
      "        blob.upload_from_string(FileStorage(file).stream.read())\n",
      "        print(\"Uploading\", file.filename, blob.name)\n",
      "        files_metadata.append({\n",
      "            \"file_path\": f\"gs://{BUCKET_NAME}/{blob_string}\",\n",
      "            \"filename\": file.filename  # Add filename to metadata\n",
      "        })\n",
      "\n",
      "    # After all uploads are complete, prepare the message\n",
      "    message_data = {\n",
      "        \"user_name\": user_name,\n",
      "        \"files\": files_metadata,\n",
      "        \"blob_folder\": blob_folder,\n",
      "        \"model_name\": model_name,\n",
      "        \"epochs\": epochs,\n",
      "        \"bucket_name\": BUCKET_NAME,\n",
      "        \"tag_version\": code_version,\n",
      "        \"project_id\": os.environ.get('PROJECT_ID')\n",
      "        \n",
      "    }\n",
      "    message_data_json = json.dumps(message_data)\n",
      "    message_data_bytes = message_data_json.encode('utf-8')\n",
      "    print(message_data_bytes)\n",
      "    publisher.publish(topic_path, message_data_bytes)\n",
      "    \n",
      "    client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "    table_ref = client.dataset(DATASET_ID).table(USER_TRAINING_STATUS)\n",
      "    table = client.get_table(table_ref)\n",
      "    row_to_insert = {\n",
      "        'email': email,\n",
      "        'training_status': False\n",
      "    }\n",
      "    print(\"This is the rows to upload\", row_to_insert)\n",
      "    client.insert_rows(table, [row_to_insert]) \n",
      "    errors = client.insert_rows(table, [row_to_insert]) \n",
      "    if errors:  # Check if there were errors\n",
      "        return 'Error submitting data: {}'.format(errors), 500\n",
      "    else:\n",
      "        print(\"Upload Successful!\")\n",
      "        return \"Upload Successful!\", 200 \n",
      "     \n",
      "\n",
      "@app.route('/home')\n",
      "def home():\n",
      "    return render_template('index.html')\n",
      "\n",
      "\n",
      "@app.route('/chat_page')\n",
      "def chat_page():\n",
      "    email = session.get('email')\n",
      "    if not email:  \n",
      "        # Redirect to login if not logged in\n",
      "        print(\"Nos regresamos al home\")\n",
      "        return redirect(url_for('home'))\n",
      "    session['conversation_track'] = []\n",
      "    return render_template('chat.html')\n",
      "\n",
      "@app.route('/send_message', methods=['POST'])\n",
      "def send_message():\n",
      "    user_message = request.json['message']\n",
      "    logger.debug(f\"Received user message: {user_message}\")\n",
      "    email = session.get('email')\n",
      "    if not email:  \n",
      "        logger.info(\"User not logged in. Redirecting to home\")\n",
      "        return redirect(url_for('home'))\n",
      "\n",
      "    endpoint = session.get('endpoint')\n",
      "    project = session.get('project')\n",
      "    location = session.get('location')\n",
      "    logger.debug(f\"Session data: endpoint={endpoint}, project={project}, location={location}\")\n",
      "    try:\n",
      "        response = predict_custom_trained_model_sample(\n",
      "            project=project,\n",
      "            endpoint_id=endpoint,\n",
      "            location=location,\n",
      "            user_input= user_message,\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.exception(f\"Error in chatbot prediction: {e}\")\n",
      "        response = \"An error occurred. Please try again later.\"\n",
      "    # logger.debug(f\"Returning JSON response: {'message': response}\")\n",
      "    return jsonify({'message': response})\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    print(\"Pues si empezo a correr esta madre\")\n",
      "    app.run(host='0.0.0.0', port=5000, debug=True) \n"
     ]
    }
   ],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prompt_unitest = 'Create unit test for the following code, include everything and ensure it compiles.'\n",
    "res_unit_test = get_chat_response(chat, pre_prompt_unitest  + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Unit Tests for Chatbot Flask Application\n",
      "\n",
      "Due to the dependencies on external services like BigQuery and AI Platform, we will focus on testing the logic within the Flask app itself. We'll use the `unittest` framework and mock external calls.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "from unittest.mock import patch\n",
      "from flask import session\n",
      "import main  # Assuming your Flask app code is in main.py\n",
      "\n",
      "class TestChatbotApp(unittest.TestCase):\n",
      "\n",
      "    def setUp(self):\n",
      "        self.app = main.app.test_client()\n",
      "        self.app.testing = True\n",
      "\n",
      "    def test_home_route(self):\n",
      "        response = self.app.get('/home')\n",
      "        self.assertEqual(response.status_code, 200)\n",
      "        self.assertIn(b'Welcome to the Chatbot', response.data)\n",
      "\n",
      "    def test_chat_page_logged_out(self):\n",
      "        response = self.app.get('/chat_page')\n",
      "        self.assertEqual(response.status_code, 302)  # Redirect to login\n",
      "        self.assertEqual(response.location, 'http://localhost/home') \n",
      "\n",
      "    @patch('main.predict_custom_trained_model_sample')\n",
      "    def test_send_message_logged_in(self, mock_predict):\n",
      "        mock_predict.return_value = \"Mock response from model\"\n",
      "        with self.app.session_transaction() as sess:\n",
      "            sess['email'] = 'test@example.com'\n",
      "            sess['endpoint'] = 'mock_endpoint'\n",
      "            sess['project'] = 'mock_project'\n",
      "            sess['location'] = 'mock_location'\n",
      "            sess['conversation_track'] = []\n",
      "        response = self.app.post('/send_message', json={'message': 'Hello'})\n",
      "        self.assertEqual(response.status_code, 200)\n",
      "        self.assertEqual(response.json, {'message': 'Mock response from model'})\n",
      "\n",
      "    def test_send_message_logged_out(self):\n",
      "        response = self.app.post('/send_message', json={'message': 'Hello'})\n",
      "        self.assertEqual(response.status_code, 302)  # Redirect to login\n",
      "        self.assertEqual(response.location, 'http://localhost/home') \n",
      "\n",
      "    def test_extract_info_from_endpoint_valid(self):\n",
      "        url = \"/projects/test-project/locations/us-central1/endpoints/mock_endpoint/operations/12345\"\n",
      "        result = main.extract_info_from_endpoint(url)\n",
      "        self.assertEqual(result['projects'], 'test-project')\n",
      "        self.assertEqual(result['locations'], 'us-central1')\n",
      "        self.assertEqual(result['endpoints'], 'mock_endpoint')\n",
      "\n",
      "    def test_extract_info_from_endpoint_invalid(self):\n",
      "        url = \"invalid_url\"\n",
      "        result = main.extract_info_from_endpoint(url)\n",
      "        self.assertIsNone(result)\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "```\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "*   **setUp:** Creates a test client for the Flask app and sets testing mode.\n",
      "*   **test\\_home\\_route:** Tests if the home route returns a 200 status code and the expected content.\n",
      "*   **test\\_chat\\_page\\_logged\\_out:** Tests if accessing the chat page without being logged in redirects to the home page.\n",
      "*   **test\\_send\\_message\\_logged\\_in:** Mocks the `predict_custom_trained_model_sample` function and tests sending a message while logged in.\n",
      "*   **test\\_send\\_message\\_logged\\_out:** Tests if sending a message without being logged in redirects to the home page.\n",
      "*   **test\\_extract\\_info\\_from\\_endpoint\\_valid:** Tests extracting information from a valid AI Platform endpoint URL. \n",
      "*   **test\\_extract\\_info\\_from\\_endpoint\\_invalid:** Tests the case of an invalid URL. \n",
      "\n",
      "**Running the Tests:**\n",
      "\n",
      "1.  Save the above code as a Python file (e.g., `test_chatbot.py`) in the same directory as your `main.py`. \n",
      "2.  Run the tests from the command line using:\n",
      "\n",
      "```bash\n",
      "python -m unittest test_chatbot.py\n",
      "```\n",
      "\n",
      "**Remember:** This is a basic example. You should expand the tests to cover more scenarios and edge cases, including signup, login, file upload, and error handling.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res_unit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_prompt = 'In the following code, only replace the print statements for useful logs and ensure that the code still compiles. The most important thing is to keep having a compiling file. Do not add any explanations, only add useful logs and improve documentation. \\n\\n'\n",
    "res = get_chat_response(chat, pre_prompt  + content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
      "from google.cloud import bigquery, storage, pubsub_v1\n",
      "from werkzeug.datastructures import FileStorage\n",
      "import bcrypt, os, base64, json\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)  \n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "app = Flask(__name__)\n",
      "app.secret_key = os.urandom(24)\n",
      "\n",
      "# Copyright 2020 Google LLC\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "# [START aiplatform_predict_custom_trained_model_sample]\n",
      "from typing import Dict, List, Union\n",
      "from google.cloud import aiplatform\n",
      "from google.protobuf import json_format\n",
      "from google.protobuf.struct_pb2 import Value\n",
      "import re\n",
      "\n",
      "# Connect to BigQuery\n",
      "logger.debug(f\"Project ID from environment: {os.environ.get('PROJECT_ID')}\")\n",
      "DATASET_ID = 'chatbot'\n",
      "USERS_TABLE = 'users'\n",
      "USER_TRAINING_STATUS = 'user_training_status'\n",
      "BUCKET_NAME = \"personalize-chatbots-v1\"\n",
      "logger.debug(f\"Using bucket: {BUCKET_NAME}\")\n",
      "PUBSUB_TOPIC = 'your-pipeline-trigger-topic'\n",
      "\n",
      "def predict_custom_trained_model_sample(\n",
      "    project: str,\n",
      "    endpoint_id: str,\n",
      "    location: str = \"us-central1\",\n",
      "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
      "    user_input: str = input\n",
      "):\n",
      "    \"\"\"Predicts text using a custom-trained Vertex AI model.\n",
      "\n",
      "    Args:\n",
      "        project: The Google Cloud project ID.\n",
      "        endpoint_id: The ID of the Vertex AI endpoint.\n",
      "        location: The region where the endpoint is located.\n",
      "        api_endpoint: The API endpoint of Vertex AI.\n",
      "        user_input: The user's input text.\n",
      "\n",
      "    Returns:\n",
      "        The predicted text from the model.\n",
      "    \"\"\"\n",
      "\n",
      "    logger.debug(\"Function predict_custom_trained_model_sample started\")\n",
      "    \n",
      "    try:\n",
      "        prompt_input = f\"Sender:\\n{user_input}\\n\\nAndres Perez:\\n\"\n",
      "        conversation_track = session.get('conversation_track')[-3:]\n",
      "        logger.debug(f\"Last 3 conversation track items: {conversation_track}\")\n",
      "        conversation_track_str = \"\\n\\n\".join(conversation_track  + [prompt_input])\n",
      "        logger.debug(f\"Joined input for prediction: {conversation_track_str}\")\n",
      "        instances={'prompt': conversation_track_str, 'max_tokens': 1024, 'temperature': 1, 'top_p': 0.7, 'top_k': 6}\n",
      "\n",
      "        client_options = {\"api_endpoint\": api_endpoint}\n",
      "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
      "        instances = instances if isinstance(instances, list) else [instances]\n",
      "        instances = [\n",
      "            json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
      "        ]\n",
      "        parameters_dict = {}\n",
      "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
      "        endpoint = client.endpoint_path(\n",
      "            project=project, location=location, endpoint=endpoint_id\n",
      "        )\n",
      "\n",
      "        response = client.predict(\n",
      "            endpoint=endpoint, instances=instances, parameters=parameters\n",
      "        )\n",
      "        predictions = response.predictions\n",
      "        pattern = r\"Perez:\\nOutput:\\n(.*)\"\n",
      "        match = re.search(pattern, predictions[0])\n",
      "\n",
      "        if match:\n",
      "            logger.info(f\"Successful prediction: {match.group(1)}\")\n",
      "            conversation_track.append(prompt_input + str(match.group(1)))\n",
      "            logger.debug(f\"Updated conversation track: {conversation_track}\")\n",
      "            session['conversation_track'] = conversation_track\n",
      "            return match.group(1)\n",
      "        else:\n",
      "            logger.error(\"Prediction not found in the response.\")\n",
      "            return \"Error: Prediction not found in the response.\"\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.exception(f\"An error occurred: {e}\")\n",
      "        raise  # Re-raise to allow for error handling at a higher level\n",
      "    \n",
      "def extract_info_from_endpoint(url):\n",
      "    \"\"\"Extracts location, endpoint, and project information from a given Vertex AI endpoint URL.\n",
      "\n",
      "    Args:\n",
      "        url: The Vertex AI endpoint URL string.\n",
      "\n",
      "    Returns:\n",
      "        A dictionary containing the extracted values:\n",
      "            locations: The region.\n",
      "            endpoints: The endpoint ID.\n",
      "            projects: The project ID.\n",
      "    \"\"\"\n",
      "\n",
      "    pattern = r\"\\/projects\\/([^\\/]+)\\/locations\\/([^\\/]+)\\/endpoints\\/([^\\/]+)\\/operations\\/([^\\/]+)\"\n",
      "    logger.debug(f\"Using regex pattern: {pattern}\")\n",
      "    match = re.search(pattern, url)\n",
      "    logger.debug(f\"Regex match result: {match}\")\n",
      "    if match:\n",
      "        return {\n",
      "            \"projects\": match.group(1),\n",
      "            \"locations\": match.group(2),\n",
      "            \"endpoints\": match.group(3)\n",
      "        }\n",
      "    else:\n",
      "        return None  \n",
      "\n",
      "@app.route('/signup', methods=['POST'])\n",
      "def signup():\n",
      "    \"\"\"Handles user signup requests.\"\"\"\n",
      "    \n",
      "    if request.method == 'POST':\n",
      "        email = request.form['email']\n",
      "        password = request.form['password']\n",
      "        confirm_password = request.form['confirm_password']\n",
      "\n",
      "        error_message = None\n",
      "        if not email or not password or not confirm_password:\n",
      "            error_message = 'Please fill in all fields.'\n",
      "        elif password != confirm_password:\n",
      "            error_message = 'Passwords do not match.'\n",
      "\n",
      "        if error_message:\n",
      "            return error_message, 400\n",
      "\n",
      "        hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n",
      "        \n",
      "        client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "        table_ref = client.dataset(DATASET_ID).table(USERS_TABLE)\n",
      "        table = client.get_table(table_ref)\n",
      "        row_to_insert = {\n",
      "            'email': email, \n",
      "            'password_hash': hashed_password.decode('utf-8')\n",
      "        }\n",
      "        client.insert_rows(table, [row_to_insert]) \n",
      "        errors = client.insert_rows(table, [row_to_insert]) \n",
      "        if errors:  \n",
      "            logger.error(f\"Error submitting data to BigQuery: {errors}\")\n",
      "            return 'Error submitting data: {}'.format(errors), 500\n",
      "        else:\n",
      "            logger.info(f\"User signup successful: {email}\")\n",
      "            session['email'] = email \n",
      "            return redirect(url_for('upload'))\n",
      "\n",
      "@app.route('/login', methods=['POST'])\n",
      "def login():\n",
      "    \"\"\"Handles user login requests.\"\"\"\n",
      "    \n",
      "    logger.debug(\"Login endpoint called\") \n",
      "    if request.method == 'POST':\n",
      "        email = request.form['email']\n",
      "        logger.debug(f\"Login attempt for email: {email}\") \n",
      "        password = request.form['password']\n",
      "\n",
      "        client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "\n",
      "        query = f\"SELECT password_hash FROM `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USERS_TABLE}` WHERE email = '{email}'\"\n",
      "        logger.debug(f\"Executing BigQuery password query: {query}\") \n",
      "        results = client.query(query).result()\n",
      "        logger.debug(f\"BigQuery password query results: {results}\") \n",
      "        stored_password_hash = None\n",
      "        for row in results:\n",
      "            logger.debug(f\"Retrieved password hash from BigQuery: {row}\") \n",
      "            stored_password_hash = row.password_hash  \n",
      "        query = f\"\"\"\n",
      "            SELECT training_status, end_point\n",
      "            FROM `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USER_TRAINING_STATUS}`\n",
      "            WHERE email = '{email}'\n",
      "            ORDER BY created_at DESC\n",
      "            LIMIT 1\n",
      "        \"\"\"\n",
      "        logger.debug(f\"Executing BigQuery training status query: {query}\") \n",
      "        results = client.query(query).result()\n",
      "        logger.debug(f\"BigQuery training status query results: {results}\") \n",
      "        user_training_status = None\n",
      "        endpoint_uri = None\n",
      "        for row in results:\n",
      "            logger.debug(f\"Retrieved training status from BigQuery: {row}\") \n",
      "            user_training_status = row.training_status  \n",
      "            endpoint_uri = row.end_point\n",
      "        if stored_password_hash and bcrypt.checkpw(password.encode('utf-8'), stored_password_hash.encode('utf-8')):\n",
      "            session['email'] = email\n",
      "            if user_training_status:\n",
      "                endpoint_details = extract_info_from_endpoint(endpoint_uri)\n",
      "                logger.debug(f\"Setting session variables for endpoint: {endpoint_details}\") \n",
      "                session['endpoint'] = endpoint_details[\"endpoints\"]\n",
      "                session['location'] = endpoint_details[\"locations\"]\n",
      "                session['project'] = endpoint_details[\"projects\"]\n",
      "                logger.info(f\"User logged in with existing training: {email}\") \n",
      "                return redirect(url_for('chat_page'))\n",
      "            else:\n",
      "                logger.info(f\"User logged in without training: {email}\") \n",
      "                return redirect(url_for('upload'))\n",
      "        else:\n",
      "            logger.warning(f\"Invalid login attempt for email: {email}\") \n",
      "            return 'Invalid email or password', 401\n",
      "\n",
      "@app.route('/upload')\n",
      "def upload():\n",
      "    \"\"\"Renders the file upload page for users to provide training data.\"\"\"\n",
      "\n",
      "    email = session.get('email')\n",
      "    logger.debug(f\"Upload page accessed by email: {email}\") \n",
      "    if not email:  \n",
      "        logger.info(\"Redirecting to home page due to missing email in session\") \n",
      "        return redirect(url_for('home'))\n",
      "    logger.debug(\"Rendering upload template\") \n",
      "    return render_template('upload.html')\n",
      "\n",
      "@app.route('/handle_upload', methods=['POST'])\n",
      "def handle_upload():\n",
      "    \"\"\"Handles file upload and triggers the training pipeline.\"\"\"\n",
      "\n",
      "    files_metadata = []\n",
      "    email = session.get('email')\n",
      "    logger.debug(f\"Handling file upload for email: {email}\")\n",
      "    code_version = request.form.get('code_version')\n",
      "    logger.debug(f\"Code version for training: {code_version}\")\n",
      "    model_name = request.form.get('model_name')\n",
      "    epochs = request.form.get('epochs')\n",
      "    logger.debug(f\"Selected model: {model_name}, Epochs: {epochs}\")\n",
      "    user_name = re.match(r'^([^@]+)', str(email)).group(1)\n",
      "\n",
      "    publisher = pubsub_v1.PublisherClient()\n",
      "    topic_path = publisher.topic_path(os.environ.get('PROJECT_ID'), PUBSUB_TOPIC)\n",
      "    logger.debug(f\"Publishing message to Pub/Sub topic: {topic_path}\") \n",
      "    blob_folder = os.path.join(user_name, 'input_data')\n",
      "\n",
      "    if not email:\n",
      "        logger.info(\"Redirecting to home page due to missing email in session\") \n",
      "        return redirect(url_for('home'))\n",
      "    for file in request.files.getlist('files'):\n",
      "        logger.debug(f\"Processing uploaded file: {file.filename}\")\n",
      "        client = storage.Client(os.environ.get('PROJECT_ID'))\n",
      "        bucket = client.get_bucket(BUCKET_NAME)\n",
      "        blob_string = os.path.join(blob_folder, file.filename)\n",
      "        blob = bucket.blob(blob_string)\n",
      "        blob.upload_from_string(FileStorage(file).stream.read())\n",
      "        logger.info(f\"Uploaded file to Cloud Storage: {blob.name}\") \n",
      "        files_metadata.append({\n",
      "            \"file_path\": f\"gs://{BUCKET_NAME}/{blob_string}\",\n",
      "            \"filename\": file.filename  \n",
      "        })\n",
      "\n",
      "    message_data = {\n",
      "        \"user_name\": user_name,\n",
      "        \"files\": files_metadata,\n",
      "        \"blob_folder\": blob_folder,\n",
      "        \"model_name\": model_name,\n",
      "        \"epochs\": epochs,\n",
      "        \"bucket_name\": BUCKET_NAME,\n",
      "        \"tag_version\": code_version,\n",
      "        \"project_id\": os.environ.get('PROJECT_ID')\n",
      "        \n",
      "    }\n",
      "    message_data_json = json.dumps(message_data)\n",
      "    message_data_bytes = message_data_json.encode('utf-8')\n",
      "    logger.debug(f\"Publishing message data to Pub/Sub: {message_data_bytes}\") \n",
      "    publisher.publish(topic_path, message_data_bytes)\n",
      "    \n",
      "    client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "    table_ref = client.dataset(DATASET_ID).table(USER_TRAINING_STATUS)\n",
      "    table = client.get_table(table_ref)\n",
      "    row_to_insert = {\n",
      "        'email': email,\n",
      "        'training_status': False\n",
      "    }\n",
      "    logger.debug(f\"Inserting training status into BigQuery: {row_to_insert}\") \n",
      "    client.insert_rows(table, [row_to_insert]) \n",
      "    errors = client.insert_rows(table, [row_to_insert]) \n",
      "    if errors:  \n",
      "        logger.error(f\"Error submitting data to BigQuery: {errors}\") \n",
      "        return 'Error submitting data: {}'.format(errors), 500\n",
      "    else:\n",
      "        logger.info(\"File upload and training initiation successful!\") \n",
      "        return \"Upload Successful!\", 200 \n",
      "     \n",
      "\n",
      "@app.route('/home')\n",
      "def home():\n",
      "    \"\"\"Renders the home page of the application.\"\"\"\n",
      "    logger.debug(\"Rendering home page\") \n",
      "    return render_template('index.html')\n",
      "\n",
      "\n",
      "@app.route('/chat_page')\n",
      "def chat_page():\n",
      "    \"\"\"Renders the chat page where users interact with the chatbot.\"\"\"\n",
      "    \n",
      "    email = session.get('email')\n",
      "    if not email:  \n",
      "        logger.info(\"Redirecting to home page due to missing email in session\") \n",
      "        return redirect(url_for('home'))\n",
      "    logger.debug(\"Initializing conversation track and rendering chat page\") \n",
      "    session['conversation_track'] = []\n",
      "    return render_template('chat.html')\n",
      "\n",
      "@app.route('/send_message', methods=['POST'])\n",
      "def send_message():\n",
      "    \"\"\"Handles sending user messages to the chatbot and returning responses.\"\"\"\n",
      "    \n",
      "    user_message = request.json['message']\n",
      "    logger.debug(f\"Received user message: {user_message}\")\n",
      "    email = session.get('email')\n",
      "    if not email:  \n",
      "        logger.info(\"User not logged in. Redirecting to home\")\n",
      "        return redirect(url_for('home'))\n",
      "\n",
      "    endpoint = session.get('endpoint')\n",
      "    project = session.get('project')\n",
      "    location = session.get('location')\n",
      "    logger.debug(f\"Session data: endpoint={endpoint}, project={project}, location={location}\")\n",
      "    try:\n",
      "        response = predict_custom_trained_model_sample(\n",
      "            project=project,\n",
      "            endpoint_id=endpoint,\n",
      "            location=location,\n",
      "            user_input= user_message,\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.exception(f\"Error in chatbot prediction: {e}\")\n",
      "        response = \"An error occurred. Please try again later.\"\n",
      "    logger.debug(f\"Returning chatbot response: {response}\") \n",
      "    return jsonify({'message': response})\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    logger.info(\"Starting Flask application\") \n",
      "    app.run(host='0.0.0.0', port=5000, debug=True) \n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_readibility = get_chat_response(chat, 'Improve readibility, do not omit anything and ensure it compiles.'  + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
      "from google.cloud import bigquery, storage, pubsub_v1\n",
      "from werkzeug.datastructures import FileStorage\n",
      "import bcrypt, os, base64, json\n",
      "import logging\n",
      "\n",
      "# Set up logging\n",
      "logger = logging.getLogger(__name__)\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "# Initialize Flask app\n",
      "app = Flask(__name__)\n",
      "app.secret_key = os.urandom(24)\n",
      "\n",
      "# Constants for BigQuery, Storage, and Pub/Sub\n",
      "DATASET_ID = 'chatbot'\n",
      "USERS_TABLE = 'users'\n",
      "USER_TRAINING_STATUS = 'user_training_status'\n",
      "BUCKET_NAME = \"personalize-chatbots-v1\"\n",
      "PUBSUB_TOPIC = 'your-pipeline-trigger-topic'\n",
      "\n",
      "# Import libraries for AI Platform predictions\n",
      "from typing import Dict, List, Union\n",
      "from google.cloud import aiplatform\n",
      "from google.protobuf import json_format\n",
      "from google.protobuf.struct_pb2 import Value\n",
      "import re\n",
      "\n",
      "# Log project ID and bucket name\n",
      "logger.debug(f\"Project ID from environment: {os.environ.get('PROJECT_ID')}\")\n",
      "logger.debug(f\"Using bucket: {BUCKET_NAME}\")\n",
      "\n",
      "\n",
      "def predict_custom_trained_model_sample(project: str, endpoint_id: str, location: str = \"us-central1\", api_endpoint: str = \"us-central1-aiplatform.googleapis.com\", user_input: str = input):\n",
      "    \"\"\"Predicts text using a custom-trained Vertex AI model.\n",
      "\n",
      "    Args:\n",
      "        project: The Google Cloud project ID.\n",
      "        endpoint_id: The ID of the Vertex AI endpoint.\n",
      "        location: The region where the endpoint is located.\n",
      "        api_endpoint: The API endpoint of Vertex AI.\n",
      "        user_input: The user's input text.\n",
      "\n",
      "    Returns:\n",
      "        The predicted text from the model.\n",
      "    \"\"\"\n",
      "\n",
      "    logger.debug(\"Function predict_custom_trained_model_sample started\")\n",
      "\n",
      "    try:\n",
      "        # Prepare prompt input for prediction\n",
      "        prompt_input = f\"Sender:\\n{user_input}\\n\\nAndres Perez:\\n\"\n",
      "        conversation_track = session.get('conversation_track')[-3:]\n",
      "        logger.debug(f\"Last 3 conversation track items: {conversation_track}\")\n",
      "        conversation_track_str = \"\\n\\n\".join(conversation_track + [prompt_input])\n",
      "        logger.debug(f\"Joined input for prediction: {conversation_track_str}\")\n",
      "        instances = {'prompt': conversation_track_str, 'max_tokens': 1024, 'temperature': 1, 'top_p': 0.7, 'top_k': 6}\n",
      "\n",
      "        # Initialize AI Platform prediction client\n",
      "        client_options = {\"api_endpoint\": api_endpoint}\n",
      "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
      "\n",
      "        # Format instances for prediction request\n",
      "        instances = instances if isinstance(instances, list) else [instances]\n",
      "        instances = [json_format.ParseDict(instance_dict, Value()) for instance_dict in instances]\n",
      "\n",
      "        # Set prediction parameters (empty in this case)\n",
      "        parameters_dict = {}\n",
      "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
      "\n",
      "        # Get endpoint path\n",
      "        endpoint = client.endpoint_path(project=project, location=location, endpoint=endpoint_id)\n",
      "\n",
      "        # Send prediction request and get response\n",
      "        response = client.predict(endpoint=endpoint, instances=instances, parameters=parameters)\n",
      "        predictions = response.predictions\n",
      "\n",
      "        # Extract predicted text using regex\n",
      "        pattern = r\"Perez:\\nOutput:\\n(.*)\"\n",
      "        match = re.search(pattern, predictions[0])\n",
      "\n",
      "        if match:\n",
      "            logger.info(f\"Successful prediction: {match.group(1)}\")\n",
      "            conversation_track.append(prompt_input + str(match.group(1)))\n",
      "            logger.debug(f\"Updated conversation track: {conversation_track}\")\n",
      "            session['conversation_track'] = conversation_track\n",
      "            return match.group(1)\n",
      "        else:\n",
      "            logger.error(\"Prediction not found in the response.\")\n",
      "            return \"Error: Prediction not found in the response.\"\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.exception(f\"An error occurred: {e}\")\n",
      "        raise  # Re-raise for error handling\n",
      "\n",
      "\n",
      "def extract_info_from_endpoint(url):\n",
      "    \"\"\"Extracts location, endpoint, and project information from a given Vertex AI endpoint URL.\n",
      "\n",
      "    Args:\n",
      "        url: The Vertex AI endpoint URL string.\n",
      "\n",
      "    Returns:\n",
      "        A dictionary containing the extracted values:\n",
      "            locations: The region.\n",
      "            endpoints: The endpoint ID.\n",
      "            projects: The project ID.\n",
      "    \"\"\"\n",
      "\n",
      "    pattern = r\"\\/projects\\/([^\\/]+)\\/locations\\/([^\\/]+)\\/endpoints\\/([^\\/]+)\\/operations\\/([^\\/]+)\"\n",
      "    logger.debug(f\"Using regex pattern: {pattern}\")\n",
      "    match = re.search(pattern, url)\n",
      "    logger.debug(f\"Regex match result: {match}\")\n",
      "    if match:\n",
      "        return {\"projects\": match.group(1), \"locations\": match.group(2), \"endpoints\": match.group(3)}\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "@app.route('/signup', methods=['POST'])\n",
      "def signup():\n",
      "    \"\"\"Handles user signup requests.\"\"\"\n",
      "\n",
      "    if request.method == 'POST':\n",
      "        email = request.form['email']\n",
      "        password = request.form['password']\n",
      "        confirm_password = request.form['confirm_password']\n",
      "\n",
      "        # Validate user input\n",
      "        error_message = None\n",
      "        if not email or not password or not confirm_password:\n",
      "            error_message = 'Please fill in all fields.'\n",
      "        elif password != confirm_password:\n",
      "            error_message = 'Passwords do not match.'\n",
      "\n",
      "        if error_message:\n",
      "            return error_message, 400\n",
      "\n",
      "        # Hash password for security\n",
      "        hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n",
      "\n",
      "        # Insert user into BigQuery\n",
      "        client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "        table_ref = client.dataset(DATASET_ID).table(USERS_TABLE)\n",
      "        table = client.get_table(table_ref)\n",
      "        row_to_insert = {'email': email, 'password_hash': hashed_password.decode('utf-8')}\n",
      "        client.insert_rows(table, [row_to_insert])\n",
      "        errors = client.insert_rows(table, [row_to_insert])\n",
      "        if errors:\n",
      "            logger.error(f\"Error submitting data to BigQuery: {errors}\")\n",
      "            return 'Error submitting data: {}'.format(errors), 500\n",
      "        else:\n",
      "            logger.info(f\"User signup successful: {email}\")\n",
      "            session['email'] = email\n",
      "            return redirect(url_for('upload'))\n",
      "\n",
      "\n",
      "@app.route('/login', methods=['POST'])\n",
      "def login():\n",
      "    \"\"\"Handles user login requests.\"\"\"\n",
      "\n",
      "    logger.debug(\"Login endpoint called\")\n",
      "    if request.method == 'POST':\n",
      "        email = request.form['email']\n",
      "        logger.debug(f\"Login attempt for email: {email}\")\n",
      "        password = request.form['password']\n",
      "\n",
      "        # Fetch user data from BigQuery\n",
      "        client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "\n",
      "        # Check password\n",
      "        query = f\"SELECT password_hash FROM `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USERS_TABLE}` WHERE email = '{email}'\"\n",
      "        logger.debug(f\"Executing BigQuery password query: {query}\")\n",
      "        results = client.query(query).result()\n",
      "        logger.debug(f\"BigQuery password query results: {results}\")\n",
      "        stored_password_hash = None\n",
      "        for row in results:\n",
      "            logger.debug(f\"Retrieved password hash from BigQuery: {row}\")\n",
      "            stored_password_hash = row.password_hash\n",
      "\n",
      "        # Check training status\n",
      "        query = f\"\"\"\n",
      "            SELECT training_status, end_point\n",
      "            FROM `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USER_TRAINING_STATUS}`\n",
      "            WHERE email = '{email}'\n",
      "            ORDER BY created_at DESC\n",
      "            LIMIT 1\n",
      "        \"\"\"\n",
      "        logger.debug(f\"Executing BigQuery training status query: {query}\")\n",
      "        results = client.query(query).result()\n",
      "        logger.debug(f\"BigQuery training status query results: {results}\")\n",
      "        user_training_status = None\n",
      "        endpoint_uri = None\n",
      "        for row in results:\n",
      "            logger.debug(f\"Retrieved training status from BigQuery: {row}\")\n",
      "            user_training_status = row.training_status\n",
      "            endpoint_uri = row.end_point\n",
      "\n",
      "        # Verify password and redirect based on training status\n",
      "        if stored_password_hash and bcrypt.checkpw(password.encode('utf-8'), stored_password_hash.encode('utf-8')):\n",
      "            session['email'] = email\n",
      "            if user_training_status:\n",
      "                endpoint_details = extract_info_from_endpoint(endpoint_uri)\n",
      "                logger.debug(f\"Setting session variables for endpoint: {endpoint_details}\")\n",
      "                session['endpoint'] = endpoint_details[\"endpoints\"]\n",
      "                session['location'] = endpoint_details[\"locations\"]\n",
      "                session['project'] = endpoint_details[\"projects\"]\n",
      "                logger.info(f\"User logged in with existing training: {email}\")\n",
      "                return redirect(url_for('chat_page'))\n",
      "            else:\n",
      "                logger.info(f\"User logged in without training: {email}\")\n",
      "                return redirect(url_for('upload'))\n",
      "        else:\n",
      "            logger.warning(f\"Invalid login attempt for email: {email}\")\n",
      "            return 'Invalid email or password', 401\n",
      "\n",
      "\n",
      "@app.route('/upload')\n",
      "def upload():\n",
      "    \"\"\"Renders the file upload page for users to provide training data.\"\"\"\n",
      "\n",
      "    email = session.get('email')\n",
      "    logger.debug(f\"Upload page accessed by email: {email}\")\n",
      "    if not email:\n",
      "        logger.info(\"Redirecting to home page due to missing email in session\")\n",
      "        return redirect(url_for('home'))\n",
      "    logger.debug(\"Rendering upload template\")\n",
      "    return render_template('upload.html')\n",
      "\n",
      "\n",
      "@app.route('/handle_upload', methods=['POST'])\n",
      "def handle_upload():\n",
      "    \"\"\"Handles file upload and triggers the training pipeline.\"\"\"\n",
      "\n",
      "    files_metadata = []\n",
      "    email = session.get('email')\n",
      "    logger.debug(f\"Handling file upload for email: {email}\")\n",
      "    code_version = request.form.get('code_version')\n",
      "    logger.debug(f\"Code version for training: {code_version}\")\n",
      "    model_name = request.form.get('model_name')\n",
      "    epochs = request.form.get('epochs')\n",
      "    logger.debug(f\"Selected model: {model_name}, Epochs: {epochs}\")\n",
      "    user_name = re.match(r'^([^@]+)', str(email)).group(1)\n",
      "\n",
      "    # Initialize Pub/Sub publisher and prepare storage paths\n",
      "    publisher = pubsub_v1.PublisherClient()\n",
      "    topic_path = publisher.topic_path(os.environ.get('PROJECT_ID'), PUBSUB_TOPIC)\n",
      "    logger.debug(f\"Publishing message to Pub/Sub topic: {topic_path}\")\n",
      "    blob_folder = os.path.join(user_name, 'input_data')\n",
      "\n",
      "    if not email:\n",
      "        logger.info(\"Redirecting to home page due to missing email in session\")\n",
      "        return redirect(url_for('home'))\n",
      "\n",
      "    # Upload files to Cloud Storage\n",
      "    for file in request.files.getlist('files'):\n",
      "        logger.debug(f\"Processing uploaded file: {file.filename}\")\n",
      "        client = storage.Client(os.environ.get('PROJECT_ID'))\n",
      "        bucket = client.get_bucket(BUCKET_NAME)\n",
      "        blob_string = os.path.join(blob_folder, file.filename)\n",
      "        blob = bucket.blob(blob_string)\n",
      "        blob.upload_from_string(FileStorage(file).stream.read())\n",
      "        logger.info(f\"Uploaded file to Cloud Storage: {blob.name}\")\n",
      "        files_metadata.append({\"file_path\": f\"gs://{BUCKET_NAME}/{blob_string}\", \"filename\": file.filename})\n",
      "\n",
      "    # Prepare and publish Pub/Sub message\n",
      "    message_data = {\n",
      "        \"user_name\": user_name,\n",
      "        \"files\": files_metadata,\n",
      "        \"blob_folder\": blob_folder,\n",
      "        \"model_name\": model_name,\n",
      "        \"epochs\": epochs,\n",
      "        \"bucket_name\": BUCKET_NAME,\n",
      "        \"tag_version\": code_version,\n",
      "        \"project_id\": os.environ.get('PROJECT_ID')\n",
      "\n",
      "    }\n",
      "    message_data_json = json.dumps(message_data)\n",
      "    message_data_bytes = message_data_json.encode('utf-8')\n",
      "    logger.debug(f\"Publishing message data to Pub/Sub: {message_data_bytes}\")\n",
      "    publisher.publish(topic_path, message_data_bytes)\n",
      "\n",
      "    # Update training status in BigQuery\n",
      "    client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "    table_ref = client.dataset(DATASET_ID).table(USER_TRAINING_STATUS)\n",
      "    table = client.get_table(table_ref)\n",
      "    row_to_insert = {'email': email, 'training_status': False}\n",
      "    logger.debug(f\"Inserting training status into BigQuery: {row_to_insert}\")\n",
      "    client.insert_rows(table, [row_to_insert])\n",
      "    errors = client.insert_rows(table, [row_to_insert])\n",
      "    if errors:\n",
      "        logger.error(f\"Error submitting data to BigQuery: {errors}\")\n",
      "        return 'Error submitting data: {}'.format(errors), 500\n",
      "    else:\n",
      "        logger.info(\"File upload and training initiation successful!\")\n",
      "        return \"Upload Successful!\", 200\n",
      "\n",
      "\n",
      "@app.route('/home')\n",
      "def home():\n",
      "    \"\"\"Renders the home page of the application.\"\"\"\n",
      "    logger.debug(\"Rendering home page\")\n",
      "    return render_template('index.html')\n",
      "\n",
      "\n",
      "@app.route('/chat_page')\n",
      "def chat_page():\n",
      "    \"\"\"Renders the chat page where users interact with the chatbot.\"\"\"\n",
      "\n",
      "    email = session.get('email')\n",
      "    if not email:\n",
      "        logger.info(\"Redirecting to home page due to missing email in session\")\n",
      "        return redirect(url_for('home'))\n",
      "    logger.debug(\"Initializing conversation track and rendering chat page\")\n",
      "    session['conversation_track'] = []\n",
      "    return render_template('chat.html')\n",
      "\n",
      "\n",
      "@app.route('/send_message', methods=['POST'])\n",
      "def send_message():\n",
      "    \"\"\"Handles sending user messages to the chatbot and returning responses.\"\"\"\n",
      "\n",
      "    user_message = request.json['message']\n",
      "    logger.debug(f\"Received user message: {user_message}\")\n",
      "    email = session.get('email')\n",
      "    if not email:\n",
      "        logger.info(\"User not logged in. Redirecting to home\")\n",
      "        return redirect(url_for('home'))\n",
      "\n",
      "    endpoint = session.get('endpoint')\n",
      "    project = session.get('project')\n",
      "    location = session.get('location')\n",
      "    logger.debug(f\"Session data: endpoint={endpoint}, project={project}, location={location}\")\n",
      "    try:\n",
      "        response = predict_custom_trained_model_sample(\n",
      "            project=project,\n",
      "            endpoint_id=endpoint,\n",
      "            location=location,\n",
      "            user_input=user_message,\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.exception(f\"Error in chatbot prediction: {e}\")\n",
      "        response = \"An error occurred. Please try again later.\"\n",
      "    logger.debug(f\"Returning chatbot response: {response}\")\n",
      "    return jsonify({'message': response})\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    logger.info(\"Starting Flask application\")\n",
      "    app.run(host='0.0.0.0', port=5000, debug=True)\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res_readibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_RUNNING_APP=gemma-chatbot-running-app,TAG_NAME=masterv6\n",
      "E0423 20:17:56.498637889  857317 backup_poller.cc:127]                 Run client channel backup poller: UNKNOWN:pollset_work {created_time:\"2024-04-23T20:17:56.498271537+00:00\", children:[UNKNOWN:Bad file descriptor {created_time:\"2024-04-23T20:17:56.49814896+00:00\", errno:9, os_error:\"Bad file descriptor\", syscall:\"epoll_wait\"}]}\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) argument --substitutions: Bad syntax for dict arg: [{substitutions}]. Please see `gcloud topic flags-file` or `gcloud topic escaping` for information on providing list or dictionary flag values with special characters.\n",
      "Usage: gcloud builds submit [[SOURCE] --no-source] [optional flags]\n",
      "  optional flags may be  --async | --no-cache | --config |\n",
      "                         --default-buckets-behavior | --dir | --disk-size |\n",
      "                         --gcs-log-dir | --gcs-source-staging-dir |\n",
      "                         --git-source-dir | --git-source-revision | --help |\n",
      "                         --ignore-file | --machine-type | --pack | --region |\n",
      "                         --revision | --no-source | --substitutions |\n",
      "                         --suppress-logs | --tag | --timeout | --worker-pool\n",
      "\n",
      "For detailed information on this command and its flags, run:\n",
      "  gcloud builds submit --help\n"
     ]
    }
   ],
   "source": [
    "TAG_NAME = 'masterv6'\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "CONTAINER_IMAGE_RUNNING_APP = f\"{CONTAINER_IMAGE_NAME}-running-app\"\n",
    "substitutions=\"\"\"\n",
    "_CONTAINER_IMAGE_RUNNING_APP={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           CONTAINER_IMAGE_RUNNING_APP,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(substitutions)\n",
    "!gcloud builds submit . --config \"components/app_flask/cloudbuild.yaml\" --substitutions {substitutions} --region={GCP_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_RUNNING_APP=gemma-chatbot-running-app,TAG_NAME=masterv6\n",
      "E0423 20:11:36.498395226  855622 backup_poller.cc:127]                 Run client channel backup poller: UNKNOWN:pollset_work {created_time:\"2024-04-23T20:11:36.498070993+00:00\", children:[UNKNOWN:Bad file descriptor {created_time:\"2024-04-23T20:11:36.497968957+00:00\", errno:9, os_error:\"Bad file descriptor\", syscall:\"epoll_wait\"}]}\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) argument --substitutions: Bad syntax for dict arg: [{substitutions}]. Please see `gcloud topic flags-file` or `gcloud topic escaping` for information on providing list or dictionary flag values with special characters.\n",
      "Usage: gcloud builds submit [[SOURCE] --no-source] [optional flags]\n",
      "  optional flags may be  --async | --no-cache | --config |\n",
      "                         --default-buckets-behavior | --dir | --disk-size |\n",
      "                         --gcs-log-dir | --gcs-source-staging-dir |\n",
      "                         --git-source-dir | --git-source-revision | --help |\n",
      "                         --ignore-file | --machine-type | --pack | --region |\n",
      "                         --revision | --no-source | --substitutions |\n",
      "                         --suppress-logs | --tag | --timeout | --worker-pool\n",
      "\n",
      "For detailed information on this command and its flags, run:\n",
      "  gcloud builds submit --help\n"
     ]
    }
   ],
   "source": [
    "TAG_NAME = 'masterv6'\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "CONTAINER_IMAGE_RUNNING_APP = f\"{CONTAINER_IMAGE_NAME}-running-app\"\n",
    "substitutions=\"\"\"\n",
    "_CONTAINER_IMAGE_RUNNING_APP={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           CONTAINER_IMAGE_RUNNING_APP,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(substitutions)\n",
    "!gcloud builds submit . --config \"components/app_flask/cloudbuild.yaml\" --substitutions {substitutions} --region={GCP_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_RUNNING_APP=gemma-chatbot-running-app,TAG_NAME=masterv6\n",
      "E0423 20:11:36.498395226  855622 backup_poller.cc:127]                 Run client channel backup poller: UNKNOWN:pollset_work {created_time:\"2024-04-23T20:11:36.498070993+00:00\", children:[UNKNOWN:Bad file descriptor {created_time:\"2024-04-23T20:11:36.497968957+00:00\", errno:9, os_error:\"Bad file descriptor\", syscall:\"epoll_wait\"}]}\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) argument --substitutions: Bad syntax for dict arg: [{substitutions}]. Please see `gcloud topic flags-file` or `gcloud topic escaping` for information on providing list or dictionary flag values with special characters.\n",
      "Usage: gcloud builds submit [[SOURCE] --no-source] [optional flags]\n",
      "  optional flags may be  --async | --no-cache | --config |\n",
      "                         --default-buckets-behavior | --dir | --disk-size |\n",
      "                         --gcs-log-dir | --gcs-source-staging-dir |\n",
      "                         --git-source-dir | --git-source-revision | --help |\n",
      "                         --ignore-file | --machine-type | --pack | --region |\n",
      "                         --revision | --no-source | --substitutions |\n",
      "                         --suppress-logs | --tag | --timeout | --worker-pool\n",
      "\n",
      "For detailed information on this command and its flags, run:\n",
      "  gcloud builds submit --help\n"
     ]
    }
   ],
   "source": [
    "TAG_NAME = 'masterv6'\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "CONTAINER_IMAGE_RUNNING_APP = f\"{CONTAINER_IMAGE_NAME}-running-app\"\n",
    "substitutions=\"\"\"\n",
    "_CONTAINER_IMAGE_RUNNING_APP={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           CONTAINER_IMAGE_RUNNING_APP,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(substitutions)\n",
    "!gcloud builds submit . --config \"components/app_flask/cloudbuild.yaml\" --substitutions {substitutions} --region={GCP_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
