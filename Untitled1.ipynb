{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ee61f0-6718-4878-b513-cebfa470d485",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gemma_2b_en\n",
      "{'model_size': '2b', 'finetuned_model_dir': './gemma_2b_en', 'finetuned_weights_path': './gemma_2b_en/model.weights.h5', 'finetuned_vocab_path': './gemma_2b_en/vocabulary.spm', 'huggingface_model_dir': './gemma_2b_en_huggingface', 'deployed_model_blob': 'gemma_2b_en/20240321153208', 'deployed_model_uri': 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240321153208', 'fine_tuned_keras_blob': 'gemma_2b_en/keras/20240321153208', 'model_name_vllm': 'gemma_2b_en-vllm', 'machine_type': 'g2-standard-8', 'accelerator_type': 'NVIDIA_L4', 'accelerator_count': 1}\n",
      "gemma_2b_en\n",
      "./gemma_2b_en\n",
      "{'model_size': '2b', 'finetuned_model_dir': './gemma_2b_en', 'finetuned_weights_path': './gemma_2b_en/model.weights.h5', 'finetuned_vocab_path': './gemma_2b_en/vocabulary.spm', 'huggingface_model_dir': './gemma_2b_en_huggingface', 'deployed_model_blob': 'gemma_2b_en/20240321153208', 'deployed_model_uri': 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240321153208', 'fine_tuned_keras_blob': 'gemma_2b_en/keras/20240321153208', 'model_name_vllm': 'gemma_2b_en-vllm', 'machine_type': 'g2-standard-8', 'accelerator_type': 'NVIDIA_L4', 'accelerator_count': 1}\n",
      "./gemma_2b_en\n",
      "This is the dictionary {{channel:task=;name=model_paths;type=Dict;}}\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321160451\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321160451')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pythagorean-20240321160451?project=24796876098\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321160451 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321160451 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321160451 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321160451 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321160451 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [convert-checkpoints-op].; Job (project_id = able-analyst-416817, job_id = 5762828788098400256) is failed due to the above error.; Failed to handle the job: {project_number = 24796876098, job_id = 5762828788098400256}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 148\u001b[0m\n\u001b[1;32m    143\u001b[0m vertex_pipelines_job \u001b[38;5;241m=\u001b[39m vertexai\u001b[38;5;241m.\u001b[39mpipeline_jobs\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[1;32m    144\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-whatsapp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    145\u001b[0m     template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-whatsapp.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m )\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m#vertex_pipelines_job.worker_pool_specs = worker_pool_specs \u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m \u001b[43mvertex_pipelines_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:323\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:850\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    849\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    853\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:366\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    360\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m    361\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[1;32m    362\u001b[0m     reserved_ip_ranges\u001b[38;5;241m=\u001b[39mreserved_ip_ranges,\n\u001b[1;32m    363\u001b[0m     create_request_timeout\u001b[38;5;241m=\u001b[39mcreate_request_timeout,\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:615\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [convert-checkpoints-op].; Job (project_id = able-analyst-416817, job_id = 5762828788098400256) is failed due to the above error.; Failed to handle the job: {project_number = 24796876098, job_id = 5762828788098400256}\"\n"
     ]
    }
   ],
   "source": [
    "from config import Config\n",
    "from util import get_model_paths_and_config, upload2bs\n",
    "\n",
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "SERVICE_ACCOUNT = 'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'\n",
    "from datetime import datetime\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "GCP_REGION='us-central1'\n",
    "IMAGE_NAME=\"gemma-chatbot\"\n",
    "TAG_NAME = 'latest'\n",
    "KAGGLE_USERNAME='andrehpereh1'\n",
    "KAGGLE_KEY='5859e39806d9456749dcbac685f04bc9'\n",
    "model_paths_and_config = get_model_paths_and_config(Config.MODEL_NAME)\n",
    "print(model_paths_and_config)\n",
    "\n",
    "from kfp import dsl\n",
    "import kfp.v2 as kfp\n",
    "from kfp.dsl import OutputPath, Artifact, InputPath\n",
    "from kfp import compiler\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "from config import Config\n",
    "print(Config.MODEL_NAME)\n",
    "model_paths_and_config = get_model_paths_and_config(Config.MODEL_NAME)\n",
    "print(model_paths_and_config)\n",
    "# from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "# from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "accelerator_count = 1\n",
    "max_model_len = 512\n",
    "dtype = 'bfloat16'\n",
    "\n",
    "vllm_args = [\n",
    "    \"--host=0.0.0.0\",\n",
    "    \"--port=7080\",\n",
    "    f\"--tensor-parallel-size={accelerator_count}\",\n",
    "    \"--swap-space=16\",\n",
    "    \"--gpu-memory-utilization=0.95\",\n",
    "    f\"--max-model-len={max_model_len}\",\n",
    "    f\"--dtype={dtype}\",\n",
    "    \"--disable-log-stats\",\n",
    "]\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "  base_image ='gcr.io/able-analyst-416817/gemma-chatbot-data-preparation:latest'\n",
    ")\n",
    "def process_whatsapp_chat_op(\n",
    "  bucket_name: str,\n",
    "  directory: str,\n",
    "  dataset_path: OutputPath('Dataset')\n",
    "):\n",
    "    import data_ingestion\n",
    "    import json\n",
    "    formatted_messages = data_ingestion.process_whatsapp_chat(bucket_name, directory)\n",
    "    with open(dataset_path, 'w') as f:\n",
    "        json.dump(formatted_messages, f)\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "  base_image = 'gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest'\n",
    ")\n",
    "def fine_tunning(\n",
    "  dataset_path: InputPath('Dataset'),\n",
    "  model_paths: dict,\n",
    "  #finetuned_weights_dir: OutputPath('Model'),\n",
    ") -> str:\n",
    "    # import test_container\n",
    "    import trainer\n",
    "    import json\n",
    "    import util\n",
    "    import os\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    os.makedirs(model_paths['finetuned_model_dir'], exist_ok=True)\n",
    "    finetuned_weights_path = os.path.join(model_paths['finetuned_model_dir'], 'model.weights.h5') \n",
    "    \n",
    "    model = trainer.finetune_gemma(dataset, model_paths, False)\n",
    "    print(\"It brought the model in here\")\n",
    "    print(type(model))\n",
    "    print(\"Its gonna save it here\", finetuned_weights_path)\n",
    "    #model.save_weights(finetuned_weights_path)\n",
    "    #model.preprocessor.tokenizer.save_assets(model_paths['finetuned_model_dir'])\n",
    "    bucket_name = 'able-analyst-416817-chatbot-v1' # move to parameter.\n",
    "    util.upload2bs(\n",
    "        local_directory = model_paths['finetuned_model_dir'], bucket_name = 'able-analyst-416817-chatbot-v1',\n",
    "        destination_subfolder = model_paths['fine_tuned_keras_blob']\n",
    "    )\n",
    "    model_gcs = \"gs://{}/{}\".format(bucket_name, model_paths['fine_tuned_keras_blob'])  \n",
    "    print(\"This is the storage bucket\", model_gcs)\n",
    "    return model_gcs\n",
    "    \n",
    "\n",
    "@dsl.component(\n",
    "  base_image = 'gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest'\n",
    ")\n",
    "def convert_checkpoints_op(\n",
    "  keras_gcs_model: str,\n",
    "  model_paths: dict\n",
    ") -> str:\n",
    "    import conversion_function\n",
    "    import os\n",
    "    import util\n",
    "    bucket_name, blob_name = os.path.dirname(keras_gcs_model).lstrip(\"gs://\").split(\"/\", 1) \n",
    "    print(\"This is the keras passed\", finetuned_weights_dir)\n",
    "    util.download_all_from_blob(bucket_name, blob_name, local_destination=model_paths['finetuned_model_dir'])\n",
    "    if os.path.exists(\"./model.weights.h5\"):\n",
    "        print(\"File exists!\")\n",
    "    else:\n",
    "        print(\"File does not exist.\")\n",
    "    converted_fined_tuned_path = conversion_function.convert_checkpoints(\n",
    "        weights_file=finetuned_weights_path,\n",
    "        size=model_paths['model_size'],\n",
    "        output_dir=model_paths['huggingface_model_dir'],\n",
    "        vocab_path=model_paths['finetuned_vocab_path']\n",
    "    )\n",
    "    return converted_fined_tuned_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dsl.pipeline\n",
    "def pythagorean(\n",
    "    bucket_name: str = \"able-analyst-416817-chatbot-v1\", directory: str = \"input_data/andrehpereh\", \n",
    "    model_paths: dict=get_model_paths_and_config(Config.MODEL_NAME)\n",
    "):\n",
    "    # from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    \n",
    "    whatup = process_whatsapp_chat_op(bucket_name = bucket_name, directory = directory)\n",
    "\n",
    "    trainer = fine_tunning(dataset_path=whatup.outputs['dataset_path'], model_paths=model_paths)\n",
    "    trainer.set_memory_limit(\"40G\").set_cpu_limit('8.0m') .set_accelerator_limit(1).add_node_selector_constraint(\"NVIDIA_L4\")\n",
    "    \n",
    "    print(\"This is the dictionary\", model_paths)\n",
    "    converted = convert_checkpoints_op(\n",
    "        keras_gcs_model=trainer.output, model_paths=model_paths\n",
    "    ).set_memory_limit(\"40G\").set_cpu_limit('8.0m').set_accelerator_limit(1).add_node_selector_constraint(\"NVIDIA_L4\")\n",
    "    \n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pythagorean, package_path=\"test-whatsapp.json\"\n",
    ")\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
    "    display_name=\"test-whatsapp\",\n",
    "    template_path=\"test-whatsapp.json\"\n",
    ")\n",
    "#vertex_pipelines_job.worker_pool_specs = worker_pool_specs \n",
    "vertex_pipelines_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d4859b-0461-4a5b-a369-b0bf8849c39a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gemma_2b_en\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_size': '2b',\n",
       " 'finetuned_model_dir': './gemma_2b_en',\n",
       " 'finetuned_weights_path': './gemma_2b_en/model.weights.h5',\n",
       " 'finetuned_vocab_path': './gemma_2b_en/vocabulary.spm',\n",
       " 'huggingface_model_dir': './gemma_2b_en_huggingface',\n",
       " 'deployed_model_blob': 'gemma_2b_en/20240321153208',\n",
       " 'deployed_model_uri': 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240321153208',\n",
       " 'fine_tuned_keras_blob': 'gemma_2b_en/keras/20240321153208',\n",
       " 'model_name_vllm': 'gemma_2b_en-vllm',\n",
       " 'machine_type': 'g2-standard-8',\n",
       " 'accelerator_type': 'NVIDIA_L4',\n",
       " 'accelerator_count': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_model_paths_and_config(Config.MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbba336-c748-4e47-95e6-8d2e0cfb0040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m118"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
