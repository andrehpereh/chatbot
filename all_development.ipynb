{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f178cfef-2782-4a77-b4f8-7d372a7c61f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.2\n",
      "2.16.1\n",
      "3.0.5\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import datetime\n",
    "import json\n",
    "import locale\n",
    "import keras\n",
    "import keras_nlp\n",
    "import torch\n",
    "import transformers\n",
    "from google.cloud import aiplatform\n",
    "from numba import cuda\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "print(keras_nlp.__version__)\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d508aa69-1810-4fc2-9a3d-735e55e4ccf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "KAGGLE_USERNAME=\"andrehpereh1\"\n",
    "KAGGLE_KEY=\"5859e39806d9456749dcbac685f04bc9\"\n",
    "KERAS_BACKEND=\"tensorflow\"\n",
    "REGION='us-central1'\n",
    "BUCKET_NAME=f\"{PROJECT_ID}-chatbot-v1\"\n",
    "BUCKET_URI=f\"gs://{BUCKET_NAME}\"\n",
    "SERVICE_ACCOUNT_NAME=\"gemma-vertexai-chatbot\"\n",
    "SERVICE_ACCOUNT=f\"{SERVICE_ACCOUNT_NAME}@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "os.environ[\"KERAS_BACKEND\"] = KERAS_BACKEND  # Or \"torch\" or \"tensorflow\".\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\"\n",
    "os.environ[\"KAGGLE_USERNAME\"] = KAGGLE_USERNAME\n",
    "os.environ[\"KAGGLE_KEY\"] = KAGGLE_KEY\n",
    "TIMESTAMP=datetime.datetime.now().strftime('%Y%m%d%H%M%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afe35194-c1b1-4a54-ac89-11c77f4c2750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a70bad4-ee58-4629-a71e-c2b368485b0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TIMESTAMP='20240313143610'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0344711-cc47-48f1-9e7b-53c135206d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"gemma_2b_en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0321807-6918-4bc3-83ec-f224e7cdb667",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_paths_and_config(model_name):\n",
    "    \"\"\"\n",
    "    Constructs paths, determines machine configuration, and gets the VLLM model name for a given model.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): The base name of the model (e.g., \"gemma_2b_en\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the following keys:\n",
    "            - 'model_size': The size of the model (\"2b\" or \"7b\").\n",
    "            - 'finetuned_model_dir': Path to the finetuned model directory.\n",
    "            - 'finetuned_weights_path': Path to the finetuned model weights.\n",
    "            - 'finetuned_vocab_path': Path to the finetuned model vocabulary.\n",
    "            - 'huggingface_model_dir': Path to the Hugging Face model directory.\n",
    "            - 'deployed_model_blob': Blob name of the deployed model in Cloud Storage.\n",
    "            - 'deployed_model_uri': URI of the deployed model in Cloud Storage.\n",
    "            - 'machine_type': The appropriate machine type.\n",
    "            - 'accelerator_type': The accelerator type.\n",
    "            - 'accelerator_count': The number of accelerators.\n",
    "            - 'model_name_vllm': The VLLM-specific model name.\n",
    "    \"\"\"\n",
    "\n",
    "    allowed_models = [\n",
    "        \"gemma_2b_en\",\n",
    "        \"gemma_instruct_2b_en\",\n",
    "        \"gemma_7b_en\",\n",
    "        \"gemma_instruct_7b_en\",\n",
    "    ]\n",
    "\n",
    "    if model_name not in allowed_models:\n",
    "        raise ValueError(f\"Invalid model_name. Supported models are: {allowed_models}\")\n",
    "\n",
    "    # Construct paths\n",
    "    model_size = model_name.split(\"_\")[-2]\n",
    "    assert model_size in (\"2b\", \"7b\")\n",
    "\n",
    "    finetuned_model_dir = f\"./{model_name}\"\n",
    "    finetuned_weights_path = f\"{finetuned_model_dir}/model.weights.h5\"\n",
    "    finetuned_vocab_path = f\"{finetuned_model_dir}/vocabulary.spm\"\n",
    "    huggingface_model_dir = f\"./{model_name}_huggingface\"\n",
    "    timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S') \n",
    "    deployed_model_blob = f\"{model_name}/{timestamp}\" \n",
    "    deployed_model_uri = f\"{BUCKET_URI}/{deployed_model_blob}\"  # Assuming BUCKET_URI is globally defined\n",
    "\n",
    "    # Determine machine configuration\n",
    "    machine_config = {\n",
    "        \"2b\": {\n",
    "            \"machine_type\": \"g2-standard-8\",\n",
    "            \"accelerator_type\": \"NVIDIA_L4\",\n",
    "            \"accelerator_count\": 1\n",
    "        },\n",
    "        \"7b\": {\n",
    "            \"machine_type\": \"g2-standard-12\",\n",
    "            \"accelerator_type\": \"NVIDIA_L4\",\n",
    "            \"accelerator_count\": 1\n",
    "        }\n",
    "    }[model_size]  # Efficient lookup\n",
    "\n",
    "    return {\n",
    "        \"model_size\": model_size,\n",
    "        \"finetuned_model_dir\": finetuned_model_dir,\n",
    "        \"finetuned_weights_path\": finetuned_weights_path,\n",
    "        \"finetuned_vocab_path\": finetuned_vocab_path,\n",
    "        \"huggingface_model_dir\": huggingface_model_dir,\n",
    "        \"deployed_model_blob\": deployed_model_blob,\n",
    "        \"deployed_model_uri\": deployed_model_uri,\n",
    "        \"model_name_vllm\": f\"{model_name}-vllm\", \n",
    "        **machine_config  # Add machine config directly\n",
    "    }\n",
    "\n",
    "# Example Usage\n",
    "model_info = get_model_paths_and_config(\"gemma_2b_en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "125d7007-ed9a-460d-b86d-60c529991b51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gs://able-analyst-416817-chatbot-v1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUCKET_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76a59c11-b306-414a-b4f7-a2d9af5427b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_whatsapp_chat(directory):\n",
    "\n",
    "    current_sender = None\n",
    "    current_file = None\n",
    "    consecutive_messages = []\n",
    "    qa_pairs_all = []  # List to store question-answer pairs\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        consecutive_messages = []\n",
    "        qa_pairs = []  # List to store question-answer pairs\n",
    "        # Check if the current item is a file and ends with \".txt\"\n",
    "        if os.path.isfile(filepath) and filename.endswith('.txt'):\n",
    "            print(filepath)\n",
    "             \n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                if line.startswith(('- ', '\\n')):\n",
    "                    continue\n",
    "\n",
    "                # Extract sender and message (Regex handles potential variations)\n",
    "                match = re.search(r'^.*-\\s(?P<sender>.*?):\\s(?P<message>.*)$', line)\n",
    "                if match:\n",
    "                    sender = match.group('sender').strip()\n",
    "                    if sender != 'Andres Perez':\n",
    "                        sender = 'Sender'\n",
    "                    message = match.group('message').strip()\n",
    "                    message = message.replace(\"<Media omitted>\", \"\")\n",
    "                    message = message.replace(\"Missed video call\", \"\")\n",
    "                    message = message.replace(\"null\", \"\")\n",
    "                    message = re.sub(r'http\\S+', '', message).strip()\n",
    "\n",
    "                    # Concatenate consecutive messages by the same sender\n",
    "                    if sender == current_sender:\n",
    "                        consecutive_messages.append(message)\n",
    "                    else:\n",
    "                        if consecutive_messages:\n",
    "                            qa_pairs.append(' '.join(consecutive_messages))\n",
    "                        current_sender = sender\n",
    "                        consecutive_messages = [f\"{sender}:\\n{message}\"]\n",
    "\n",
    "            # Add the last set of messages\n",
    "            if consecutive_messages:\n",
    "                qa_pairs.append(', '.join(consecutive_messages))\n",
    "        \n",
    "        qa_pairs_all.extend(qa_pairs)\n",
    "    if len(qa_pairs_all) % 2 != 0:\n",
    "        qa_pairs_all = qa_pairs_all[:-1]\n",
    "\n",
    "    res = np.array(qa_pairs_all).reshape(len(qa_pairs_all) // 2, 2)\n",
    "    formatted_messages = [f\"{message_pair[0]}\\n\\n{message_pair[1]}\" for message_pair in res]\n",
    "    return formatted_messages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1bce1a4-ee3d-46dc-a3ab-7419642f7193",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_size': '2b', 'finetuned_model_dir': './gemma_2b_en', 'finetuned_weights_path': './gemma_2b_en/model.weights.h5', 'finetuned_vocab_path': './gemma_2b_en/vocabulary.spm', 'huggingface_model_dir': './gemma_2b_en_huggingface', 'deployed_model_blob': 'gemma_2b_en/20240314144813', 'deployed_model_uri': 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240314144813', 'model_name_vllm': 'gemma_2b_en-vllm', 'machine_type': 'g2-standard-8', 'accelerator_type': 'NVIDIA_L4', 'accelerator_count': 1}\n",
      "./CHAT/WhatsApp Chat with Ilse Flatmate.txt\n",
      "./CHAT/WhatsApp Chat with Ruben Ewald Puijker.txt\n",
      "./CHAT/WhatsApp Chat with Mike Haarlem.txt\n",
      "./CHAT/WhatsApp Chat with Anki.txt\n",
      "./CHAT/WhatsApp Chat with Michael.txt\n",
      "./CHAT/WhatsApp Chat with Rosa Rosa Rosa.txt\n",
      "['Sender:\\nFooooddd\\n\\nAndres Perez:\\nComing :)', \"Sender:\\nCan I maybe borrow your iron?\\n\\nAndres Perez:\\nIt's not my iron But yeah haha Or is it? ğŸ¤”ğŸ§\", 'Sender:\\nHahah I donâ€™t know But cool :)\\n\\nAndres Perez:\\n', 'Sender:\\nğŸ˜‚ğŸ˜‚ Enough for a while then Thereâ€™s also some leftover pasta in the fridge if you want And thereâ€™s also cheese so feel freeğŸ˜†âœŒğŸ»\\n\\nAndres Perez:\\nJust waking up from my nap ğŸ˜´ So wanna watch the race? Thanks ğŸ¥°', 'Sender:\\nIâ€™m at my sister right now, but the race starts at 10 and I think Iâ€™ll be back by then Race starts at 9, so Iâ€™ll be home just in timeğŸ˜…\\n\\nAndres Perez:\\nOhhh haha you lied to me']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_paths_and_config = get_model_paths_and_config(model_name)\n",
    "print(model_paths_and_config) \n",
    "# Example Usage:\n",
    "directory = \"./CHAT\" \n",
    "data = process_whatsapp_chat(directory)\n",
    "print(data[123:128])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2604f444-a5ba-4265-9f01-5c1cb5bd9fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def finetune_gemma(data: list[str], model_paths:dict, model_name: str, rank_lora: int=6, sequence_length: int=256, epochs: int=2, batch_size: int=1) :\n",
    "    # keras_nlp.models.GemmaCausalLM.from_preset(model)\n",
    "    # Reduce the input sequence length to limit memory usage\n",
    "    model = keras_nlp.models.GemmaCausalLM.from_preset(model_name)\n",
    "    model.summary()\n",
    "    model.backbone.enable_lora(rank=rank_lora)\n",
    "    model.summary()\n",
    "    model.preprocessor.sequence_length = sequence_length\n",
    "\n",
    "    # Use AdamW (a common optimizer for transformer models)\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    # Exclude layernorm and bias terms from decay\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        sampler=\"greedy\",\n",
    "    )\n",
    "    model.fit(data, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "    os.makedirs(model_paths['finetuned_model_dir'], exist_ok=True)\n",
    "    model.save_weights(model_paths['finetuned_weights_path'])\n",
    "    model.preprocessor.tokenizer.save_assets(model_paths['finetuned_model_dir'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ecf7a725-0311-4841-98f3-c075334484e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./gemma_2b_en'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths['finetuned_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b34468ba-3503-4814-b382-838092f308f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 12:31:22.942482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20758 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>â”ƒ<span style=\"font-weight: bold\">                                             Vocab # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   â”‚                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   â”‚                                             \u001b[38;5;34m256,000\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gemma_backbone                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> â”‚ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               â”‚                           â”‚                 â”‚ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_embedding               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> â”‚ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gemma_backbone                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        â”‚   \u001b[38;5;34m2,506,172,416\u001b[0m â”‚ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        â”‚\n",
       "â”‚ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               â”‚                           â”‚                 â”‚ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_embedding               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      â”‚     \u001b[38;5;34m524,288,000\u001b[0m â”‚ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>â”ƒ<span style=\"font-weight: bold\">                                             Vocab # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   â”‚                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   â”‚                                             \u001b[38;5;34m256,000\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gemma_backbone                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,854,400</span> â”‚ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               â”‚                           â”‚                 â”‚ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_embedding               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> â”‚ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ gemma_backbone                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        â”‚   \u001b[38;5;34m2,506,854,400\u001b[0m â”‚ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        â”‚\n",
       "â”‚ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               â”‚                           â”‚                 â”‚ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ token_embedding               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      â”‚     \u001b[38;5;34m524,288,000\u001b[0m â”‚ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
       "â”‚ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,854,400</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,854,400\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">681,984</span> (2.60 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m681,984\u001b[0m (2.60 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 12:31:39.430876: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710333120.755607   29836 service.cc:145] XLA service 0x7f51dc004620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1710333120.755651   29836 service.cc:153]   StreamExecutor device (0): NVIDIA L4, Compute Capability 8.9\n",
      "2024-03-13 12:32:01.566440: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "W0000 00:00:1710333123.583961   29836 assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n",
      "2024-03-13 12:32:05.704324: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8904\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1710333128.347034   30036 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_738', 32 bytes spill stores, 32 bytes spill loads\n",
      "\n",
      "I0000 00:00:1710333130.625467   30032 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_523', 80 bytes spill stores, 80 bytes spill loads\n",
      "\n",
      "I0000 00:00:1710333131.671395   30038 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_737', 28 bytes spill stores, 24 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/50\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m33:43\u001b[0m 41s/step - loss: 0.9324 - sparse_categorical_accuracy: 0.2549"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1710333140.390153   29836 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 172ms/step - loss: 0.7961 - sparse_categorical_accuracy: 0.2737\n",
      "Epoch 2/2\n",
      "\u001b[1m50/50\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 173ms/step - loss: 0.7824 - sparse_categorical_accuracy: 0.2860\n"
     ]
    }
   ],
   "source": [
    "gemma_lm = finetune_gemma(data=data[:50], model_paths=model_paths_and_config, model_name=model_name, rank_lora=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69ca8bba-4f76-4be7-af5e-d4176e508db4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "\n",
    "def convert_checkpoints(\n",
    "    weights_file, size, vocab_path, output_dir,\n",
    "    convertion_https_dir=\"https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma\",\n",
    "    conversion_script = \"export_gemma_to_hf.py\"\n",
    "):\n",
    "    \"\"\"Downloads the conversion script and runs the Gemma to HuggingFace model conversion. \n",
    "\n",
    "    Args:\n",
    "        f_weights_path (str): Path to the fine-tuned model weights.\n",
    "        model_size (str):  The size of the model (e.g., \"base\", \"large\").\n",
    "        f_vocab_path (str): Path to the fine-tuned vocabulary file.\n",
    "        huggingface_model_dir (str): Output directory for the HuggingFace model.\n",
    "    \"\"\"\n",
    "    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "    # Download the conversion script\n",
    "    if not os.path.exists(conversion_script):\n",
    "        try:\n",
    "            subprocess.run([\"wget\", \"-nv\", \"-nc\", f\"{https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma}/{CONVERSION_SCRIPT}\"], check=True)\n",
    "        except subprocess.SubprocessError as e:\n",
    "            print(f\"Download failed: {e}\")\n",
    "            exit(1) \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Run the conversion script (assuming 'KERAS_BACKEND' is set in the environment)\n",
    "    try:\n",
    "        subprocess.run([\n",
    "            \"python\", \n",
    "            \"export_gemma_to_hf.py\", \n",
    "            \"--weights_file\", weights_file,\n",
    "            \"--size\", size,\n",
    "            \"--vocab_path\", vocab_path,\n",
    "            \"--output_dir\", output_dir\n",
    "        ], check=True)\n",
    "    except subprocess.SubprocessError as e:\n",
    "        print(f\"Conversion failed: {e}\")\n",
    "        exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d1c67c-5fc9-4475-af44-d2427a73964e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d7ea600-005b-4bba-992d-9071211a3307",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-13 13:41:50.614297: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-13 13:41:50.673464: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-13 13:42:23.029736: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-13 13:42:23.031421: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9838 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "/home/jupyter/.local/lib/python3.10/site-packages/keras/src/saving/saving_lib.py:396: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 146 variables. \n",
      "  trackable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Loading Keras weights from file `./gemma_2b_en/model.weights.h5`...\n",
      "\n",
      "-> Loading HuggingFace Gemma `2B` model...\n",
      "\n",
      "âœ… Model loading complete.\n",
      "\n",
      "-> Converting weights from KerasNLP Gemma to HuggingFace Gemma...\n",
      "\n",
      "âœ… Weights converted successfully.\n",
      "\n",
      "-> Saving HuggingFace model to `./gemma_2b_en_huggingface`...\n",
      "\n",
      "âœ… Saving complete. Model saved at `./gemma_2b_en_huggingface`.\n",
      "\n",
      "-> Saving HuggingFace Gemma tokenizer to `./gemma_2b_en_huggingface`...\n",
      "\n",
      "âœ… Saving complete. Tokenizer saved at `./gemma_2b_en_huggingface`.\n",
      "Runtime: 257.6642062664032 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "convert_checkpoints(\n",
    "    weights_file=model_paths_and_config['finetuned_weights_path'],\n",
    "    size=model_paths_and_config['model_size'],\n",
    "    output_dir=model_paths_and_config['huggingface_model_dir'],\n",
    "    vocab_path=model_paths_and_config['finetuned_vocab_path'],\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(\"Runtime:\", runtime, \"seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c2106b2-ef43-4c82-a17a-716d70bcf62b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "\n",
    "def upload_directory(local_directory, bucket_name, destination_subfolder=\"\"):\n",
    "    \"\"\"Uploads a local directory and its contents to a Google Cloud Storage bucket.\n",
    "\n",
    "    Args:\n",
    "        local_directory (str): Path to the local directory.\n",
    "        bucket_name (str): Name of the target Google Cloud Storage bucket.\n",
    "        destination_subfolder (str, optional): Prefix to append to the path within the bucket. \n",
    "                                        Defaults to \"\".\n",
    "    \"\"\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "    for root, _, files in os.walk(local_directory):\n",
    "        for file in files:\n",
    "            local_path = os.path.join(root, file)\n",
    "            # Construct the path within the bucket\n",
    "            blob_path = os.path.join(destination_subfolder, os.path.relpath(local_path, local_directory))\n",
    "            blob = bucket.blob(blob_path)\n",
    "            blob.upload_from_filename(local_path)\n",
    "            print(f\"Uploaded {local_path} to gs://{bucket_name}/{blob_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0eec881c-8a52-48bf-a93c-df567aedd730",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded ./gemma_2b_en_huggingface/tokenizer_config.json to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/tokenizer_config.json\n",
      "Uploaded ./gemma_2b_en_huggingface/config.json to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/config.json\n",
      "Uploaded ./gemma_2b_en_huggingface/special_tokens_map.json to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/special_tokens_map.json\n",
      "Uploaded ./gemma_2b_en_huggingface/model-00003-of-00003.safetensors to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/model-00003-of-00003.safetensors\n",
      "Uploaded ./gemma_2b_en_huggingface/model-00002-of-00003.safetensors to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/model-00002-of-00003.safetensors\n",
      "Uploaded ./gemma_2b_en_huggingface/model.safetensors.index.json to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/model.safetensors.index.json\n",
      "Uploaded ./gemma_2b_en_huggingface/generation_config.json to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/generation_config.json\n",
      "Uploaded ./gemma_2b_en_huggingface/model-00001-of-00003.safetensors to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/model-00001-of-00003.safetensors\n",
      "Uploaded ./gemma_2b_en_huggingface/tokenizer.model to gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610/tokenizer.model\n"
     ]
    }
   ],
   "source": [
    "upload_directory(local_directory = model_paths_and_config['huggingface_model_dir'], bucket_name = BUCKET_NAME, destination_subfolder = model_paths_and_config['deployed_model_blob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea5a28-8c3c-44d7-87aa-1952de6c3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "61b32639-f0ac-459a-924b-db590770a837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    suffix = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d09469ea-bb85-4d3f-ac2b-261e85bdb872",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dde3188-506b-4be4-a814-5b5834d17b45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'machine_type': 'g2-standard-8', 'accelerator_type': 'NVIDIA_L4', 'accelerator_count': 1, 'model_name_vllm': 'gemma_2b_en-vllm'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c07dfde0-7192-48af-9730-d3b692cf878a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com',\n",
       " 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240314144450')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT,model_paths_and_config['deployed_model_uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "251fc433-3657-434e-a64c-6b351bf143ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/24796876098/locations/us-central1/models/3453839801239732224/operations/5586651600438427648\n",
      "Model created. Resource name: projects/24796876098/locations/us-central1/models/3453839801239732224@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/24796876098/locations/us-central1/models/3453839801239732224@1')\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/24796876098/locations/us-central1/endpoints/3872666870003793920/operations/3601514142814437376\n",
      "Endpoint created. Resource name: projects/24796876098/locations/us-central1/endpoints/3872666870003793920\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/24796876098/locations/us-central1/endpoints/3872666870003793920')\n",
      "Deploying model to Endpoint : projects/24796876098/locations/us-central1/endpoints/3872666870003793920\n",
      "Deploy Endpoint model backing LRO: projects/24796876098/locations/us-central1/endpoints/3872666870003793920/operations/772127676918923264\n",
      "Endpoint model deployed. Resource name: projects/24796876098/locations/us-central1/endpoints/3872666870003793920\n"
     ]
    }
   ],
   "source": [
    "max_model_len = 2048\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=model_paths_and_config['model_name_vllm'],\n",
    "    model_uri=model_paths_and_config['deployed_model_uri'],\n",
    "    service_account='gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com',\n",
    "    machine_type=model_paths_and_config['machine_type'],\n",
    "    accelerator_type=model_paths_and_config['accelerator_type'],\n",
    "    accelerator_count=model_paths_and_config['accelerator_count'],\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69dc40ff-d622-423b-8c43-f066e93907e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SERVICE_ACCOUNT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88520f0-174a-4cb7-97c4-41a1fbcaca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "able-analyst-416817-chatbot-v1/gemma_2b_en/20240313143610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b0e2a926-dac3-4dfd-a184-515685b4b657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0f6f329e-9925-42fe-8362-3ca5c97f1cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the plan for tonight?\n",
      "I'm going to the movies with my friends.\n",
      "\n",
      "Sender:\n",
      "What time do you think you'll be back?\n",
      "\n",
      "Andres Perez:\n",
      "I think I'll be back around 10:00.\n",
      "\n",
      "Sender:\n",
      "What time do you think you\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What would you like to drink?\n",
      "I would like a beer.\n",
      "\n",
      "Sender:\n",
      "What kind of beer?\n",
      "\n",
      "Andres Perez:\n",
      "I would like a Corona.\n",
      "\n",
      "Sender:\n",
      "What kind of Corona?\n",
      "\n",
      "Andres Perez:\n",
      "I would like a Corona Light.\n",
      "\n",
      "Sender:\n",
      "What kind of Corona Light\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Are you coming tonight?\n",
      "I'm not sure. I'm not sure.\n",
      "\n",
      "Sender:\n",
      "I'm not sure. I'm not sure.\n",
      "\n",
      "Andres Perez:\n",
      "I'm not sure. I'm not sure.\n",
      "\n",
      "Sender:\n",
      "I'm not sure. I\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "So, what happened?\n",
      "I was in the middle of a conversation with a friend, and I was trying to explain to him why I was so upset. I was trying to explain to him why I was so angry. I was trying to explain to him why I was so upset. I was trying to\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Bro\n",
      "I am a 20 year old student from the University of Puerto Rico, Rio Piedras Campus. I am currently studying to become a teacher. I am a very active person and I love to travel. I am a very outgoing person and I love to meet new people.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Was she hot?\n",
      "I don't know. I don't know.\n",
      "\n",
      "Sender:\n",
      "I don't know. I don't know.\n",
      "\n",
      "Andres Perez:\n",
      "I don't know. I don't know.\n",
      "\n",
      "Sender:\n",
      "I don't know. I\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"What is the plan for tonight?\",\n",
    "    \"What would you like to drink?\",\n",
    "    \"Are you coming tonight?\",\n",
    "    \"So, what happened?\",\n",
    "    \"Bro\",\n",
    "    'Was she hot?'\n",
    "]\n",
    "\n",
    "# Prompt template for the training data and the finetuning tests\n",
    "PROMPT_TEMPLATE = \"Sender:\\n{instruction}\\n\\nAndres Perez:\\n{response}\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    PROMPT_TEMPLATE.format(instruction=example, response=\"\")\n",
    "    for example in TEST_EXAMPLES\n",
    "]\n",
    "\n",
    "def test_vertexai_endpoint(endpoint: aiplatform.Endpoint):\n",
    "    for question, prompt in zip(TEST_EXAMPLES, TEST_PROMPTS):\n",
    "        instance = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 56,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 1,\n",
    "            \"raw_response\": True,\n",
    "        }\n",
    "        response = endpoint.predict(instances=[instance])\n",
    "        output = response.predictions[0]\n",
    "        print(f\"{question}\\n{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_vertexai_endpoint(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0cec9e65-39a7-4117-a112-d82a83e91cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create artifact repository.\n",
    "ARTIFACT_REGISTRY=\"gemma-chatbot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2911da47-ef52-4f0d-97ef-988c7a30cf6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create request issued for: [bert-sentiment-classifier]\n",
      "Waiting for operation [projects/able-analyst-416817/locations/us-central1/opera\n",
      "tions/5d3aaf30-7a60-4c44-97e4-b6fbe08ba0ad] to complete...done.                \n",
      "Created repository [bert-sentiment-classifier].\n"
     ]
    }
   ],
   "source": [
    "!gcloud artifacts repositories create $ARTIFACT_REGISTRY \\\n",
    "    --repository-format=docker \\\n",
    "    --location=$REGION \\\n",
    "    --description=\"A Docker repository for my chatbot project Gemma family\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "518bd694-aa5b-45f4-934a-894c33b2bf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cee8bf5-820b-4c11-8c17-c667cd2a69a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd38dea-1f7f-486d-bf3e-1b3a1f69582c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6c518-79e1-4ec6-a832-39ae6f364ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a33de96-dc3d-4c11-bf93-39a577149e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1cae7-c300-4cd1-aa97-c7fef2efaa5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05aa3f-9a40-41db-8553-b65d7dee4352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a998bf0-63a8-4c0e-9931-2387986375c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5470051c-6e9c-46ce-b7d7-ed413fd8a319",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m118"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
