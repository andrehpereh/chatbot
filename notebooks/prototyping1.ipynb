{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f178cfef-2782-4a77-b4f8-7d372a7c61f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "SERVICE_ACCOUNT = 'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'\n",
    "from datetime import datetime\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "GCP_REGION='us-central1'\n",
    "TAG_NAME = 'masterv9'\n",
    "KAGGLE_USERNAME='andrehpereh1'\n",
    "# KAGGLE_KEY=''\n",
    "CONTAINER_IMAGE_NAME_DATA_PREP = f\"{CONTAINER_IMAGE_NAME}-data-preparation\"\n",
    "CONTAINER_IMAGE_NAME_FINE_TUNE = f\"{CONTAINER_IMAGE_NAME}-fine-tunning\"\n",
    "CONTAINER_IMAGE_NAME_RUN_APP = f\"{CONTAINER_IMAGE_NAME}-running-app\"\n",
    "CONTAINER_IMAGE_NAME_PIPELINE = f\"{CONTAINER_IMAGE_NAME}-pipeline-app\"\n",
    "CONTAINER_IMAGE_RUNNING_APP = f\"{CONTAINER_IMAGE_NAME}-running-app\"\n",
    "BUCKET_NAME = 'able-analyst-416817-chatbot-v1'\n",
    "# BUCKET_URI = f\"gs://{BUCKET_NAME}\"\n",
    "FINE_TUNE_FLAG=  False\n",
    "EPOCHS=12\n",
    "MODEL_NAME=\"gemma_2b_en\"\n",
    "\n",
    "substitutions=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME_PIPELINE={},\\\n",
    "TAG_NAME={},\\\n",
    "_BUCKET_NAME={},\\\n",
    "_FINE_TUNE_FLAG={},\\\n",
    "_EPOCHS={},\\\n",
    "_MODEL_NAME={}\n",
    "\"\"\".format(\n",
    "           CONTAINER_IMAGE_NAME_PIPELINE,\n",
    "           TAG_NAME,\n",
    "           BUCKET_NAME,\n",
    "           FINE_TUNE_FLAG,\n",
    "           EPOCHS,\n",
    "           MODEL_NAME\n",
    "           ).strip()\n",
    "print(substitutions)\n",
    "!gcloud builds submit . --config \"components/pipeline/cloudbuild.yaml\" --substitutions {substitutions} --region={GCP_REGION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f946012-e7f1-4438-ad6e-d662efe3f040",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0b11ab-4f76-406e-87a8-5a7970c5fec2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "GCP_REGION = 'us-central1'\n",
    "TAG_NAME = 'masterv4'\n",
    "\n",
    "CONTAINER_IMAGE_NAME_PIPELINE = f\"{CONTAINER_IMAGE_NAME}-pipeline-app\"\n",
    "CONTAINER_IMAGE_RUNNING_APP = f\"{CONTAINER_IMAGE_NAME}-running-app\"\n",
    "\n",
    "#,_CONTAINER_IMAGE_RUNNING_APP={CONTAINER_IMAGE_RUNNING_APP}\n",
    "substitutions=f\"_CONTAINER_IMAGE_NAME_PIPELINE={CONTAINER_IMAGE_NAME_PIPELINE},TAG_NAME={TAG_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f135a2-2060-418d-b9aa-98d86181abac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit . --config \"./cloudbuil_cloudrun.yaml\" --substitutions {substitutions} --region=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b8ed60-33ed-4ea9-888b-e62721c8728c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "!gcloud functions deploy hello_world_trigger \\\n",
    "--gen2 \\\n",
    "--region='us-central1' \\\n",
    "--runtime=python39 \\\n",
    "--source='./components/cloud_functions/' \\\n",
    "--entry-point='trigger_pipeline_cloud_function' \\\n",
    "--trigger-topic='your-pipeline-trigger-topic' \\\n",
    "--memory='2048MB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5b43c6-efab-4e39-b7a2-0cb7367badce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kfp\n",
    "kfp.__version__\n",
    "dir(kfp.dsl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978a7bfb-2182-44f1-a764-67e428f92352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from components.cloud_functions.main import trigger_pipeline_cloud_function\n",
    "import json\n",
    "\n",
    "data = {\n",
    "    \"user_name\": \"andrehpereh\",\n",
    "    \"files\": [\n",
    "        {\"file_path\": \"gs://personalize-chatbots-v1/andrehpereh/input_data/WhatsApp Chat with Anki.txt\", \"filename\": \"WhatsApp Chat with Anki.txt\"},\n",
    "        {\"file_path\": \"gs://personalize-chatbots-v1/andrehpereh/input_data/WhatsApp Chat with Ilse Flatmate.txt\", \"filename\": \"WhatsApp Chat with Ilse Flatmate.txt\"},\n",
    "        {\"file_path\": \"gs://personalize-chatbots-v1/andrehpereh/input_data/WhatsApp Chat with Michael.txt\", \"filename\": \"WhatsApp Chat with Michael.txt\"}\n",
    "    ],\n",
    "    \"blob_folder\": \"andrehpereh/input_data\",\n",
    "    \"model_name\": \"gemma_2b_en\",\n",
    "    \"epochs\": \"12\",\n",
    "    \"bucket_name\": \"personalize-chatbots-v1\",\n",
    "    \"project_id\": \"able-analyst-416817\"\n",
    "}\n",
    "\n",
    "# Convert dictionary to JSON string\n",
    "json_data = json.dumps(data)\n",
    "\n",
    "# Encode JSON string to bytes\n",
    "data_bytes = json_data.encode('utf-8')\n",
    "\n",
    "# Pass the encoded bytes to the function\n",
    "trigger_pipeline_cloud_function()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da46322-3139-4427-b552-1f444750a378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"andrehpereh1\"\n",
    "# os.environ['KAGGLE_KEY'] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508aa69-1810-4fc2-9a3d-735e55e4ccf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import util\n",
    "importlib.reload(util)\n",
    "model_paths_and_config = util.get_model_paths_and_config(Config.MODEL_NAME)\n",
    "model_paths_and_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69027026-fb4f-4de3-9e5c-84f7ec881d29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Config.TRAIN_DATA_DIR, Config.BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b737395-4d42-4a7e-becf-9fbc07a2580d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = process_whatsapp_chat(Config.BUCKET_NAME, Config.TRAIN_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc1897-d11a-4e8c-852c-af78ad7656cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04ebef-6bad-4c2d-aced-34083491afce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_paths_and_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67913c94-353d-43f3-8026-827aafdbbdbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_weights_path = finetune_gemma(data=data[:50], model_paths=model_paths_and_config, fine_tune_flag=False, model_name=Config.MODEL_NAME, rank_lora=Config.SEQUENCE_LENGTH, sequence_length=Config.SEQUENCE_LENGTH, epochs=Config.EPOCHS, batch_size=Config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6d2d38-4f7b-4eca-bf72-31027d060ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "cuda.select_device(device.id)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762d8ec4-0a5c-4718-a599-34b74305715c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "type(finetuned_weights_path)\n",
    "print(model_paths_and_config['finetuned_weights_path'])\n",
    "print(model_paths_and_config['finetuned_model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8d15a7-6344-4faf-8a22-69f203a74676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil cp gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5 ./test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fca7dfb-6fde-4a9f-b879-9be138a35564",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "finetuned_weights_path.save_weights(\"gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/test/model.weights.h5\")\n",
    "finetuned_weights_path.preprocessor.tokenizer.save_assets(\"gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511c2ed7-d7fc-4c1b-a73f-e44940701a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_dir = convert_checkpoints(\n",
    "    weights_file=model_paths_and_config['finetuned_weights_path'],\n",
    "    size=model_paths_and_config['model_size'],\n",
    "    output_dir=model_paths_and_config['huggingface_model_dir'],\n",
    "    vocab_path=model_paths_and_config['finetuned_vocab_path'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202aae46-9fdc-40f6-9d3e-d4f7cb50278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=model_paths_and_config['huggingface_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221ec4a-8da6-463b-943d-c4d66b9615df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_paths_and_config, Config.BUCKET_NAME\n",
    "bucket_name, blob_name = os.path.dirname(\"gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5\").lstrip(\"gs://\").split(\"/\", 1) \n",
    "bucket_name, blob_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9b7e0c-e4bd-4ed8-95a8-16eac527e81d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.path.basename(\"gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad8c09-c59c-4424-8165-24e5716740e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = upload2bs(local_directory = output_dir, bucket_name = Config.BUCKET_NAME, destination_subfolder = model_paths_and_config['deployed_model_blob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ac8551-f497-4b79-b6bd-41b5034c3011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import util\n",
    "importlib.reload(util)\n",
    "\n",
    "util.download_all_from_blob(Config.BUCKET_NAME, \"gemma_2b_en_raw/gemma_2b_en\", './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16877b-31ae-44c2-8b07-d90e5ad9e849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Config.BUCKET_NAME\n",
    "model_paths_and_config['huggingface_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9a6a6-9cf8-4fb4-a9e2-69f867cc31c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_paths_and_config['deployed_model_blob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea5a28-8c3c-44d7-87aa-1952de6c3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=Config.PROJECT_ID, location=Config.REGION, staging_bucket=Config.BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b32639-f0ac-459a-924b-db590770a837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    suffix = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07dfde0-7192-48af-9730-d3b692cf878a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up artificially since the model was already in a bucket\n",
    "model_paths_and_config['deployed_model_uri'] = 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240314162107'\n",
    "model_paths_and_config['model_name_vllm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251fc433-3657-434e-a64c-6b351bf143ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_model_len = 2048\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=model_paths_and_config['model_name_vllm'],\n",
    "    model_uri=model_paths_and_config['deployed_model_uri'],\n",
    "    service_account=Config.SERVICE_ACCOUNT,\n",
    "    machine_type=model_paths_and_config['machine_type'],\n",
    "    accelerator_type=model_paths_and_config['accelerator_type'],\n",
    "    accelerator_count=model_paths_and_config['accelerator_count'],\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc40ff-d622-423b-8c43-f066e93907e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88520f0-174a-4cb7-97c4-41a1fbcaca81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e2a926-dac3-4dfd-a184-515685b4b657",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f329e-9925-42fe-8362-3ca5c97f1cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"What is the plan for tonight?\",\n",
    "    \"What would you like to drink?\",\n",
    "    \"Are you coming tonight?\"\n",
    "]\n",
    "\n",
    "# Prompt template for the training data and the finetuning tests\n",
    "PROMPT_TEMPLATE = \"Sender:\\n{instruction}\\n\\nAndres Perez:\\n{response}\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    PROMPT_TEMPLATE.format(instruction=example, response=\"\")\n",
    "    for example in TEST_EXAMPLES\n",
    "]\n",
    "\n",
    "def test_vertexai_endpoint(endpoint: aiplatform.Endpoint):\n",
    "    for question, prompt in zip(TEST_EXAMPLES, TEST_PROMPTS):\n",
    "        instance = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 56,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 1,\n",
    "            \"raw_response\": True,\n",
    "        }\n",
    "        response = endpoint.predict(instances=[instance])\n",
    "        output = response.predictions[0]\n",
    "        print(f\"{question}\\n{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_vertexai_endpoint(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518bd694-aa5b-45f4-934a-894c33b2bf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "SERVICE_ACCOUNT = 'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'\n",
    "from datetime import datetime\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "GCP_REGION='us-central1'\n",
    "IMAGE_NAME=\"gemma-chatbot\"\n",
    "TAG_NAME = 'latest'\n",
    "KAGGLE_USERNAME='andrehpereh1'\n",
    "# KAGGLE_KEY=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d98f42-6a85-4324-81a7-dd1c6b5bab4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-data-preparation\",\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd38dea-1f7f-486d-bf3e-1b3a1f69582c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Runs the data_preparation component image. (Development, when tested should be moved to the main cloudbuild in the project folder)\n",
    "# Pay attention to the \".\" after summit. Might need some changes when move to the master pipeline.\n",
    "!gcloud builds submit . --timeout=15m --config \"components/data_preparation/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "# DO not forget the tag\n",
    "#!docker run gcr.io/able-analyst-416817/gemma-chatbot-data-preparation:latest data_ingestion.py --bucket-name 'able-analyst-416817-chatbot-v1' --directory 'input_data/andrehpereh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0d0f14-b406-4a5a-bc6b-fa4e8da5cc62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9250f9bf-5bf0-4bd2-8f5f-3f10daf9dc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_KAGGLE_USERNAME={},\\\n",
    "_KAGGLE_KEY={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-fine-tunning\",\n",
    "           KAGGLE_USERNAME,\n",
    "           KAGGLE_KEY,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "\n",
    "# Builds image\n",
    "!gcloud builds submit . --config \"components/fine_tunning/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef60c94-45b0-4cf2-a7d1-cf96b3f6ad6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0452fcec-bd44-4ab8-801b-e6f804b5a432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961b567-3239-4c10-915f-67fd98515c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = [\"Sender: FoooodddAndres Perez: Coming :)\", \"Sender: Can I maybe borrow your iron? Andres Perez: It\\'s not my iron But yeah haha Or is it?\"]\n",
    "model_paths = \"\"\"{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"}\"\"\"\n",
    "print(len(data))\n",
    "model_paths = json.dumps(model_paths)\n",
    "#!docker run gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest trainer.py --data {data} --model-paths {model_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090fa27a-121a-4954-8ddc-973415226bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python ./components/fine_tunning/trainer.py --data {data} --model-paths {model_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb2aa69-a793-4a4f-b8e4-92731c92b6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_paths_and_config['huggingface_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d84c44-c65c-4ae1-9a4c-6d293f139495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python ./components/fine_tunning/conversion_function.py --weights-file {model_paths_and_config['finetuned_weights_path']} --size {model_paths_and_config['model_size']} --vocab-path {model_paths_and_config['finetuned_vocab_path']} --output-dir {model_paths_and_config['huggingface_model_dir']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6c134-bd9a-4fda-8e29-31ef6c8aea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = convert_checkpoints(\n",
    "    weights_file=model_paths_and_config['finetuned_weights_path'],\n",
    "    size=model_paths_and_config['model_size'],\n",
    "    output_dir=model_paths_and_config['huggingface_model_dir'],\n",
    "    vocab_path=model_paths_and_config['finetuned_vocab_path'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6404c6c-d998-4253-bbf3-3f83b29abe94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-experimental\",\n",
    "           TAG_NAME,\n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "data = '\"[\\\\\"Sender: FoooodddAndres Perez: Coming :)\\\\\", \\\\\"Sender: Can I maybe borrow your iron? Andres Perez: It\\'s not my iron But yeah haha Or is it?\\\\\"]\"'\n",
    "model_paths = \"\"\"{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"}\"\"\"\n",
    "data_json = json.dumps(data)\n",
    "model_paths_json = json.dumps(model_paths)\n",
    "#!gcloud builds submit . --timeout=15m --config \"components/experimental/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "!docker run gcr.io/able-analyst-416817/gemma-chatbot-experimental:latest experimental.py {data_json} {model_paths_json}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff6c518-79e1-4ec6-a832-39ae6f364ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Re-runs the image to restart the website, service account might be needed with this one.  (Development, when tested should be moved to the main cloudbuild in the project folder)\n",
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_GCP_REGION={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-running-app\",\n",
    "           GCP_REGION,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "!gcloud builds submit . --timeout=15m --config \"components/app_flask/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "#!gcloud builds submit . --timeout=15m --config \"cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1cae7-c300-4cd1-aa97-c7fef2efaa5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05aa3f-9a40-41db-8553-b65d7dee4352",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_GCP_REGION={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-running-app\",\n",
    "           GCP_REGION,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "!gcloud builds submit . --timeout=15m --config \"components/app_flask/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "#!gcloud builds submit . --timeout=15m --config \"cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb10f3c-b135-470b-a74b-0c825e0f0761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa1979-e2f1-41ce-a3f4-bab8465866e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b974a3-cab2-425e-bfc5-dcb01d45b5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from util import get_model_paths_and_config, upload2bs\n",
    "\n",
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "from google.cloud import compute_v1\n",
    "\n",
    "project = PROJECT_ID  # Replace with your project ID\n",
    "\n",
    "# Regions to consider\n",
    "regions = [\"us-central1\", \"europe-west4\", \"asia-east1\"]  \n",
    "\n",
    "client = compute_v1.AcceleratorTypesClient()\n",
    "\n",
    "for region in regions:\n",
    "    zone_client = compute_v1.ZonesClient()\n",
    "    all_zones = zone_client.list(project=project)\n",
    "\n",
    "    zone_list = [zone for zone in all_zones if zone.region == f\"regions/{region}\"]\n",
    "\n",
    "    for zone in zone_list:\n",
    "        result = client.describe(\n",
    "            project=project, zone=zone.name, accelerator_type=\"nvidia-l4\"\n",
    "        )\n",
    "        # If the result isn't an error, the 'nvidia-l4' type is available\n",
    "        GCP_REGION = region  # Update your region variable\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11402c19-d1b8-4b8c-a959-932ca09fcf18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7ac36-bc0e-4e00-928f-1c7247b9685e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09398956-e97e-411b-a1fe-9e05ff61e6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!bq mk chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a52d27-b74b-4211-b650-7f16444fe8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# Construct a BigQuery client object\n",
    "os.environ['PROJECT_ID'] = 'able-analyst-416817'\n",
    "client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
    "\n",
    "# Define your dataset and table information\n",
    "project_id = os.environ.get('PROJECT_ID')\n",
    "project = os.environ.get('PROJECT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdfdea-c57b-4a26-ac5f-2775ff291bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset_id = \"chatbot\"\n",
    "table_id = \"users\" \n",
    "\n",
    "# Schema definition\n",
    "schema = [\n",
    "    # bigquery.SchemaField(\"user_id\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"email\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"password_hash\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", description=\"Record creation timestamp\")  \n",
    "    #bigquery.SchemaField(\"salt\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    #bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", mode=\"REQUIRED\"),\n",
    "    #bigquery.SchemaField(\"last_login\", \"TIMESTAMP\", mode=\"NULLABLE\") \n",
    "]\n",
    "\n",
    "# Create a table object\n",
    "table = bigquery.Table(project_id + \".\" + dataset_id + \".\" + table_id, schema=schema)\n",
    "\n",
    "# Create the table in BigQuery\n",
    "table = client.create_table(table)  \n",
    "print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41c1b33-ba5a-4b0f-aef8-c7a872f1a6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_id = \"chatbot\"\n",
    "table_id = \"user_training_status\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed69c43-8407-460b-bd2d-1ac0ed39a610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Schema definition\n",
    "schema = [\n",
    "    bigquery.SchemaField(\"email\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"training_status\", \"BOOL\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"end_point\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", description=\"Record creation timestamp\")  \n",
    "]\n",
    "\n",
    "# Create a table object\n",
    "table = bigquery.Table(project_id + \".\" + dataset_id + \".\" + table_id, schema=schema)\n",
    "\n",
    "# Create the table in BigQuery\n",
    "table = client.create_table(table)  \n",
    "print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f319dab7-7bf8-4346-9d43-952f158d9fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_id = \"chatbot\"\n",
    "table_id = \"users_endpoints\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8af8d90-a372-40a9-a08b-5284ad58321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = [\n",
    "    bigquery.SchemaField(\"email\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"training_status\", \"BOOL\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"end_point\", \"STRING\", mode=\"NULLABLE\"),\n",
    "    bigquery.SchemaField(\"created_at\", \"TIMESTAMP\", description=\"Record creation timestamp\")  \n",
    "]\n",
    "\n",
    "# Create a table object\n",
    "table = bigquery.Table(project_id + \".\" + dataset_id + \".\" + table_id, schema=schema)\n",
    "\n",
    "# Create the table in BigQuery\n",
    "table = client.create_table(table)  \n",
    "print(\"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc4f405-127a-44db-8937-76a56866689a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5b4e23-227e-40bb-9158-b4b5b5951a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "#This part can be wrapped in a function.\n",
    "query = f\"\"\"\n",
    "UPDATE `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USER_TRAINING_STATUS}`\n",
    "SET end_point = '{endpoint_resource}'  \n",
    "WHERE email = '{email}' \n",
    "\"\"\"\n",
    "# Create a query job configuration\n",
    "job_config = bigquery.QueryJobConfig()\n",
    "\n",
    "# Execute the update query\n",
    "query_job = client.query(query, job_config=job_config)\n",
    "query_job.result() \n",
    "\n",
    "print(\"End point has been stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f9bed4-ff5d-4bc7-b8bf-dde6ec82ac09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0cca0-72a6-42d8-b738-328e0add6fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe300209-7695-45e5-8f3e-79f49969d4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1eaf461-bbfa-467b-86e4-88e62caea3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import bigquery\n",
    "DATASET_ID = 'chatbot' # This should be moved to a config file\n",
    "USER_TRAINING_STATUS = 'user_training_status' # This should be moved to a config file\n",
    "#This part can be wrapped in a function\n",
    "email = 'andreshperesh@gmail.com'\n",
    "\n",
    "resource_uri = \"https://us-central1-aiplatform.googleapis.com/v1/projects/24796876098/locations/us-central1/endpoints/2572265671740096512/operations/6634221199406661632\"\n",
    "\n",
    "print(\"This is the passed end pooint\", resource_uri)\n",
    "\n",
    "print(\"This is the project\", project)\n",
    "\n",
    "client = bigquery.Client(project)\n",
    "print(\"This is the client\", client)\n",
    "table_ref = client.dataset(DATASET_ID).table(USER_TRAINING_STATUS)\n",
    "table = client.get_table(table_ref)\n",
    "row_to_insert = {\n",
    "    'email': email,\n",
    "    'end_point': resource_uri,\n",
    "    'training_status': True\n",
    "}\n",
    "client.insert_rows(table, [row_to_insert]) \n",
    "errors = client.insert_rows(table, [row_to_insert])\n",
    "if errors:  # Check if there were errors\n",
    "    print(\"The model has been trained, but error updating resource_uri for {}: {}\".format(email, errors))\n",
    "else:\n",
    "    print(\"User training has been updated\")\n",
    "print(\"End point has been stored.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee126b2-b9c1-4c46-93e1-4e92a861ce05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import re  \n",
    "\n",
    "# ... rest of your imports ...\n",
    "\n",
    "def predict_custom_trained_model_sample(\n",
    "Â Â project: str,\n",
    "Â Â endpoint_id: str,\n",
    "Â Â location: str = \"us-central1\",\n",
    "Â Â api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "Â Â user_input: str = input\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    `instances` can be either single instance of type dict or a list\n",
    "    of instances.\n",
    "    \"\"\"\n",
    "    logging.basicConfig(level=logging.INFO)  # Set up basic logging\n",
    "\n",
    "    prompt_input = f\"Sender:\\n{user_input}\\n\\nAndres Perez:\\n\",\n",
    "    instances = {'prompt': prompt_input[0], 'max_tokens': 256, 'temperature': 1.4, 'top_p': 0.8, 'top_k': 4}\n",
    "\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "\n",
    "    try: \n",
    "        # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "        instances = instances if isinstance(instances, list) else [instances]\n",
    "        instances = [\n",
    "          json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "        ]\n",
    "        parameters_dict = {}\n",
    "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "        endpoint = client.endpoint_path(\n",
    "          project=project, location=location, endpoint=endpoint_id\n",
    "        )\n",
    "\n",
    "        response = client.predict(\n",
    "          endpoint=endpoint, instances=instances, parameters=parameters\n",
    "        )\n",
    "\n",
    "        # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "        predictions = response.predictions\n",
    "        pattern = r\"Perez:\\nOutput:\\n(.*)\"\n",
    "        match = re.search(pattern, predictions[0])\n",
    "\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        logging.warning(\"Prediction not found in the response\")\n",
    "        return \"Error: Prediction not found in the response.\"\n",
    "\n",
    "    except Exception as e:  # Log any exceptions\n",
    "        logging.error(\"Prediction failed: %s\", e) \n",
    "        return \"Error: Problem generating response.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bf6c94-56d4-4e74-8364-00e2af0a26a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "string = \"https://us-central1-aiplatform.googleapis.com/v1/projects/24796876098/locations/us-central1/endpoints/2572265671740096512/operations/6634221199406661632\"\n",
    "\n",
    "pattern = r\"\\/projects\\/([^\\/]+)\\/locations\\/([^\\/]+)\\/endpoints\\/([^\\/]+)\\/operations\\/([^\\/]+)\"\n",
    "\n",
    "match = re.search(pattern, string)\n",
    "\n",
    "if match:\n",
    "    project_id = match.group(1)\n",
    "    location = match.group(2)\n",
    "    endpoint_id = match.group(3)\n",
    "    operation_id = match.group(4)\n",
    "\n",
    "    print(\"Project ID:\", project_id)\n",
    "    print(\"Location:\", location)\n",
    "    print(\"Endpoint ID:\", endpoint_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9eac52-5aa7-4fd9-8e16-a22323f5a2cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Wednesday at 12-33 AM.txt', 'r') as file:\n",
    "    contents = file.read()\n",
    "    print(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90da5fe5-1df8-4391-ba2c-f97b97b8cf1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metal-sky-419309\n"
     ]
    }
   ],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "import os\n",
    "print(PROJECT_ID)\n",
    "os.environ['PROJECT_ID'] = PROJECT_ID\n",
    "project_id = PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc3ad00a-7aaf-4cb0-ad67-2bc700e1bf68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vertexai\n",
    "from vertexai.generative_models import GenerativeModel, ChatSession\n",
    "\n",
    "project_id = project_id\n",
    "location = \"us-central1\"\n",
    "vertexai.init(project=project_id, location=location)\n",
    "model = GenerativeModel(\"gemini-1.5-pro-preview-0409\")\n",
    "chat = model.start_chat()\n",
    "\n",
    "def get_chat_response(chat: ChatSession, prompt: str) -> str:\n",
    "    text_response = []\n",
    "    responses = chat.send_message(prompt, stream=True)\n",
    "    for chunk in responses:\n",
    "        text_response.append(chunk.text)\n",
    "    return \"\".join(text_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c575c169-6b59-4256-b8de-4f51495dc5dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The colors of the rainbow, in order, are: red, orange, yellow, green, blue, indigo, and violet. You can remember them with the acronym ROY G. BIV! ðŸŒˆ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What are all the colors in a rainbow?\"\n",
    "print(get_chat_response(chat, prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c2b047b-3260-4ce1-b71d-57182295a83c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "yes = \"\"\"Write a well writen article based on the following Python code. The article is intended a tutorial for an advance-level Python audience. The article  about  a chatbot described briefly below. Include as many technical details as possible and expand concepts involved, be creative.\n",
    "\n",
    "\"I turned myself into a chatbot. Go chat with my virtual bot.\n",
    "\n",
    "Do you know that feeling when you're learning about machine learning and you want to go beyond tutorials? That's how I felt, and it led me to build a website where anyone can create their own custom chatbot!\n",
    "\n",
    "This project has been an awesome way to get my hands on #LLMs, #finetunning, #Kubernetes, #Containers, #MachineLearning, #Pipelineorchestration, #GoogleCloud, and #chatbots:\n",
    "\n",
    "#Python + #GoogleCloud:\n",
    "Python has been the backbone of this entire project. Here's how it integrates with the Google Cloud tools I've been using:\n",
    "\n",
    "#VertexAI: My go-to for building, training, and deploying those chatbot models.\n",
    "#KFP (KufeFlow Pipelines): KFP helped me orchestrate the complex machine learning pipelines involved in training the chatbot.\n",
    "#BigQuery: lets me interact with and manage user data.\n",
    "#CloudStorage: Where all the raw conversations that train the chatbots live.\n",
    "#GemmaModels: I leveraged Gemma pre-trained models as a starting point for fine-tuning the chatbot's responses.\n",
    "#GeminiAPI: Turns my transcripts into the kind of data the system can learn from.\n",
    "#Containers #Docker: I containerized each component of the pipeline for the chatbot to make it portable and scalable across different environments.\n",
    "#CloudFunctions: These serverless functions were used to trigger the chatbot training pipeline whenever a new user registered.\n",
    "#Pub/Sub: The backbone of communication, letting my system seamlessly notify components when a new chatbot is born.\n",
    "#Flask: For that friendly website where the magic happens.\n",
    "#CloudBuild & #GitHub: Streamlining my code management and deployment has added a whole new layer of professionalism to the project.\n",
    "\n",
    "Finally, I signed up on the website and fed it with a few of my old transcripts into the system to see what it would come up with. Turns out, it's kind of weird to have an AI version of yourself!\n",
    "\n",
    "The full code is below. Write the article well articulated, link the different topics, components and make a nice story telling about it. Make it as detailed as possible and include as many technical aspects as possible\n",
    "Include, details in the code. Include additional information not present in the text, but relevant for the use case. Expand in the details of the codebase\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f98a60e9-1ed7-4961-b724-5244930d74f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc_prompt = \"\"\"The following text containes the codebase of a project which is split by files, improve the documentation of each file by adding a detailed description of the file\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c625f7a-95bb-4a0a-853e-ea3ef112482d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def scan_and_append_text(directory_path, output_file=\"gigantic_code.txt\"):\n",
    "    \"\"\"Scans a directory recursively, reads code files, and appends content to an output file with filename separators.\n",
    "\n",
    "    Args:\n",
    "        directory_path (str): The path to the directory to scan.\n",
    "        output_file (str, optional): The name of the output file. Defaults to \"gigantic_code.txt\".\n",
    "    \"\"\"\n",
    "\n",
    "    with open(output_file, 'a') as output:  # Open output file in append mode\n",
    "        for root, _, files in os.walk(directory_path):\n",
    "            for file in files:\n",
    "                if file.endswith(('.py', '.java', '.js', '.cpp', '.c', 'html', 'css', 'Dockerfile')):  # Add your code file extensions\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    try:\n",
    "                        output.write(f'\\n--- File: {file_path} ---\\n')  # File separator with name\n",
    "                        with open(file_path, 'r') as f:\n",
    "                            res = get_chat_response(chat, doc_prompt + f.read()) if file.endswith(('.andreh')) else f.read()\n",
    "                            output.write(res)  # Read and append\n",
    "                    except (IOError, OSError) as e:\n",
    "                        print(f\"Error reading file: {file_path} - {e}\")\n",
    "# Example usage\n",
    "directory_to_scan = \"./\" \n",
    "scan_and_append_text(directory_to_scan)\n",
    "with open(\"gigantic_code.txt\", 'r') as f:\n",
    "    all_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "09fe78fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- File: ./cluodbuild_compiler.py ---\n",
      "## Improved Documentation and Logging for Cloud Build File Merger\n",
      "\n",
      "```python\n",
      "import json\n",
      "import yaml\n",
      "import re\n",
      "\n",
      "def merge_cloudbuild_files(child_files, descriptions, master_filepath=\"master_cloudbuild.json\", timeout_hours = 60 * 60):\n",
      "    \"\"\"\n",
      "    Merges multiple Cloud Build YAML files into a single master JSON file, adding component descriptions and calculating required substitutions.\n",
      "\n",
      "    This function combines individual Cloud Build configurations defined in separate YAML files into a single master JSON file. \n",
      "    It also adds descriptive comments for each component and identifies necessary substitutions based on referenced variables.\n",
      "\n",
      "    Args:\n",
      "        child_files (list): A list of paths to the child Cloud Build YAML files.\n",
      "        descriptions (list): A list of descriptions corresponding to each child file, providing context for each component.\n",
      "        master_filepath (str, optional): The desired filepath for the output master JSON file. Defaults to \"master_cloudbuild.json\".\n",
      "        timeout_hours (int, optional): The timeout for the master build in hours. Defaults to 60 hours.\n",
      "\n",
      "    Returns:\n",
      "        set: A set of substitution variables required in the master build configuration.\n",
      "\n",
      "    Raises:\n",
      "        FileNotFoundError: If any of the child files cannot be found.\n",
      "        ValueError: If any of the child files is not valid YAML, the number of descriptions doesn't match the number of child files,\n",
      "                    or a child file has an invalid Cloud Build format (missing 'steps' key).\n",
      "\n",
      "    \"\"\"\n",
      "    substitutions = []\n",
      "    variable_pattern = re.compile(r'\\$_[A-Z_]+')  # Pattern to match substitution variables\n",
      "\n",
      "    if len(child_files) != len(descriptions):\n",
      "        raise ValueError(\"Number of descriptions must match the number of child files\")\n",
      "\n",
      "    master_config = {'steps': []}\n",
      "\n",
      "    for child_file, description in zip(child_files, descriptions):\n",
      "        try:\n",
      "            with open(child_file, 'r') as f:\n",
      "                child_config = yaml.safe_load(f)\n",
      "\n",
      "            if 'steps' not in child_config:\n",
      "                raise ValueError(f\"Invalid Cloud Build format in '{child_file}': missing 'steps' key\")\n",
      "\n",
      "            # Extract variables and add to substitutions list\n",
      "            variables = variable_pattern.findall(json.dumps(child_config))\n",
      "            substitutions.extend(variable[1:] for variable in variables)\n",
      "\n",
      "            # Add description as a comment before the steps\n",
      "            master_config['steps'].append({'name': 'bash', 'args': [f'echo --- {description} ---']})\n",
      "\n",
      "            # Append steps from child config to master config\n",
      "            master_config['steps'].extend(child_config['steps'])\n",
      "\n",
      "        except FileNotFoundError:\n",
      "            raise FileNotFoundError(f\"Child Cloud Build file not found: '{child_file}'\")\n",
      "        except yaml.YAMLError as e:\n",
      "            raise ValueError(f\"Invalid YAML in '{child_file}': {e}\")\n",
      "\n",
      "    # Set timeout for master build\n",
      "    master_config['timeout'] = f'{timeout_hours * 60 * 60}s'\n",
      "\n",
      "    # Write master config to JSON file\n",
      "    with open(master_filepath, 'w') as f:\n",
      "        json.dump(master_config, f, indent=2)\n",
      "\n",
      "    # Log the generated substitutions for debugging/information\n",
      "    print(f\"Substitutions required: {set(substitutions)}\")\n",
      "\n",
      "    return set(substitutions)\n",
      "\n",
      "def find_missing_elements(my_set, long_string):\n",
      "    \"\"\"\n",
      "    Finds elements from a set that are not present in a comma-separated string.\n",
      "\n",
      "    This function is used to identify missing substitutions from a set of required substitutions and a given string of provided substitutions.\n",
      "\n",
      "    Args:\n",
      "        my_set (set): A set of strings to check for presence.\n",
      "        long_string (str): A comma-separated string that might contain elements from the set.\n",
      "\n",
      "    Returns:\n",
      "        list: A list of elements from the set that are missing in the long string.\n",
      "\n",
      "    \"\"\"\n",
      "    missing_elements = set(my_set)\n",
      "    for part in long_string.split(','):\n",
      "        for element in part.split('='):\n",
      "            element = element.strip()\n",
      "            if element in missing_elements:\n",
      "                missing_elements.remove(element)\n",
      "    return list(missing_elements)\n",
      "```\n",
      "\n",
      "**Improvements:**\n",
      "\n",
      "* **Detailed Docstring:** The docstring now provides a comprehensive explanation of the function's purpose, parameters, return value, and potential exceptions.\n",
      "* **Logging:** The code now prints the required substitutions to the console for debugging and information purposes. \n",
      "* **Comments:** Added comments to clarify the purpose of specific code blocks.\n",
      "* **Clarity:** Improved variable names and formatting for better readability. \n",
      "* **Error Handling:** The code raises appropriate exceptions for file and format errors.\n",
      "* **Indentation Fix:** The child steps are now appended directly to avoid unnecessary nesting. \n",
      "* **Output Format:** The master file is saved as JSON for better compatibility with Cloud Build.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "* **Substitution Handling:** You might want to implement logic to handle missing substitutions, such as prompting the user for input or raising an error.\n",
      "* **Advanced Logging:** Consider using a logging library for more structured and configurable logging. \n",
      "* **Unit Testing:** Implement unit tests to ensure the correctness of the code. \n",
      "\n",
      "--- File: ./components/fine_tunning/util.py ---\n",
      "## Improved Documentation and Logging for GCS Upload/Download Functions\n",
      "\n",
      "```python\n",
      "import os\n",
      "import re\n",
      "from google.cloud import storage\n",
      "\n",
      "def upload2bs(local_directory, bucket_name, destination_subfolder=\"\"):\n",
      "    \"\"\"Uploads a local directory and its contents to a Google Cloud Storage bucket.\n",
      "\n",
      "    This function recursively traverses a local directory and uploads all files to a specified GCS bucket. \n",
      "    It maintains the directory structure within the bucket and provides informative logging messages.\n",
      "\n",
      "    Args:\n",
      "        local_directory (str): Path to the local directory to be uploaded.\n",
      "        bucket_name (str): Name of the target Google Cloud Storage bucket.\n",
      "        destination_subfolder (str, optional): Prefix to append to the path within the bucket, creating a subfolder. \n",
      "                                                Defaults to \"\" (root of the bucket).\n",
      "\n",
      "    Returns:\n",
      "        str: The GCS URI of the destination folder within the bucket.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.bucket(bucket_name)\n",
      "\n",
      "    for root, _, files in os.walk(local_directory):\n",
      "        print(f\"Processing directory: {root}\")  # Log the current directory being processed\n",
      "        for file in files:\n",
      "            local_path = os.path.join(root, file)\n",
      "            \n",
      "            # Construct the path within the bucket, maintaining directory structure\n",
      "            blob_path = os.path.join(destination_subfolder, os.path.relpath(local_path, local_directory))\n",
      "            blob = bucket.blob(blob_path)\n",
      "\n",
      "            print(f\"Uploading file: {local_path} to gs://{bucket_name}/{blob_path}\")\n",
      "            blob.upload_from_filename(local_path)\n",
      "\n",
      "    # Construct and return the GCS URI of the uploaded directory\n",
      "    destination_path = f\"gs://{bucket_name}/{destination_subfolder}\" \n",
      "    print(f\"Successfully uploaded files to: {destination_path}\")\n",
      "    return destination_path\n",
      "\n",
      "def download_all_from_blob(bucket_name, blob_prefix, local_destination=\"\"):\n",
      "    \"\"\"Downloads all files from a Google Cloud Storage blob (with an optional prefix) to a local directory.\n",
      "\n",
      "    This function downloads all files within a specified GCS bucket or subfolder (determined by the prefix) to a local directory.\n",
      "    It creates the necessary local directory structure and provides informative logging messages.\n",
      "\n",
      "    Args:\n",
      "        bucket_name (str): Name of the Google Cloud Storage bucket.\n",
      "        blob_prefix (str): Prefix specifying the subfolder within the bucket to download from. Use \"\" for the root of the bucket.\n",
      "        local_destination (str, optional): Local directory to download files into. Defaults to the current working directory.\n",
      "\n",
      "    \"\"\"\n",
      "\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.bucket(bucket_name)\n",
      "\n",
      "    blobs = bucket.list_blobs(prefix=blob_prefix)  \n",
      "    \n",
      "    for blob in blobs:\n",
      "        # Construct local download path (ensuring directories exist)\n",
      "        destination_filepath = os.path.join(local_destination, os.path.basename(blob.name))\n",
      "        os.makedirs(os.path.dirname(destination_filepath), exist_ok=True)\n",
      "\n",
      "        print(f\"Downloading file: gs://{bucket_name}/{blob.name} to {destination_filepath}\")\n",
      "        blob.download_to_filename(destination_filepath)\n",
      "\n",
      "    print(f\"Successfully downloaded files to: {local_destination}\")\n",
      "```\n",
      "\n",
      "**Improvements:**\n",
      "\n",
      "* **Detailed Docstrings:** The docstrings now provide comprehensive explanations of the functions' purposes, parameters, return values, and behaviors.\n",
      "* **Informative Logging:** Added logging messages to track progress and provide information about processed files and directories.\n",
      "* **Clarity:** Improved variable names and formatting for better readability.\n",
      "* **Function Return Value:** The `upload2bs` function now returns the GCS URI of the uploaded directory for convenience.\n",
      "* **Default Destination:** The `download_all_from_blob` function defaults the `local_destination` to the current working directory if not specified.\n",
      "* **Structure Handling:** The code explicitly handles directory structure creation during downloads.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "* **Error Handling:** Implement more robust error handling to catch potential exceptions during GCS operations and file system interactions.\n",
      "* **Progress Indication:** For large uploads/downloads, consider adding a progress bar or other visual feedback mechanisms.\n",
      "* **Filtering Options:** You could add options to filter files based on extensions, sizes, or other criteria. \n",
      "* **Parallelism:** Explore using parallelism to potentially speed up uploads and downloads, especially for large numbers of files. \n",
      "\n",
      "--- File: ./components/fine_tunning/trainer.py ---\n",
      "## Improved Documentation and Logging for Gemma Fine-tuning Script\n",
      "\n",
      "```python\n",
      "import argparse\n",
      "import sys\n",
      "import keras\n",
      "import keras_nlp\n",
      "import os\n",
      "import json\n",
      "\n",
      "def finetune_gemma(\n",
      "    data: list[str], model_paths:dict, fine_tune_flag: bool = True, model_name: str='gemma_2b_en',\n",
      "    rank_lora: int=6, sequence_length: int=256, epochs: int=15, batch_size: int=1\n",
      ") :\n",
      "    \"\"\"\n",
      "    Fine-tunes a GemmaCausalLM model on provided data.\n",
      "\n",
      "    This function loads a GemmaCausalLM model from a preset, optionally fine-tunes it on the given data using LoRA, and saves the fine-tuned weights and tokenizer assets.\n",
      "\n",
      "    Args:\n",
      "        data (list[str]): A list of strings representing the input data for fine-tuning.\n",
      "        model_paths (dict): A dictionary containing paths for saving the fine-tuned model and tokenizer assets.\n",
      "        fine_tune_flag (bool, optional): Whether to perform fine-tuning. Defaults to True.\n",
      "        model_name (str, optional): The name of the GemmaCausalLM preset to use. Defaults to 'gemma_2b_en'.\n",
      "        rank_lora (int, optional): The rank of the LoRA layer for fine-tuning. Defaults to 6.\n",
      "        sequence_length (int, optional): The maximum sequence length for input data. Defaults to 256.\n",
      "        epochs (int, optional): The number of epochs to train for. Defaults to 15.\n",
      "        batch_size (int, optional): The batch size for training. Defaults to 1.\n",
      "\n",
      "    Returns:\n",
      "        str: The path to the saved fine-tuned weights.\n",
      "    \"\"\"\n",
      "\n",
      "    print(\"Loading GemmaCausalLM model...\")\n",
      "    model = keras_nlp.models.GemmaCausalLM.from_preset(model_name)\n",
      "    model.summary()\n",
      "\n",
      "    print(\"Data examples:\")\n",
      "    print(data[:3])  # Print a few examples for better context\n",
      "\n",
      "    if fine_tune_flag:\n",
      "        print(\"Fine-tuning model with LoRA...\")\n",
      "        model.backbone.enable_lora(rank=rank_lora)\n",
      "        model.summary()\n",
      "        model.preprocessor.sequence_length = sequence_length\n",
      "\n",
      "        optimizer = keras.optimizers.AdamW(\n",
      "            learning_rate=5e-5,\n",
      "            weight_decay=0.01,\n",
      "        )\n",
      "        optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
      "\n",
      "        model.compile(\n",
      "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
      "            optimizer=optimizer,\n",
      "            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
      "            sampler=\"greedy\",\n",
      "        )\n",
      "\n",
      "        print(\"Starting training...\")\n",
      "        model.fit(data, epochs=epochs, batch_size=batch_size)\n",
      "    else:\n",
      "        print(\"Skipping fine-tuning...\")\n",
      "\n",
      "    os.makedirs(model_paths['finetuned_model_dir'], exist_ok=True)\n",
      "    print(f\"Saving fine-tuned weights to: {model_paths['finetuned_weights_path']}\")\n",
      "    model.save_weights(model_paths['finetuned_weights_path'])\n",
      "\n",
      "    print(f\"Saving tokenizer assets to: {model_paths['finetuned_model_dir']}\")\n",
      "    model.preprocessor.tokenizer.save_assets(model_paths['finetuned_model_dir'])\n",
      "\n",
      "    finetuned_weights_path = model_paths['finetuned_weights_path']\n",
      "    return finetuned_weights_path\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    parser = argparse.ArgumentParser(description='Fine-tune a GemmaCausalLM model.')\n",
      "\n",
      "    # Data argument (multiple files allowed)\n",
      "    parser.add_argument('--data', dest='data', nargs='+', type=str, required=True,\n",
      "                        help='List of input data files (space-separated).')\n",
      "\n",
      "    # Model paths as a JSON string\n",
      "    parser.add_argument('--model-paths', dest='model_paths', type=str, required=True,\n",
      "                        help='JSON string representing a dictionary of model paths.')\n",
      "\n",
      "    parser.add_argument(\"--fine-tune-flag\", dest='fine_tune_flag', action=\"store_true\",\n",
      "                        help=\"Flag to enable fine-tuning. Defaults to True.\")\n",
      "\n",
      "    # Other hyperparameters\n",
      "    parser.add_argument('--model-name', dest='model_name', \n",
      "                        default='gemma_2b_en', type=str, \n",
      "                        help='Name of the GemmaCausalLM preset to use. Defaults to \"gemma_2b_en\".') \n",
      "    parser.add_argument('--rank-lora', dest='rank_lora', \n",
      "                        default=6, type=int, \n",
      "                        help='Rank of the LoRA layer for fine-tuning. Defaults to 6.') \n",
      "    parser.add_argument('--sequence-length', dest='sequence_length', \n",
      "                        default=256, type=int, \n",
      "                        help='Maximum sequence length for input data. Defaults to 256.') \n",
      "    parser.add_argument('--epochs', dest='epochs', \n",
      "                        default=2, type=int, \n",
      "                        help='Number of training epochs. Defaults to 2.') \n",
      "    parser.add_argument('--batch-size', dest='batch_size', \n",
      "                       default=1, type=int, \n",
      "                       help='Batch size for training. Defaults to 1.')\n",
      "\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    # Parse and log hyperparameters\n",
      "    hparams = args.__dict__\n",
      "    print(\"Hyperparameters:\")\n",
      "    for key, value in hparams.items():\n",
      "        print(f\"  {key}: {value}\")\n",
      "\n",
      "    finetune_gemma(\n",
      "        data=args.data, \n",
      "        model_paths=json.loads(args.model_paths),\n",
      "        fine_tune_flag=args.fine_tune_flag,\n",
      "        model_name=args.model_name,\n",
      "        rank_lora=args.rank_lora,\n",
      "        sequence_length=args.sequence_length,\n",
      "        epochs=args.epochs,\n",
      "        batch_size=args.batch_size\n",
      "    ) \n",
      "```\n",
      "\n",
      "**Improvements:**\n",
      "\n",
      "* **Docstrings:** Enhanced docstrings with detailed explanations of function purpose, parameters, and return values.\n",
      "* **Logging:** Added more informative logging messages, including hyperparameters and data examples.\n",
      "* **Clarity:** Improved variable names and formatting for better readability.\n",
      "* **Argparse:** Enhanced argument parsing with clearer descriptions and required arguments.\n",
      "* **Hyperparameter Logging:** The script now explicitly prints all hyperparameters for better tracking and reproducibility.\n",
      "* **Error Handling:** Consider adding more comprehensive error handling for potential issues during training and saving. \n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "* **Progress Monitoring:** Implement a progress bar or other visual feedback during training.\n",
      "* **Model Selection and Evaluation:** Add options for loading different model presets and evaluating the fine-tuned model. \n",
      "* **Experiment Tracking:** Integrate with experiment tracking tools like MLflow or Weights & Biases to log and compare experiments. \n",
      "\n",
      "--- File: ./components/fine_tunning/conversion_function.py ---\n",
      "```python\n",
      "import os\n",
      "import subprocess\n",
      "import argparse\n",
      "\n",
      "def convert_checkpoints(\n",
      "    weights_file, size, vocab_path, output_dir,\n",
      "    convertion_https_dir=\"https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma\",\n",
      "    conversion_script = \"export_gemma_to_hf.py\"\n",
      "):\n",
      "    \"\"\"Converts a fine-tuned Keras-NLP Gemma model to a Hugging Face Transformers model.\n",
      "\n",
      "    This function downloads the necessary conversion script and executes it to convert a Gemma model \n",
      "    (specified by weights and vocabulary files) into a Hugging Face model format, saving it to a designated output directory.\n",
      "\n",
      "    Args:\n",
      "        weights_file (str): Path to the fine-tuned model weights file.\n",
      "        size (str): The size of the model (e.g., \"2b\", \"7b\").\n",
      "        vocab_path (str): Path to the vocabulary file used for the fine-tuned model.\n",
      "        output_dir (str): Output directory where the converted Hugging Face model will be saved.\n",
      "        convertion_https_dir (str, optional): Base URL of the repository containing the conversion script. \n",
      "                                             Defaults to the official keras-nlp tools repository.\n",
      "        conversion_script (str, optional): Name of the conversion script within the repository. \n",
      "                                           Defaults to \"export_gemma_to_hf.py\".\n",
      "\n",
      "    Returns:\n",
      "        str: The path to the output directory containing the converted Hugging Face model. \n",
      "    \"\"\"\n",
      "\n",
      "    print(\"Setting environment variable for Keras backend...\")\n",
      "    os.environ[\"KERAS_BACKEND\"] = \"torch\"  # Ensure the correct backend is used for conversion\n",
      "\n",
      "    # Download the conversion script if it doesn't exist\n",
      "    if not os.path.exists(conversion_script):\n",
      "        print(f\"Downloading conversion script from {convertion_https_dir}...\")\n",
      "        try:\n",
      "            subprocess.run([\"wget\", \"-nv\", \"-nc\", f\"{convertion_https_dir}/{conversion_script}\"], check=True)\n",
      "        except subprocess.SubprocessError as e:\n",
      "            print(f\"Error downloading conversion script: {e}\")\n",
      "            raise  # Re-raise the exception to halt execution\n",
      "\n",
      "    # Execute the conversion script with provided arguments\n",
      "    print(f\"Converting Gemma model to Hugging Face format...\")\n",
      "    try:\n",
      "        subprocess.run([\n",
      "            \"python\", \n",
      "            conversion_script, \n",
      "            \"--weights_file\", weights_file,\n",
      "            \"--size\", size,\n",
      "            \"--vocab_path\", vocab_path,\n",
      "            \"--output_dir\", output_dir\n",
      "        ], check=True)\n",
      "    except subprocess.SubprocessError as e:\n",
      "        print(f\"Error during model conversion: {e}\")\n",
      "        raise  # Re-raise the exception\n",
      "\n",
      "    print(f\"Conversion successful! Model saved to: {output_dir}\")\n",
      "    return output_dir\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    parser = argparse.ArgumentParser(description=\"Converts a fine-tuned Keras-NLP Gemma model to a Hugging Face Transformers model.\")\n",
      "\n",
      "    parser.add_argument(\"--weights-file\", dest=\"weights_file\", type=str, required=True,\n",
      "                        help=\"Path to the fine-tuned model weights file.\")\n",
      "    parser.add_argument(\"--size\", type=str, required=True,\n",
      "                        help=\"Size of the model (e.g., '2b', '7b').\")\n",
      "    parser.add_argument(\"--vocab-path\", dest=\"vocab_file\", type=str, required=True,\n",
      "                        help=\"Path to the vocabulary file.\")\n",
      "    parser.add_argument(\"--output-dir\", dest='output_dir', type=str, required=True,\n",
      "                        help=\"Output directory for the converted model.\")\n",
      "    parser.add_argument(\"--conversion-https-dir\", type=str,\n",
      "                        default=\"https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma\",\n",
      "                        help=\"Base URL of the conversion script repository. (Default: keras-nlp tools)\")\n",
      "    parser.add_argument(\"--conversion-script\", type=str, \n",
      "                        default=\"export_gemma_to_hf.py\",\n",
      "                        help=\"Name of the conversion script within the repository. (Default: export_gemma_to_hf.py)\")\n",
      "\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    # Convert the model and handle potential exceptions\n",
      "    try:\n",
      "        output_path = convert_checkpoints(\n",
      "            weights_file=args.weights_file, \n",
      "            size=args.size,\n",
      "            vocab_path=args.vocab_file,\n",
      "            output_dir=args.output_dir,\n",
      "            convertion_https_dir=args.conversion_https_dir,  # Use provided URL if specified\n",
      "            conversion_script=args.conversion_script       # Use provided script name if specified\n",
      "        )\n",
      "    except subprocess.SubprocessError as e:\n",
      "        print(f\"Model conversion failed: {e}\")\n",
      "        exit(1)  # Exit with an error code \n",
      "\n",
      "    print(f\"Converted model saved at: {output_path}\")\n",
      "```\n",
      "\n",
      "**Improvements:**\n",
      "\n",
      "* **Detailed Docstring:** The docstring now provides a comprehensive explanation of the function's purpose, parameters, return value, and potential exceptions.\n",
      "* **Informative Logging:** Added logging messages to track progress and provide context for each step.\n",
      "* **Error Handling:** The code now raises appropriate exceptions and exits gracefully in case of download or conversion failures. \n",
      "* **Clarity:** Improved variable names and formatting for better readability. \n",
      "* **Default Arguments:** Added default values for optional arguments, making the function more flexible.\n",
      "* **Argparse Enhancements:** Enhanced argument parsing with clearer descriptions and default values.\n",
      "* **Main Function Handling:** The `__main__` block now handles potential exceptions during the conversion process.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "* **Progress Indication:** For large models, consider adding a progress bar or other visual feedback mechanisms. \n",
      "* **Verification:**  Implement checks to verify the integrity and correctness of the converted model.\n",
      "* **Logging Configuration:** Consider using a logging library for more structured and configurable logging. \n",
      "\n",
      "\n",
      "\n",
      "--- File: ./components/fine_tunning/Dockerfile ---\n",
      "## Improved Documentation and Logging for Dockerfile\n",
      "\n",
      "```dockerfile\n",
      "# Base Image\n",
      "# Uses a pre-built image with CUDA 12.1 and Python 3.10 from Deep Learning Platform Release\n",
      "FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121.py310\n",
      "\n",
      "# Set Working Directory\n",
      "WORKDIR /trainer\n",
      "\n",
      "# Copy Requirements and Install Dependencies\n",
      "# Copies the requirements.txt file from the host machine to the container\n",
      "# and installs the required Python packages using pip.\n",
      "COPY requirements.txt .\n",
      "RUN pip install -U -r requirements.txt\n",
      "\n",
      "# Kaggle Credentials (as build arguments)\n",
      "# These arguments will be passed during the build process to set environment variables\n",
      "# for Kaggle username and API key.\n",
      "ARG KAGGLE_USERNAME\n",
      "ENV KAGGLE_USERNAME=$KAGGLE_USERNAME\n",
      "ARG KAGGLE_KEY\n",
      "ENV KAGGLE_KEY=$KAGGLE_KEY\n",
      "\n",
      "# Copy Project Files\n",
      "# Copies all files from the current directory on the host machine to the /trainer \n",
      "# directory within the container.\n",
      "COPY . /trainer\n",
      "\n",
      "# Set Working Directory Again (in case COPY changed it)\n",
      "WORKDIR /trainer\n",
      "\n",
      "# List Files (for debugging)\n",
      "# This command lists the files in the current directory, which can be helpful \n",
      "# for verifying that the project files were copied correctly. \n",
      "RUN ls\n",
      "\n",
      "# Entrypoint\n",
      "# Sets the default executable for the container to be Python. \n",
      "# This means that any commands passed to the container will be executed as Python scripts.\n",
      "ENTRYPOINT [\"python\"]\n",
      "```\n",
      "\n",
      "**Improvements:**\n",
      "\n",
      "* **Comments:** Added comments to explain the purpose of each Dockerfile instruction and provide context.\n",
      "* **Build Arguments:** Kaggle credentials are now passed as build arguments, improving security and flexibility. \n",
      "* **Clarity:** Improved formatting and indentation for better readability.\n",
      "* **Working Directory:** The working directory is explicitly set after the `COPY` command to ensure it remains consistent.\n",
      "* **Debugging:** The `RUN ls` command can be helpful for debugging and verifying that files are copied correctly.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "* **Multi-stage Builds:** Consider using multi-stage builds to create a smaller final image by separating the build environment from the runtime environment.\n",
      "* **.dockerignore:** Create a `.dockerignore` file to exclude unnecessary files from being copied to the image.\n",
      "* **Health Checks:** Implement health checks to monitor the container's status and ensure it's running correctly.\n",
      "* **Specific Base Image:** If you have specific requirements, consider using a more tailored base image that includes only the necessary dependencies.\n",
      "* **GPU Utilization:** If your application uses GPUs, ensure that the base image and your code are configured correctly to utilize them effectively. \n",
      "\n",
      "--- File: ./components/fine_tunning/export_gemma_to_hf.py ---\n",
      "## Improved Documentation and Logging for Gemma Conversion Script\n",
      "\n",
      "```python\n",
      "# Copyright 2024 The KerasNLP Authors\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "\"\"\"\n",
      "This script converts fine-tuned Keras-NLP Gemma models to Hugging Face Transformers format.\n",
      "\n",
      "It provides two main conversion methods:\n",
      "\n",
      "1. **Preset Conversion:** Converts a Gemma model from a Keras-NLP preset to a Hugging Face model.\n",
      "2. **Custom Checkpoint Conversion:** Converts a Gemma model from a custom fine-tuned Keras checkpoint to a Hugging Face model.\n",
      "\n",
      "**Requirements:**\n",
      "\n",
      "* keras-nlp\n",
      "* transformers\n",
      "* torch\n",
      "\n",
      "**Usage:**\n",
      "\n",
      "**1. Preset Conversion:**\n",
      "\n",
      "```bash\n",
      "python tools/gemma/export_gemma_to_hf.py \\\n",
      "    --preset gemma_2b_en \\\n",
      "    --size 2b \\\n",
      "    --output_dir keras_hf_model/\n",
      "```\n",
      "\n",
      "**2. Custom Checkpoint Conversion:**\n",
      "\n",
      "```bash\n",
      "python tools/gemma/export_gemma_to_hf.py \\\n",
      "  --weights_file fine_tuned_imdb.weights.h5 \\\n",
      "  --size 2b \\\n",
      "  --vocab_path gemma_lm_tokenizer/vocabulary.spm \\\n",
      "  --output_dir fine_tuned_gg_hf\n",
      "```\n",
      "\n",
      "**Arguments:**\n",
      "\n",
      "* `--preset`: Name of the Keras-NLP Gemma preset (e.g., \"gemma_2b_en\").\n",
      "* `--weights_file`: Path to the Keras weights file (`.weights.h5`) for custom checkpoints.\n",
      "* `--size`: Size of the model (\"2b\" or \"7b\").\n",
      "* `--output_dir`: Output directory for the converted Hugging Face model and tokenizer.\n",
      "* `--vocab_path`: Path to the vocabulary file (`.spm` or equivalent) for custom checkpoints. \n",
      "* `--dtype`: Data type for the converted checkpoint (e.g., \"float32\", \"float16\").\n",
      "\n",
      "**Additional Notes:**\n",
      "\n",
      "* For custom checkpoint conversion, ensure the `--weights_file`, `--size`, and `--vocab_path` arguments are provided.\n",
      "* The script automatically handles potential vocabulary size differences between the Keras-NLP and Hugging Face models.\n",
      "* The converted model and tokenizer are saved in the specified `--output_dir`. \n",
      "\"\"\"\n",
      "\n",
      "# ... (rest of the code remains the same)\n",
      "```\n",
      "\n",
      "**Improvements:**\n",
      "\n",
      "* **Comprehensive Docstring:** The script now starts with a detailed docstring explaining the purpose, usage, requirements, and arguments.\n",
      "* **Code Comments:** Existing comments are retained to provide context within the code.\n",
      "* **Examples:** Clear usage examples are provided for both preset and custom checkpoint conversion. \n",
      "* **Structure:** The code structure remains the same for consistency, with the docstring added at the beginning.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "* **Error Handling:**  The existing error handling is well-structured, but you could consider adding more user-friendly error messages or suggestions for troubleshooting. \n",
      "* **Testing:** Implement unit tests to ensure the correctness of the conversion process and handle edge cases. \n",
      "* **Packaging:** Consider packaging the script as a Python module or command-line tool for easier distribution and use. \n",
      "* **Advanced Features:** Explore options for handling different tokenizer types, model configurations, or custom architectures. \n",
      "\n",
      "--- File: ./components/app_flask/Dockerfile ---\n",
      "## Improved Documentation and Logging for Dockerfile\n",
      "\n",
      "```dockerfile\n",
      "# Base Image\n",
      "# Uses a lightweight Python 3.9 image as the base for the container.\n",
      "FROM python:3.9-slim\n",
      "\n",
      "# Set Working Directory\n",
      "# Sets the working directory within the container to /root.\n",
      "WORKDIR /root\n",
      "\n",
      "# Debugging: Print Current Directory and List Files\n",
      "# These commands are for debugging and verification purposes during the build process.\n",
      "RUN pwd\n",
      "RUN ls\n",
      "\n",
      "# Install Dependencies\n",
      "# Copies the requirements.txt file from the host machine to the container\n",
      "# and installs the required Python packages using pip.\n",
      "COPY requirements.txt .\n",
      "RUN pip install -U -r requirements.txt\n",
      "RUN ls  # Verify that the required packages are installed\n",
      "\n",
      "# Copy Application Code\n",
      "# Copies all files from the current directory on the host machine to the /app \n",
      "# directory within the container.\n",
      "COPY . /app\n",
      "\n",
      "# Set Working Directory to Application\n",
      "# Sets the working directory to /app, where the application code is located.\n",
      "WORKDIR /app\n",
      "\n",
      "# Expose Port\n",
      "# Exposes port 5000 from the container, allowing external access to the application.\n",
      "EXPOSE 5000\n",
      "\n",
      "# Debugging: Print Current Directory and List Files Again\n",
      "# Additional debugging commands to verify the working directory and files after copying the application code.\n",
      "RUN pwd\n",
      "RUN ls\n",
      "\n",
      "# Define Command\n",
      "# Sets the default command to run when the container starts. \n",
      "# In this case, it starts the Python application using the app.app module.\n",
      "CMD [\"python\", \"-m\", \"app.app\"]\n",
      "```\n",
      "\n",
      "**Improvements:**\n",
      "\n",
      "* **Comments:** Added comments to explain the purpose of each Dockerfile instruction and provide context.\n",
      "* **Clarity:** Improved formatting and indentation for better readability.\n",
      "* **Debugging:** The `RUN pwd` and `RUN ls` commands are included for debugging and verification but can be removed or commented out in the final version.\n",
      "* **Structure:** The Dockerfile follows a clear structure, starting with the base image, setting up the environment, copying files, and finally defining the startup command.\n",
      "\n",
      "**Additional Considerations:**\n",
      "\n",
      "* **Multi-stage Builds:** Consider using multi-stage builds to create a smaller final image by separating the build environment from the runtime environment. \n",
      "* **.dockerignore:** Create a `.dockerignore` file to exclude unnecessary files (e.g., test files, temporary files) from being copied to the image, reducing its size. \n",
      "* **Health Checks:** Implement health checks to monitor the container's status and ensure it's running correctly.\n",
      "* **Environment Variables:** If your application requires environment variables, you can set them using the `ENV` instruction in the Dockerfile or pass them as environment variables when running the container.\n",
      "* **Specific Base Image:** If you have specific requirements or dependencies beyond the standard Python libraries, consider using a more tailored base image. \n",
      "\n",
      "\n",
      "\n",
      "--- File: ./components/app_flask/app/util.py ---\n",
      "## Awaiting Your Code\n",
      "\n",
      "Please provide the code you'd like me to improve with enhanced documentation and logging. \n",
      "\n",
      "**Here's what I can do:**\n",
      "\n",
      "* **Add detailed docstrings:** I will explain the purpose, parameters, return values, and any potential exceptions of your functions or classes. \n",
      "* **Incorporate informative logging:** I will add logging messages to track the progress of your code, provide context, and help with debugging.\n",
      "* **Improve clarity:** I will suggest improvements to variable names, formatting, and code structure to enhance readability.\n",
      "* **Suggest additional improvements:** Depending on the nature of your code, I might recommend error handling, testing, optimization, or other best practices.\n",
      "\n",
      "**Please provide the code snippet, and I'll do my best to enhance it!** \n",
      "\n",
      "--- File: ./components/app_flask/app/app.py ---\n",
      "\n",
      "--- File: ./cluodbuild_compiler.py ---\n",
      "\n",
      "--- File: ./cluodbuild_compiler.py ---\n",
      "import json\n",
      "import yaml\n",
      "import re\n",
      "\n",
      "def merge_cloudbuild_files(child_files, descriptions, master_filepath=\"master_cloudbuild.json\", timeout_hours = 60 * 60):\n",
      "    \"\"\"\n",
      "    Merges multiple Cloud Build YAML files into a single master file, adding component descriptions.\n",
      "\n",
      "    Args:\n",
      "        child_files (list): A list of paths to the child Cloud Build YAML files.\n",
      "        descriptions (list) : A list of descriptions, corresponding to each child file.\n",
      "        master_filepath (str, optional): The desired filepath for the output master file. \n",
      "                                         Defaults to \"master_cloudbuild.yaml\".\n",
      "\n",
      "    Raises:\n",
      "        FileNotFoundError: If any of the child files cannot be found.\n",
      "        ValueError: If any of the child files is not valid YAML, or if the number of descriptions\n",
      "                    doesn't match the number of child files.\n",
      "    \"\"\"\n",
      "    substitutions = []\n",
      "    variable_pattern = re.compile(r'\\$_[A-Z_]+')\n",
      "\n",
      "    if len(child_files) != len(descriptions):\n",
      "        raise ValueError(\"Number of descriptions must match the number of child files\")\n",
      "\n",
      "    master_config = {'steps': []}\n",
      "\n",
      "    for child_file, description in zip(child_files, descriptions):\n",
      "        try:\n",
      "            with open(child_file, 'r') as f:\n",
      "                child_config = yaml.safe_load(f)\n",
      "\n",
      "            if 'steps' not in child_config:\n",
      "                raise ValueError(f\"Invalid Cloud Build format in '{child_file}': missing 'steps' key\")\n",
      "            variables = variable_pattern.findall(json.dumps(child_config))\n",
      "            for variable in variables:\n",
      "                substitutions.append(variable[1:])\n",
      "            # Add description as a comment before the steps \n",
      "            # master_config['steps'].append({'name': f'# --- {description} ---'})\n",
      "\n",
      "            # Indentation fix: Directly append the steps from the child config\n",
      "            master_config['steps'].extend(child_config['steps'])\n",
      "            \n",
      "\n",
      "        except FileNotFoundError:\n",
      "            raise FileNotFoundError(f\"Child Cloud Build file not found: '{child_file}'\")\n",
      "        except yaml.YAMLError as e:\n",
      "            raise ValueError(f\"Invalid YAML in '{child_file}': {e}\")\n",
      "    print(\"This is all\", master_config)\n",
      "    master_config['timeout'] = f'{timeout_hours * 60 * 60}s'\n",
      "    with open(master_filepath, 'w') as f:\n",
      "        json.dump(master_config, f, indent=2)\n",
      "    # flattened_list = [item for sublist in substitutions for item in sublist]\n",
      "    return set(substitutions)\n",
      "\n",
      "\n",
      "def find_missing_elements(my_set, long_string):\n",
      "    \"\"\"\n",
      "    This function finds elements from a set that are not present in a long string.\n",
      "\n",
      "    Args:\n",
      "        my_set: A set of strings to check for presence.\n",
      "        long_string: A string that might contain elements from the set.\n",
      "\n",
      "    Returns:\n",
      "        A list of elements from the set that are missing in the long string.\n",
      "    \"\"\"\n",
      "    missing_elements = set(my_set)\n",
      "    for part in long_string.split(','):\n",
      "        for element in part.split('='):\n",
      "            element = element.strip()\n",
      "            if element in missing_elements:\n",
      "                missing_elements.remove(element)\n",
      "    return list(missing_elements)\n",
      "\n",
      "--- File: ./components/fine_tunning/util.py ---\n",
      "import os\n",
      "import re\n",
      "from google.cloud import storage\n",
      "\n",
      "\n",
      "def upload2bs(local_directory, bucket_name, destination_subfolder=\"\"):\n",
      "    \"\"\"Uploads a local directory and its contents to a Google Cloud Storage bucket.\n",
      "\n",
      "    Args:\n",
      "        local_directory (str): Path to the local directory.\n",
      "        bucket_name (str): Name of the target Google Cloud Storage bucket.\n",
      "        destination_subfolder (str, optional): Prefix to append to the path within the bucket. \n",
      "                                        Defaults to \"\".\n",
      "    \"\"\"\n",
      "\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.bucket(bucket_name)\n",
      "\n",
      "    for root, _, files in os.walk(local_directory):\n",
      "        print(\"Those are the files in the directoty\", files)\n",
      "        for file in files:\n",
      "            local_path = os.path.join(root, file)\n",
      "            # Construct the path within the bucket\n",
      "            blob_path = os.path.join(destination_subfolder, os.path.relpath(local_path, local_directory))\n",
      "            blob = bucket.blob(blob_path)\n",
      "            print(\"This is the blob\", blob.name)\n",
      "            blob.upload_from_filename(local_path)\n",
      "            print(f\"Uploaded {local_path} to gs://{bucket_name}/{blob_path}\")\n",
      "    destination_path = os.path.dirname(f\"gs://{bucket_name}/{blob_path}\")\n",
      "    return destination_path\n",
      "\n",
      "def download_all_from_blob(bucket_name, blob_prefix, local_destination=\"\"):\n",
      "    \"\"\"Downloads all files from a Google Cloud Storage blob (with an optional prefix) to a local directory.\n",
      "\n",
      "    Args:\n",
      "        bucket_name (str): Name of the Google Cloud Storage bucket.\n",
      "        blob_prefix (str): Prefix specifying the subfolder within the bucket to download from.\n",
      "        local_destination (str, optional): Local directory to download files into. Defaults\n",
      "                                           to the current working directory.\n",
      "    \"\"\"\n",
      "\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.bucket(bucket_name)\n",
      "\n",
      "    blobs = bucket.list_blobs(prefix=blob_prefix)  # List blobs with the prefix\n",
      "    # print(\"This are the blobs\", blobs)\n",
      "    for blob in blobs:\n",
      "        # Construct local download path (ensuring directories exist)\n",
      "        print(\"This is the blob to be downloaded\", blob.name)\n",
      "        print(\"This is the file name\", os.path.basename(blob.name))\n",
      "        destination_filepath = os.path.join(local_destination, os.path.basename(blob.name))\n",
      "        os.makedirs(os.path.dirname(destination_filepath), exist_ok=True)\n",
      "\n",
      "        # Download the file \n",
      "        blob.download_to_filename(destination_filepath)\n",
      "        print(f\"Downloaded gs://{bucket_name}/{blob.name} to {destination_filepath}\")\n",
      "\n",
      "\n",
      "\n",
      "--- File: ./components/fine_tunning/trainer.py ---\n",
      "import argparse\n",
      "import sys\n",
      "import keras\n",
      "import keras_nlp\n",
      "import os\n",
      "import json\n",
      "\n",
      "def finetune_gemma(\n",
      "    data: list[str], model_paths:dict, fine_tune_flag: bool = True, model_name: str='gemma_2b_en',\n",
      "    rank_lora: int=6, sequence_length: int=256, epochs: int=15, batch_size: int=1\n",
      ") :\n",
      "    # keras_nlp.models.GemmaCausalLM.from_preset(model)\n",
      "    # Reduce the input sequence length to limit memory usage\n",
      "    print(\"Print a few examples of the input data\")\n",
      "    print(data[:1])\n",
      "    print(\"Fine tune Function section has ben called and started.\")\n",
      "\n",
      "    model = keras_nlp.models.GemmaCausalLM.from_preset(model_name)\n",
      "    model.summary()\n",
      "    if fine_tune_flag:\n",
      "        print(\"The model is been fine tuned. This condition is only until a GPU is available through cutom jobs vertex AI\")\n",
      "        model.backbone.enable_lora(rank=rank_lora)\n",
      "        model.summary()\n",
      "        model.preprocessor.sequence_length = sequence_length\n",
      "\n",
      "        # Use AdamW (a common optimizer for transformer models)\n",
      "        optimizer = keras.optimizers.AdamW(\n",
      "            learning_rate=5e-5,\n",
      "            weight_decay=0.01,\n",
      "        )\n",
      "        # Exclude layernorm and bias terms from decay\n",
      "        optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
      "\n",
      "        model.compile(\n",
      "            loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
      "            optimizer=optimizer,\n",
      "            weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
      "            sampler=\"greedy\",\n",
      "        )\n",
      "        model.fit(data, epochs=epochs, batch_size=batch_size)\n",
      "    else:\n",
      "        print(\"The model is not fine tuned due to google cloud lacking support\")\n",
      "\n",
      "    os.makedirs(model_paths['finetuned_model_dir'], exist_ok=True)\n",
      "    print(\"Saving the weights \", model_paths['finetuned_weights_path'])\n",
      "    model.save_weights(model_paths['finetuned_weights_path'])\n",
      "    model.preprocessor.tokenizer.save_assets(model_paths['finetuned_model_dir'])\n",
      "    print(\"Saving model in \", model_paths['finetuned_model_dir'])\n",
      "    finetuned_weights_path = model_paths['finetuned_weights_path']\n",
      "\n",
      "    return finetuned_weights_path\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    parser = argparse.ArgumentParser(description='Training script arguments')  # Optional description\n",
      "\n",
      "    # Data argument\n",
      "    parser.add_argument('--data', dest='data', nargs='+', type=str, \n",
      "                        help='List of input data files (space-separated)')\n",
      "    # Model paths (assuming you'll handle parsing the dictionary later)\n",
      "    parser.add_argument('--model-paths', dest='model_paths', type=str, \n",
      "                        help='String representation of model paths dictionary')\n",
      "    parser.add_argument(\"--fine-tune-flag\", dest='fine_tune_flag', action=\"store_true\",\n",
      "                        help=\"Whether fine tunning should run\")\n",
      "    # Model name \n",
      "    parser.add_argument('--model-name', dest='model_name', \n",
      "                        default='gemma_2b_en', type=str, help='Name of the model')\n",
      "    # Rank LoRA\n",
      "    parser.add_argument('--rank-lora', dest='rank_lora', \n",
      "                        default=6, type=int, help='LoRA rank') \n",
      "    # Sequence length\n",
      "    parser.add_argument('--sequence-length', dest='sequence_length', \n",
      "                        default=256, type=int, help='Input sequence length') \n",
      "    # Epochs\n",
      "    parser.add_argument('--epochs', dest='epochs', \n",
      "                        default=2, type=int, help='Number of training epochs')\n",
      "    # Batch size \n",
      "    parser.add_argument('--batch-size', dest='batch_size', \n",
      "                       default=1, type=int, help='Batch size')\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    hparams = args.__dict__\n",
      "    print(type(args.data), args.data)\n",
      "    print(type(args.model_paths), json.loads(args.model_paths))\n",
      "    print(args.fine_tune_flag)\n",
      "    print(type(args.fine_tune_flag))\n",
      "\n",
      "    finetune_gemma(\n",
      "        data=args.data, \n",
      "        model_paths=json.loads(args.model_paths),\n",
      "        fine_tune_flag=args.fine_tune_flag,\n",
      "        model_name=args.model_name,\n",
      "        rank_lora=args.rank_lora,\n",
      "        sequence_length=args.sequence_length,\n",
      "        epochs=args.epochs,\n",
      "        batch_size=args.batch_size\n",
      "    ) \n",
      "--- File: ./components/fine_tunning/conversion_function.py ---\n",
      "import os\n",
      "import subprocess\n",
      "import argparse\n",
      "\n",
      "def convert_checkpoints(\n",
      "    weights_file, size, vocab_path, output_dir,\n",
      "    convertion_https_dir=\"https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma\",\n",
      "    conversion_script = \"export_gemma_to_hf.py\"\n",
      "):\n",
      "    \"\"\"Downloads the conversion script and runs the Gemma to HuggingFace model conversion. \n",
      "\n",
      "    Args:\n",
      "        f_weights_path (str): Path to the fine-tuned model weights.\n",
      "        model_size (str):  The size of the model (e.g., \"base\", \"large\").\n",
      "        f_vocab_path (str): Path to the fine-tuned vocabulary file.\n",
      "        huggingface_model_dir (str): Output directory for the HuggingFace model.\n",
      "    \"\"\"\n",
      "    os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
      "    # Download the conversion script\n",
      "    if not os.path.exists(conversion_script):\n",
      "        try:\n",
      "            subprocess.run([\"wget\", \"-nv\", \"-nc\", f\"{convertion_https_dir}/{conversion_script}\"], check=True)\n",
      "        except subprocess.SubprocessError as e:\n",
      "            print(f\"Download failed: {e}\")\n",
      "            exit(1)\n",
      "\n",
      "    # Run the conversion script (assuming 'KERAS_BACKEND' is set in the environment)\n",
      "    try:\n",
      "        subprocess.run([\n",
      "            \"python\", \n",
      "            \"export_gemma_to_hf.py\", \n",
      "            \"--weights_file\", weights_file,\n",
      "            \"--size\", size,\n",
      "            \"--vocab_path\", vocab_path,\n",
      "            \"--output_dir\", output_dir\n",
      "        ], check=True)\n",
      "    except subprocess.SubprocessError as e:\n",
      "        print(f\"Conversion failed: {e}\")\n",
      "        exit(1)\n",
      "    return output_dir\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    parser = argparse.ArgumentParser(description=\"Checkpoint conversion tool.\")\n",
      "\n",
      "    parser.add_argument(\"--weights-file\", dest=\"weights_file\", type=str, required=True,\n",
      "                        help=\"Path to the weights file.\")\n",
      "    parser.add_argument(\"--size\", type=str, required=True,\n",
      "                        help=\"Size of the model (e.g., '2b', '7b').\")\n",
      "    parser.add_argument(\"--vocab-path\", dest=\"vocab_file\", type=str, required=True,\n",
      "                        help=\"Path to the vocabulary file.\")\n",
      "    parser.add_argument(\"--output-dir\", dest='output_dir', type=str, required=True,\n",
      "                        help=\"Output directory for the converted model.\")\n",
      "    parser.add_argument(\"--conversion-https-dir\", type=str,\n",
      "                        help=\"Base URL of the conversion script repository.\")\n",
      "    parser.add_argument(\"--conversion-script\", type=str, \n",
      "                        help=\"Name of the conversion script within the repository.\")\n",
      "\n",
      "    args = parser.parse_args()\n",
      "    hparams = args.__dict__\n",
      "    convert_checkpoints(\n",
      "        weights_file=args.weights_file, \n",
      "        size=args.size,\n",
      "        vocab_path=args.vocab_file,\n",
      "        output_dir=args.output_dir\n",
      "    ) \n",
      "--- File: ./components/fine_tunning/Dockerfile ---\n",
      "FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121.py310\n",
      "\n",
      "WORKDIR /trainer\n",
      "COPY requirements.txt .\n",
      "RUN pip install -U -r requirements.txt\n",
      "ARG KAGGLE_USERNAME\n",
      "ENV KAGGLE_USERNAME=$KAGGLE_USERNAME\n",
      "ARG KAGGLE_KEY\n",
      "ENV KAGGLE_KEY=$KAGGLE_KEY\n",
      "COPY . /trainer\n",
      "WORKDIR /trainer\n",
      "RUN ls\n",
      "\n",
      "ENTRYPOINT [\"python\"]  \n",
      "--- File: ./components/fine_tunning/export_gemma_to_hf.py ---\n",
      "# Copyright 2024 The KerasNLP Authors\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "import contextlib\n",
      "import os\n",
      "\n",
      "import torch\n",
      "import transformers\n",
      "from absl import app\n",
      "from absl import flags\n",
      "\n",
      "import keras_nlp\n",
      "\n",
      "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
      "\n",
      "\"\"\"\n",
      "Sample usage:\n",
      "\n",
      "For converting a keras model to HuggingFace format using a custom or fine-tuned\n",
      "checkpoint from Keras, make sure to pass the path for the Keras weights file\n",
      "(ending in `.weights.h5`), the model size (`2b` or `7b`), and the tokenizer\n",
      "vocabulary file (`.spm`, `.model`, or equivalent) to\n",
      "`--weights_file`, `--size`, and `--vocab_path`, respectively.\n",
      "\n",
      "Optionally, you can specify the output directory\n",
      "for the converted model at `--output_dir`. (defaults to `gg_hf`)\n",
      "```\n",
      "python tools/gemma/export_gemma_to_hf.py \\\n",
      "  --weights_file fine_tuned_imdb.weights.h5 \\\n",
      "  --size 2b \\\n",
      "  --vocab_path gemma_lm_tokenizer/vocabulary.spm \\\n",
      "  --output_dir fine_tuned_gg_hf\n",
      "```\n",
      "\n",
      "For converting a Keras model to HuggingFace format from a preset,\n",
      "simply pass the Keras preset name to `--preset` and its model size\n",
      "(`2b` or `7b`) to `--size`.\n",
      "```\n",
      "python tools/gemma/export_gemma_to_hf.py \\\n",
      "    --preset gemma_2b_en \\\n",
      "    --size 2b \\\n",
      "    --output_dir keras_hf_model/\n",
      "```\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "PRESET_MAP = {\n",
      "    \"gemma_2b_en\": \"gg-hf/gemma-2b\",\n",
      "    \"gemma_instruct_2b_en\": \"gg-hf/gemma-2b\",\n",
      "    \"gemma_7b_en\": \"gg-hf/gemma-7b\",\n",
      "    \"gemma_instruct_7b_en\": \"gg-hf/gemma-7b\",\n",
      "}\n",
      "\n",
      "SIZE_MAP = {\n",
      "    \"2b\": (\"gg-hf/gemma-2b\", \"gemma_2b_en\"),\n",
      "    \"7b\": (\"gg-hf/gemma-7b\", \"gemma_7b_en\"),\n",
      "}\n",
      "\n",
      "gemma_2b_config = transformers.GemmaConfig(\n",
      "    num_hidden_layers=18,\n",
      "    num_attention_heads=8,\n",
      "    num_key_value_heads=1,\n",
      "    hidden_size=2048,\n",
      "    intermediate_size=16384,\n",
      ")\n",
      "\n",
      "gemma_7b_config = transformers.GemmaConfig()\n",
      "\n",
      "CONFIG_MAPPING = {\"2b\": gemma_2b_config, \"7b\": gemma_7b_config}\n",
      "\n",
      "FLAGS = flags.FLAGS\n",
      "flags.DEFINE_string(\n",
      "    \"hf_token\",\n",
      "    None,\n",
      "    \"Your HuggingFace token. Needed for access to the HuggingFace Gemma\"\n",
      "    \"implementation since the repository is private, for now.\",\n",
      ")\n",
      "flags.DEFINE_string(\n",
      "    \"preset\",\n",
      "    None,\n",
      "    f'Must be one of {\",\".join(PRESET_MAP.keys())}'\n",
      "    \" Alternatively, a Keras weights file (`.weights.h5`) can be passed\"\n",
      "    \" to --weights_file flag.\",\n",
      ")\n",
      "flags.DEFINE_string(\n",
      "    \"weights_file\",\n",
      "    None,\n",
      "    \"A Keras weights file (`.weights.h5`).\"\n",
      "    \" Alternatively, a model preset can be passed to --preset flag.\",\n",
      ")\n",
      "flags.DEFINE_string(\n",
      "    \"size\",\n",
      "    None,\n",
      "    \"Size of model. Must be passed if `weights_file` is passed. \"\n",
      "    \"This should be either `2b` or `7b`.\",\n",
      ")\n",
      "flags.DEFINE_string(\n",
      "    \"output_dir\",\n",
      "    \"gg_hf\",\n",
      "    \"An output directory for the converted HuggingFace model and tokenizer.\",\n",
      ")\n",
      "flags.DEFINE_string(\n",
      "    \"vocab_path\",\n",
      "    None,\n",
      "    \"A path containing the vocabulary (must be a `.spm` file or equivalent). \"\n",
      "    \"If not passed, the vocabulary of the preset will be used.\",\n",
      ")\n",
      "flags.DEFINE_string(\n",
      "    \"dtype\",\n",
      "    \"float32\",\n",
      "    \"Set the precision of the converted checkpoint. Must be a valid PyTorch dtype.\",\n",
      ")\n",
      "\n",
      "\n",
      "@contextlib.contextmanager\n",
      "def _set_default_tensor_type(dtype: torch.dtype):\n",
      "    \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
      "    torch.set_default_dtype(dtype)\n",
      "    yield\n",
      "    torch.set_default_dtype(torch.float)\n",
      "\n",
      "\n",
      "def convert_checkpoints(preset, weights_file, size, output_dir, vocab_path):\n",
      "    if preset is not None:\n",
      "        hf_id = PRESET_MAP[preset]\n",
      "        print(f\"\\n-> Loading KerasNLP Gemma model with preset `{preset}`...\")\n",
      "        keras_nlp_model = keras_nlp.models.GemmaCausalLM.from_preset(preset)\n",
      "    else:\n",
      "        hf_id, keras_preset = SIZE_MAP[size.lower()]\n",
      "        print(f\"\\n-> Loading Keras weights from file `{weights_file}`...\")\n",
      "        keras_nlp_model = keras_nlp.models.GemmaCausalLM.from_preset(\n",
      "            keras_preset\n",
      "        )\n",
      "        keras_nlp_model.load_weights(weights_file)\n",
      "\n",
      "    print(f\"\\n-> Loading HuggingFace Gemma `{size.upper()}` model...\")\n",
      "    hf_model = transformers.GemmaForCausalLM(CONFIG_MAPPING[size.lower()])\n",
      "\n",
      "    print(\"\\nâœ… Model loading complete.\")\n",
      "    print(\"\\n-> Converting weights from KerasNLP Gemma to HuggingFace Gemma...\")\n",
      "\n",
      "    # Token embedding (with vocab size difference handling)\n",
      "    keras_embedding = keras_nlp_model.backbone.token_embedding.weights[0]\n",
      "    hf_vocab_size = hf_model.model.embed_tokens.weight.shape[0]\n",
      "    keras_nlp_vocab_size = keras_embedding.value.shape[0]\n",
      "    if hf_vocab_size < keras_nlp_vocab_size:\n",
      "        diff = keras_nlp_vocab_size - hf_vocab_size\n",
      "        update_state_dict(\n",
      "            hf_model.model.embed_tokens,\n",
      "            \"weight\",\n",
      "            keras_embedding.value[:-diff, :],\n",
      "        )\n",
      "    else:\n",
      "        update_state_dict(\n",
      "            hf_model.model.embed_tokens,\n",
      "            \"weight\",\n",
      "            keras_embedding.value,\n",
      "        )\n",
      "\n",
      "    # Decoder blocks\n",
      "    for i in range(keras_nlp_model.backbone.num_layers):\n",
      "        decoder_block = keras_nlp_model.backbone.get_layer(f\"decoder_block_{i}\")\n",
      "\n",
      "        # Pre-attention norm\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].input_layernorm,\n",
      "            \"weight\",\n",
      "            decoder_block.pre_attention_norm.weights[0].value,\n",
      "        )\n",
      "\n",
      "        # Attention\n",
      "        query_target_shape = hf_model.model.layers[\n",
      "            i\n",
      "        ].self_attn.q_proj.weight.shape\n",
      "        query_tensor = decoder_block.attention.query_dense.weights[0].value\n",
      "        query_tensor = query_tensor.transpose(1, 2).reshape(query_target_shape)\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].self_attn.q_proj, \"weight\", query_tensor\n",
      "        )\n",
      "\n",
      "        key_target_shape = hf_model.model.layers[\n",
      "            i\n",
      "        ].self_attn.k_proj.weight.shape\n",
      "        key_tensor = decoder_block.attention.key_dense.weights[0].value\n",
      "        key_tensor = key_tensor.transpose(1, 2).reshape(key_target_shape)\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].self_attn.k_proj, \"weight\", key_tensor\n",
      "        )\n",
      "\n",
      "        value_target_shape = hf_model.model.layers[\n",
      "            i\n",
      "        ].self_attn.v_proj.weight.shape\n",
      "        value_tensor = decoder_block.attention.value_dense.weights[0].value\n",
      "        value_tensor = value_tensor.transpose(1, 2).reshape(value_target_shape)\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].self_attn.v_proj, \"weight\", value_tensor\n",
      "        )\n",
      "\n",
      "        out_target_shape = hf_model.model.layers[\n",
      "            i\n",
      "        ].self_attn.o_proj.weight.shape\n",
      "        keras_out_tensor = decoder_block.attention.output_dense.weights[0].value\n",
      "        out_tensor = keras_out_tensor.reshape(\n",
      "            (out_target_shape[1], out_target_shape[0])  # Transpose target size\n",
      "        ).transpose(0, 1)\n",
      "\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].self_attn.o_proj, \"weight\", out_tensor\n",
      "        )\n",
      "\n",
      "        # Post-attention norm\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].post_attention_layernorm,\n",
      "            \"weight\",\n",
      "            decoder_block.pre_ffw_norm.weights[0].value,\n",
      "        )\n",
      "\n",
      "        # MLP (Feed-forward)\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].mlp.gate_proj,\n",
      "            \"weight\",\n",
      "            decoder_block.gating_ffw.weights[0].value.transpose(0, 1),\n",
      "        )\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].mlp.up_proj,\n",
      "            \"weight\",\n",
      "            decoder_block.gating_ffw_2.weights[0].value.transpose(0, 1),\n",
      "        )\n",
      "        update_state_dict(\n",
      "            hf_model.model.layers[i].mlp.down_proj,\n",
      "            \"weight\",\n",
      "            decoder_block.ffw_linear.weights[0].value.transpose(0, 1),\n",
      "        )\n",
      "\n",
      "    # Final norm\n",
      "    update_state_dict(\n",
      "        hf_model.model.norm,\n",
      "        \"weight\",\n",
      "        keras_nlp_model.backbone.layers[-1].weights[0].value,\n",
      "    )\n",
      "\n",
      "    print(\"\\nâœ… Weights converted successfully.\")\n",
      "    print(f\"\\n-> Saving HuggingFace model to `{output_dir}`...\")\n",
      "\n",
      "    # Save model to HF Transformers format\n",
      "    os.makedirs(output_dir, exist_ok=True)\n",
      "    hf_model.save_pretrained(output_dir)\n",
      "\n",
      "    print(f\"\\nâœ… Saving complete. Model saved at `{output_dir}`.\")\n",
      "\n",
      "    # Tokenizer\n",
      "\n",
      "    if not vocab_path:\n",
      "        tokenizer_preset = preset or SIZE_MAP[size.lower()]\n",
      "        print(\n",
      "            \"\\n-> Loading KerasNLP Gemma tokenizer with \"\n",
      "            f\"preset `{tokenizer_preset}`...\"\n",
      "        )\n",
      "        keras_nlp_tokenizer = keras_nlp.models.GemmaTokenizer.from_preset(\n",
      "            tokenizer_preset\n",
      "        )\n",
      "        # Save tokenizer state\n",
      "        keras_nlp_tokenizer.save_assets(output_dir)\n",
      "        vocab_path = os.path.join(output_dir, \"vocabulary.spm\")\n",
      "        print(\"\\nâœ… Tokenizer loading complete.\")\n",
      "\n",
      "    hf_tokenizer = transformers.GemmaTokenizer(vocab_path)\n",
      "\n",
      "    print(f\"\\n-> Saving HuggingFace Gemma tokenizer to `{output_dir}`...\")\n",
      "    # Save tokenizer to HF Transformers format\n",
      "    hf_tokenizer.save_pretrained(output_dir)\n",
      "\n",
      "    print(f\"\\nâœ… Saving complete. Tokenizer saved at `{output_dir}`.\")\n",
      "\n",
      "\n",
      "def update_state_dict(layer, weight_name: str, tensor: torch.Tensor) -> None:\n",
      "    \"\"\"Updates the state dict for a weight given a tensor.\"\"\"\n",
      "    assert (\n",
      "        tensor.shape == layer.state_dict()[weight_name].shape\n",
      "    ), f\"{tensor.shape} vs {layer.state_dict()[weight_name].shape}\"\n",
      "    layer.state_dict()[weight_name].copy_(tensor)\n",
      "\n",
      "\n",
      "def flag_error_handler():\n",
      "    if not FLAGS.preset and not FLAGS.weights_file:\n",
      "        raise ValueError(\n",
      "            \"Please pass either a valid Keras preset to `--preset`\"\n",
      "            \" or supply a Keras weights file (`.weights.h5`) and model size\"\n",
      "            \" (`2b` or `7b`) to `--weights_file` and `--size`, respectively.\"\n",
      "        )\n",
      "    if FLAGS.weights_file:\n",
      "        if FLAGS.preset:\n",
      "            raise ValueError(\n",
      "                \"Both `--preset` and `--weights_file` flags cannot be supplied \"\n",
      "                \"at the same time. Either supply a valid Keras preset to \"\n",
      "                \"`--preset`or supply a Keras `.weights.h5` file and \"\n",
      "                \"model size (`2b` or `7b`) to `--weights_file` and `--size`, \"\n",
      "                \"respectively.\"\n",
      "            )\n",
      "        if not str(FLAGS.weights_file).endswith(\".weights.h5\"):\n",
      "            raise ValueError(\n",
      "                \"Please pass a valid Keras weights file ending in `.weights.h5`.\"\n",
      "            )\n",
      "        if not FLAGS.size:\n",
      "            raise ValueError(\n",
      "                \"The `size` flag must be passed if a weights file is passed. \"\n",
      "                \"Please pass the appropriate size (`2b` or `7b`) for your \"\n",
      "                \"model to the `--size` flag.\"\n",
      "            )\n",
      "        if FLAGS.size.lower() not in [\"2b\", \"7b\"]:\n",
      "            raise ValueError(\n",
      "                \"Invalid `size`. Please pass the appropriate size (`2b` or `7b`) \"\n",
      "                \"for your model to the `--size` flag.\"\n",
      "            )\n",
      "    if FLAGS.dtype:\n",
      "        dtype = getattr(torch, FLAGS.dtype)\n",
      "        if not isinstance(dtype, torch.dtype):\n",
      "            raise ValueError(\n",
      "                \"Invalid `dtype`. Please pass a valid PyTorch data type (e.g. \"\n",
      "                \"`float32', 'float16`, etc.) to the `--dtype` flag.\"\n",
      "            )\n",
      "\n",
      "\n",
      "def main(_):\n",
      "    flag_error_handler()\n",
      "    with _set_default_tensor_type(getattr(torch, FLAGS.dtype)):\n",
      "        convert_checkpoints(\n",
      "            FLAGS.preset,\n",
      "            FLAGS.weights_file,\n",
      "            FLAGS.size,\n",
      "            FLAGS.output_dir,\n",
      "            FLAGS.vocab_path,\n",
      "        )\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    flags.mark_flag_as_required(\"size\")\n",
      "    app.run(main)\n",
      "\n",
      "--- File: ./components/app_flask/Dockerfile ---\n",
      "FROM python:3.9-slim\n",
      "\n",
      "WORKDIR /root\n",
      "RUN pwd\n",
      "RUN ls\n",
      "\n",
      "COPY requirements.txt .\n",
      "RUN pip install -U -r requirements.txt\n",
      "RUN ls\n",
      "\n",
      "COPY . /app \n",
      "\n",
      "WORKDIR /app\n",
      "\n",
      "EXPOSE 5000\n",
      "\n",
      "RUN pwd\n",
      "RUN ls\n",
      "\n",
      "CMD [\"python\", \"-m\", \"app.app\"]  \n",
      "--- File: ./components/app_flask/app/util.py ---\n",
      "\n",
      "--- File: ./components/app_flask/app/app.py ---\n",
      "from flask import Flask, render_template, request, jsonify, redirect, url_for, session\n",
      "from google.cloud import bigquery, storage, pubsub_v1\n",
      "from werkzeug.datastructures import FileStorage\n",
      "import bcrypt, os, base64, json\n",
      "import logging\n",
      "\n",
      "logger = logging.getLogger(__name__)  # Use the function's module name\n",
      "logger.setLevel(logging.DEBUG)\n",
      "\n",
      "app = Flask(__name__)\n",
      "app.secret_key = os.urandom(24)\n",
      "\n",
      "# Copyright 2020 Google LLC\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     https://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\n",
      "# [START aiplatform_predict_custom_trained_model_sample]\n",
      "from typing import Dict, List, Union\n",
      "from google.cloud import aiplatform\n",
      "from google.protobuf import json_format\n",
      "from google.protobuf.struct_pb2 import Value\n",
      "import re\n",
      "\n",
      "# Connect to BigQuery\n",
      "os.environ['PROJECT_ID'] = 'able-analyst-416817'\n",
      "DATASET_ID = 'chatbot'\n",
      "USERS_TABLE = 'users'\n",
      "USER_TRAINING_STATUS = 'user_training_status'\n",
      "BUCKET_NAME = \"personalize-chatbots-v1\"\n",
      "print(BUCKET_NAME)\n",
      "PUBSUB_TOPIC = 'your-pipeline-trigger-topic'\n",
      "\n",
      "def predict_custom_trained_model_sample(\n",
      "    project: str,\n",
      "    endpoint_id: str,\n",
      "    location: str = \"us-central1\",\n",
      "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
      "    user_input: str = input\n",
      "):\n",
      "    logger.debug(\"Function predict_custom_trained_model_sample started\")\n",
      "    \"\"\"\n",
      "    `instances` can be either single instance of type dict or a list\n",
      "    of instances.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        prompt_input = f\"Sender:\\n{user_input}\\n\\nAndres Perez:\\n\"\n",
      "        # The two below should be a parameter.\n",
      "        conversation_track = session.get('conversation_track')[-3:]\n",
      "        # conversation_track_keeper = conversation_track\n",
      "        print(\"These are the last two values\", conversation_track)\n",
      "        conversation_track_str = \"\\n\\n\".join(conversation_track  + [prompt_input])\n",
      "        print(\"This is the joined input for prediction\")\n",
      "        print(conversation_track_str)\n",
      "        instances={'prompt': conversation_track_str, 'max_tokens': 1024, 'temperature': 1, 'top_p': 0.7, 'top_k': 6}\n",
      "\n",
      "        # The AI Platform services require regional API endpoints.\n",
      "        client_options = {\"api_endpoint\": api_endpoint}\n",
      "        # Initialize client that will be used to create and send requests.\n",
      "        # This client only needs to be created once, and can be reused for multiple requests.\n",
      "        client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
      "        # The format of each instance should conform to the deployed model's prediction input schema.\n",
      "        instances = instances if isinstance(instances, list) else [instances]\n",
      "        instances = [\n",
      "            json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
      "        ]\n",
      "        parameters_dict = {}\n",
      "        parameters = json_format.ParseDict(parameters_dict, Value())\n",
      "        endpoint = client.endpoint_path(\n",
      "            project=project, location=location, endpoint=endpoint_id\n",
      "        )\n",
      "\n",
      "        response = client.predict(\n",
      "            endpoint=endpoint, instances=instances, parameters=parameters\n",
      "        )\n",
      "        # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
      "        predictions = response.predictions\n",
      "        pattern = r\"Perez:\\nOutput:\\n(.*)\"\n",
      "        match = re.search(pattern, predictions[0])\n",
      "\n",
      "        if match:\n",
      "            logger.info(f\"Successful prediction: {match.group(1)}\")\n",
      "            print(conversation_track)\n",
      "            print(\"This is the last one\")\n",
      "            print(prompt_input + str(match.group(1)))\n",
      "            conversation_track.append(prompt_input + str(match.group(1)))\n",
      "            print(\"This is the 3 tracker\", conversation_track)\n",
      "            session['conversation_track'] = conversation_track\n",
      "            return match.group(1)\n",
      "        else:\n",
      "            logger.error(\"Prediction not found in the response.\")\n",
      "            return \"Error: Prediction not found in the response.\"\n",
      "\n",
      "    except Exception as e:\n",
      "        logger.exception(f\"An error occurred: {e}\")\n",
      "        raise  # Re-raise to allow for error handling at a higher level\n",
      "    \n",
      "def extract_info_from_endpoint(url):\n",
      "    \"\"\"Extracts location, endpoint, and project information from a given AIPlatform URL.\n",
      "\n",
      "    Args:\n",
      "        url: The AIPlatform URL string.\n",
      "\n",
      "    Returns:\n",
      "        A dictionary containing the extracted values:\n",
      "            locations: The region.\n",
      "            endpoints: The endpoint ID.\n",
      "            projects: The project ID.\n",
      "    \"\"\"\n",
      "\n",
      "    pattern = r\"\\/projects\\/([^\\/]+)\\/locations\\/([^\\/]+)\\/endpoints\\/([^\\/]+)\\/operations\\/([^\\/]+)\"\n",
      "    print(pattern)\n",
      "    match = re.search(pattern, url)\n",
      "    print(match)\n",
      "    if match:\n",
      "        return {\n",
      "            \"projects\": match.group(1),\n",
      "            \"locations\": match.group(2),\n",
      "            \"endpoints\": match.group(3)\n",
      "        }\n",
      "    else:\n",
      "        return None  # Or you could raise an exception if the URL is invalid\n",
      "\n",
      "@app.route('/signup', methods=['POST'])\n",
      "def signup():\n",
      "    if request.method == 'POST':\n",
      "        email = request.form['email']\n",
      "        password = request.form['password']\n",
      "        confirm_password = request.form['confirm_password']\n",
      "\n",
      "        # Data validation\n",
      "        error_message = None\n",
      "        if not email or not password or not confirm_password:\n",
      "            error_message = 'Please fill in all fields.'\n",
      "        elif password != confirm_password:\n",
      "            error_message = 'Passwords do not match.'\n",
      "        # Add more validation rules as needed (e.g., email format, password strength)\n",
      "\n",
      "        if error_message:\n",
      "            return error_message, 400\n",
      "\n",
      "        # Hash the password for security\n",
      "        hashed_password = bcrypt.hashpw(password.encode('utf-8'), bcrypt.gensalt())\n",
      "        \n",
      "        client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "        table_ref = client.dataset(DATASET_ID).table(USERS_TABLE)\n",
      "        table = client.get_table(table_ref)\n",
      "        row_to_insert = {\n",
      "            'email': email, \n",
      "            'password_hash': hashed_password.decode('utf-8')\n",
      "        }\n",
      "        client.insert_rows(table, [row_to_insert]) \n",
      "        errors = client.insert_rows(table, [row_to_insert]) \n",
      "        if errors:  # Check if there were errors\n",
      "            return 'Error submitting data: {}'.format(errors), 500\n",
      "        else:\n",
      "            # session['user_id'] = user_id  # Assuming you fetched the user's ID\n",
      "            session['email'] = email \n",
      "            return redirect(url_for('upload'))\n",
      "\n",
      "@app.route('/login', methods=['POST'])\n",
      "def login():\n",
      "    print(\"Si entro en este pedo\")\n",
      "    if request.method == 'POST':\n",
      "        email = request.form['email']\n",
      "        print(\"Si entro en este email\", email)\n",
      "        password = request.form['password']\n",
      "        print(\"Si entro en este password\", password)\n",
      "\n",
      "        # Fetch user data from BigQuery\n",
      "        client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "\n",
      "        # Check passwords.\n",
      "        query = f\"SELECT password_hash FROM `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USERS_TABLE}` WHERE email = '{email}'\"\n",
      "        print(\"This is el query...\", query)\n",
      "        results = client.query(query).result()\n",
      "        print(\"This is the results\", results, type(results))\n",
      "        stored_password_hash = None\n",
      "        for row in results:\n",
      "            print(\"This is each row\", row)\n",
      "            stored_password_hash = row.password_hash  # Assuming 'password' is the column name\n",
      "        # Check if user already trained.\n",
      "        query = f\"\"\"\n",
      "            SELECT training_status, end_point\n",
      "            FROM `{os.environ.get('PROJECT_ID')}.{DATASET_ID}.{USER_TRAINING_STATUS}`\n",
      "            WHERE email = '{email}'\n",
      "            ORDER BY created_at DESC\n",
      "            LIMIT 1\n",
      "        \"\"\"\n",
      "        print(\"This is el query...\", query)\n",
      "        results = client.query(query).result()\n",
      "        print(\"This is the results\", results, type(results))\n",
      "        user_training_status = None\n",
      "        endpoint_uri = None\n",
      "        for row in results:\n",
      "            print(\"This is each row\", row)\n",
      "            user_training_status = row.training_status  # Assuming 'training_status' is the column name\n",
      "            endpoint_uri = row.end_point\n",
      "        # Verify password\n",
      "        if stored_password_hash and bcrypt.checkpw(password.encode('utf-8'), stored_password_hash.encode('utf-8')):\n",
      "            session['email'] = email\n",
      "            if user_training_status:\n",
      "                endpoint_details = extract_info_from_endpoint(endpoint_uri)\n",
      "                print(\"This is the email\", email)\n",
      "                session['endpoint'] = endpoint_details[\"endpoints\"]\n",
      "                session['location'] = endpoint_details[\"locations\"]\n",
      "                session['project'] = endpoint_details[\"projects\"]\n",
      "                print(f\"This is the endpoint{session['endpoint']}, projects{session['project']}, locations{session['location']}\")\n",
      "                print(user_training_status)\n",
      "                return redirect(url_for('chat_page'))\n",
      "            else:\n",
      "                return redirect(url_for('upload'))\n",
      "            # Successful login\n",
      "        else:\n",
      "            # Invalid credentials\n",
      "            return 'Invalid email or password', 401\n",
      "\n",
      "@app.route('/upload')\n",
      "def upload():\n",
      "    email = session.get('email')\n",
      "    print(\"This is the email, ahuevito\", email)\n",
      "    if not email:  \n",
      "        # Redirect to login if not logged in\n",
      "        print(\"Nos regresamos al home\")\n",
      "        return redirect(url_for('home'))\n",
      "    print(\"Aqui llegaaaa\")\n",
      "    return render_template('upload.html')\n",
      "\n",
      "@app.route('/handle_upload', methods=['POST'])\n",
      "def handle_upload():\n",
      "    files_metadata = []\n",
      "    email = session.get('email')\n",
      "    print(\"This is the email, ahuevito\", email)\n",
      "    print(type(email))\n",
      "    code_version = request.form.get('code_version')\n",
      "    print(\"This is the code version\", code_version)\n",
      "    model_name = request.form.get('model_name')\n",
      "    epochs = request.form.get('epochs')\n",
      "    print(f\"Selected model: {model_name}, Epochs: {epochs}\")\n",
      "    user_name = re.match(r'^([^@]+)', str(email)).group(1)\n",
      "    print(\"This is the code version\", code_version)\n",
      "\n",
      "    publisher = pubsub_v1.PublisherClient()\n",
      "    topic_path = publisher.topic_path(os.environ.get('PROJECT_ID'), PUBSUB_TOPIC)\n",
      "    print(\"This is the topic path\", topic_path)\n",
      "    blob_folder = os.path.join(user_name, 'input_data')\n",
      "\n",
      "    if not email:\n",
      "        print(\"Nos regresamos al home\")\n",
      "        # Redirect to login if not logged in\n",
      "        return redirect(url_for('home'))\n",
      "    print(\"Creo que si jalo, python\")\n",
      "    for file in request.files.getlist('files'):\n",
      "        print(file)\n",
      "        print(\"This is the type\", type(file))\n",
      "        client = storage.Client(os.environ.get('PROJECT_ID'))\n",
      "        bucket = client.get_bucket(BUCKET_NAME)\n",
      "        blob_string = os.path.join(blob_folder, file.filename)\n",
      "        blob = bucket.blob(blob_string)\n",
      "        blob.upload_from_string(FileStorage(file).stream.read())\n",
      "        print(\"Uploading\", file.filename, blob.name)\n",
      "        files_metadata.append({\n",
      "            \"file_path\": f\"gs://{BUCKET_NAME}/{blob_string}\",\n",
      "            \"filename\": file.filename  # Add filename to metadata\n",
      "        })\n",
      "\n",
      "    # After all uploads are complete, prepare the message\n",
      "    message_data = {\n",
      "        \"user_name\": user_name,\n",
      "        \"files\": files_metadata,\n",
      "        \"blob_folder\": blob_folder,\n",
      "        \"model_name\": model_name,\n",
      "        \"epochs\": epochs,\n",
      "        \"bucket_name\": BUCKET_NAME,\n",
      "        \"tag_version\": code_version,\n",
      "        \"project_id\": os.environ.get('PROJECT_ID')\n",
      "        \n",
      "    }\n",
      "    message_data_json = json.dumps(message_data)\n",
      "    message_data_bytes = message_data_json.encode('utf-8')\n",
      "    print(message_data_bytes)\n",
      "    publisher.publish(topic_path, message_data_bytes)\n",
      "    \n",
      "    client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
      "    table_ref = client.dataset(DATASET_ID).table(USER_TRAINING_STATUS)\n",
      "    table = client.get_table(table_ref)\n",
      "    row_to_insert = {\n",
      "        'email': email,\n",
      "        'training_status': False\n",
      "    }\n",
      "    print(\"This is the rows to upload\", row_to_insert)\n",
      "    client.insert_rows(table, [row_to_insert]) \n",
      "    errors = client.insert_rows(table, [row_to_insert]) \n",
      "    if errors:  # Check if there were errors\n",
      "        return 'Error submitting data: {}'.format(errors), 500\n",
      "    else:\n",
      "        print(\"Upload Successful!\")\n",
      "        return \"Upload Successful!\", 200 \n",
      "     \n",
      "\n",
      "@app.route('/home')\n",
      "def home():\n",
      "    return render_template('index.html')\n",
      "\n",
      "\n",
      "@app.route('/chat_page')\n",
      "def chat_page():\n",
      "    email = session.get('email')\n",
      "    if not email:  \n",
      "        # Redirect to login if not logged in\n",
      "        print(\"Nos regresamos al home\")\n",
      "        return redirect(url_for('home'))\n",
      "    session['conversation_track'] = []\n",
      "    return render_template('chat.html')\n",
      "\n",
      "@app.route('/send_message', methods=['POST'])\n",
      "def send_message():\n",
      "    user_message = request.json['message']\n",
      "    logger.debug(f\"Received user message: {user_message}\")\n",
      "    email = session.get('email')\n",
      "    if not email:  \n",
      "        logger.info(\"User not logged in. Redirecting to home\")\n",
      "        return redirect(url_for('home'))\n",
      "\n",
      "    endpoint = session.get('endpoint')\n",
      "    project = session.get('project')\n",
      "    location = session.get('location')\n",
      "    logger.debug(f\"Session data: endpoint={endpoint}, project={project}, location={location}\")\n",
      "    try:\n",
      "        response = predict_custom_trained_model_sample(\n",
      "            project=project,\n",
      "            endpoint_id=endpoint,\n",
      "            location=location,\n",
      "            user_input= user_message,\n",
      "        )\n",
      "    except Exception as e:\n",
      "        logger.exception(f\"Error in chatbot prediction: {e}\")\n",
      "        response = \"An error occurred. Please try again later.\"\n",
      "    # logger.debug(f\"Returning JSON response: {'message': response}\")\n",
      "    return jsonify({'message': response})\n",
      "\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    print(\"Pues si empezo a correr esta madre\")\n",
      "    app.run(host='0.0.0.0', port=5000, debug=True) \n",
      "--- File: ./components/app_flask/app/__ini__.py ---\n",
      "\n",
      "--- File: ./components/app_flask/app/static/upload.js ---\n",
      "const fileInput = document.getElementById('fileInput');\n",
      "const codeVersion = document.getElementById('codeVersion');\n",
      "const uploadButton = document.getElementById('uploadButton');\n",
      "const preview = document.getElementById('preview');\n",
      "const modelSelect = document.getElementById('model_name');\n",
      "const epochsSelect = document.getElementById('epochs');\n",
      "\n",
      "\n",
      "uploadButton.addEventListener('click', () => {\n",
      "    const files = fileInput.files;\n",
      "    const code_version = codeVersion.value;\n",
      "    const model_name = modelSelect.value;\n",
      "    const epochs = epochsSelect.value;\n",
      "\n",
      "    // Basic preview (you can enhance this)\n",
      "    preview.innerHTML = ''; // Clear previous previews \n",
      "    for (let i = 0; i < files.length; i++) {\n",
      "        preview.innerHTML += `<p>${files[i].name}</p>`;\n",
      "    }\n",
      "\n",
      "    // Create form data for sending to Flask Server\n",
      "    const formData = new FormData();\n",
      "    for (let i = 0; i < files.length; i++) {\n",
      "        formData.append('files', files[i]);\n",
      "    }\n",
      "    formData.append('code_version', code_version);\n",
      "    formData.append('model_name', model_name);\n",
      "    formData.append('epochs', epochs);\n",
      "    console.log(\"Hasta aqui jala bien\");\n",
      "    // Send to Flask using Fetch API (example)\n",
      "    fetch('/handle_upload', { \n",
      "        method: 'POST',\n",
      "        body: formData\n",
      "    })\n",
      "    .then(response => {\n",
      "        if (response.ok) {\n",
      "          console.log('File uploaded successfully!');\n",
      "          // *** Show the notification ***\n",
      "          const notification = document.getElementById('upload-notification');\n",
      "          notification.style.display = 'block'; // Show it\n",
      "          // Optionally hide after a few seconds\n",
      "          setTimeout(() => {\n",
      "            notification.style.display = 'none';\n",
      "              window.location.href = '/home';\n",
      "          }, 5000); // Hide after 3 seconds\n",
      "        } else {\n",
      "          console.error('Upload failed:', response.statusText);\n",
      "        }\n",
      "    });\n",
      "});\n",
      "\n",
      "// Make sure epochsSelect is correctly referencing the DOM element\n",
      "\n",
      "for (let i = 1; i <= 20; i++) {\n",
      "    const option = document.createElement('option');\n",
      "    option.value = i;\n",
      "    option.text = i;\n",
      "    epochsSelect.appendChild(option);\n",
      "}\n",
      "--- File: ./components/app_flask/app/static/upload.css ---\n",
      "/* Overall Styling */\n",
      "body {\n",
      "  font-family: Arial, sans-serif;\n",
      "  background-color: #d9f2e6; /* Soft, light background */\n",
      "  margin: 0;\n",
      "  padding: 25px;\n",
      "}\n",
      "\n",
      ".container {\n",
      "  width: 450px; /* Adjust width as needed */\n",
      "  margin: 50px auto;\n",
      "  padding: 30px;\n",
      "  background-color: #fff; /* White background */\n",
      "  border-radius: 8px;\n",
      "  box-shadow: 0px 4px 12px rgba(0, 0, 0, 0.1);\n",
      "}\n",
      "\n",
      ".container h1 {\n",
      "  color: #333;  /* Dark grey header */\n",
      "  text-align: center;\n",
      "  margin-bottom: 30px;\n",
      "}\n",
      "\n",
      "/* Input Elements and Labels */\n",
      "label {\n",
      "  font-weight: bold;\n",
      "  display: block;\n",
      "  margin-bottom: 5px;\n",
      "}\n",
      "\n",
      "input[type=\"file\"], \n",
      "input[type=\"text\"], \n",
      "select {\n",
      "  width: 100%; \n",
      "  padding: 12px;\n",
      "  border: 1px solid #ddd;\n",
      "  border-radius: 4px;\n",
      "  margin-bottom: 15px;\n",
      "  box-sizing: border-box;\n",
      "} \n",
      "\n",
      "/* Upload Button */\n",
      "#uploadButton {\n",
      "  background-color: #008cba; /* Blue button */\n",
      "  color: white;\n",
      "  padding: 15px 30px;\n",
      "  border: none;\n",
      "  border-radius: 5px;\n",
      "  cursor: pointer;\n",
      "  font-weight: 600;  \n",
      "  transition: background-color 0.3s; /* Smooth transition */\n",
      "  margin: 0 auto; /* Center the button within its container */\n",
      "  display: block; /* Treat the button like a block element */\n",
      "}\n",
      "\n",
      "#uploadButton:hover {\n",
      "background-color: #006ba1; /* Darker blue on hover */\n",
      "/* No specific centering needed - the existing 'margin: 0 auto' still applies */\n",
      "}\n",
      "\n",
      "/* Upload Notification */\n",
      "#upload-notification {\n",
      "  background-color: #d9f2e6; /* Light green */\n",
      "  color: #206a4f; /* Dark green text */\n",
      "  border: 1px solid #b7e4cf;\n",
      "  padding: 25px; /* Increased padding for more space */\n",
      "  border-radius: 8px; /* Slightly rounded corners */\n",
      "  position: fixed;\n",
      "  top: 50%;\n",
      "  left: 50%;\n",
      "  transform: translate(-50%, -50%); /* Precise centering */\n",
      "  text-align: center;\n",
      "  box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.2); /* More prominent shadow */\n",
      "  width: 400px; /* Adjust the width as needed */\n",
      "  z-index: 1000; /* Ensure it's on top of other elements */\n",
      "}\n",
      "\n",
      "--- File: ./components/app_flask/app/static/style.css ---\n",
      " @import url('https://fonts.googleapis.com/css?family=Poppins:400,500,600,700&display=swap');\n",
      "*{\n",
      "  margin: 0;\n",
      "  padding: 0;\n",
      "  box-sizing: border-box;\n",
      "  font-family: 'Poppins', sans-serif;\n",
      "}\n",
      "html,body{\n",
      "  display: grid;\n",
      "  height: 100%;\n",
      "  width: 100%;\n",
      "  place-items: center;\n",
      "  background: -webkit-linear-gradient(left, #b7e4cf, #d9f2e6);\n",
      "}\n",
      "::selection{\n",
      "  background: #1a75ff;\n",
      "  color: #fff;\n",
      "}\n",
      ".wrapper{\n",
      "  overflow: hidden;\n",
      "  max-width: 390px;\n",
      "  background: #fff;\n",
      "  padding: 30px;\n",
      "  border-radius: 15px;\n",
      "  box-shadow: 0px 15px 20px rgba(0,0,0,0.1);\n",
      "}\n",
      ".wrapper .title-text{\n",
      "  display: flex;\n",
      "  width: 200%;\n",
      "}\n",
      ".wrapper .title{\n",
      "  width: 50%;\n",
      "  font-size: 35px;\n",
      "  font-weight: 600;\n",
      "  text-align: center;\n",
      "  transition: all 0.6s cubic-bezier(0.68,-0.55,0.265,1.55);\n",
      "}\n",
      ".wrapper .slide-controls{\n",
      "  position: relative;\n",
      "  display: flex;\n",
      "  height: 50px;\n",
      "  width: 100%;\n",
      "  overflow: hidden;\n",
      "  margin: 30px 0 10px 0;\n",
      "  justify-content: space-between;\n",
      "  border: 1px solid lightgrey;\n",
      "  border-radius: 15px;\n",
      "}\n",
      ".slide-controls .slide{\n",
      "  height: 100%;\n",
      "  width: 100%;\n",
      "  color: #fff;\n",
      "  font-size: 18px;\n",
      "  font-weight: 500;\n",
      "  text-align: center;\n",
      "  line-height: 48px;\n",
      "  cursor: pointer;\n",
      "  z-index: 1;\n",
      "  transition: all 0.6s ease;\n",
      "}\n",
      ".slide-controls label.signup{\n",
      "  color: #000;\n",
      "}\n",
      ".slide-controls .slider-tab{\n",
      "  position: absolute;\n",
      "  height: 100%;\n",
      "  width: 50%;\n",
      "  left: 0;\n",
      "  z-index: 0;\n",
      "  border-radius: 15px;\n",
      "  background: -webkit-linear-gradient(left,#003366,#004080,#0059b3\n",
      ", #0073e6);\n",
      "  transition: all 0.6s cubic-bezier(0.68,-0.55,0.265,1.55);\n",
      "}\n",
      "input[type=\"radio\"]{\n",
      "  display: none;\n",
      "}\n",
      "#signup:checked ~ .slider-tab{\n",
      "  left: 50%;\n",
      "}\n",
      "#signup:checked ~ label.signup{\n",
      "  color: #fff;\n",
      "  cursor: default;\n",
      "  user-select: none;\n",
      "}\n",
      "#signup:checked ~ label.login{\n",
      "  color: #000;\n",
      "}\n",
      "#login:checked ~ label.signup{\n",
      "  color: #000;\n",
      "}\n",
      "#login:checked ~ label.login{\n",
      "  cursor: default;\n",
      "  user-select: none;\n",
      "}\n",
      ".wrapper .form-container{\n",
      "  width: 100%;\n",
      "  overflow: hidden;\n",
      "}\n",
      ".form-container .form-inner{\n",
      "  display: flex;\n",
      "  width: 200%;\n",
      "}\n",
      ".form-container .form-inner form{\n",
      "  width: 50%;\n",
      "  transition: all 0.6s cubic-bezier(0.68,-0.55,0.265,1.55);\n",
      "}\n",
      ".form-inner form .field{\n",
      "  height: 50px;\n",
      "  width: 100%;\n",
      "  margin-top: 20px;\n",
      "}\n",
      ".form-inner form .field input{\n",
      "  height: 100%;\n",
      "  width: 100%;\n",
      "  outline: none;\n",
      "  padding-left: 15px;\n",
      "  border-radius: 15px;\n",
      "  border: 1px solid lightgrey;\n",
      "  border-bottom-width: 2px;\n",
      "  font-size: 17px;\n",
      "  transition: all 0.3s ease;\n",
      "}\n",
      ".form-inner form .field input:focus{\n",
      "  border-color: #1a75ff;\n",
      "  /* box-shadow: inset 0 0 3px #fb6aae; */\n",
      "}\n",
      ".form-inner form .field input::placeholder{\n",
      "  color: #999;\n",
      "  transition: all 0.3s ease;\n",
      "}\n",
      "form .field input:focus::placeholder{\n",
      "  color: #1a75ff;\n",
      "}\n",
      ".form-inner form .pass-link{\n",
      "  margin-top: 5px;\n",
      "}\n",
      ".form-inner form .signup-link{\n",
      "  text-align: center;\n",
      "  margin-top: 30px;\n",
      "}\n",
      ".form-inner form .pass-link a,\n",
      ".form-inner form .signup-link a{\n",
      "  color: #1a75ff;\n",
      "  text-decoration: none;\n",
      "}\n",
      ".form-inner form .pass-link a:hover,\n",
      ".form-inner form .signup-link a:hover{\n",
      "  text-decoration: underline;\n",
      "}\n",
      "form .btn{\n",
      "  height: 50px;\n",
      "  width: 100%;\n",
      "  border-radius: 15px;\n",
      "  position: relative;\n",
      "  overflow: hidden;\n",
      "}\n",
      "form .btn .btn-layer{\n",
      "  height: 100%;\n",
      "  width: 300%;\n",
      "  position: absolute;\n",
      "  left: -100%;\n",
      "  background: -webkit-linear-gradient(right,#003366,#004080,#0059b3\n",
      ", #0073e6);\n",
      "  border-radius: 15px;\n",
      "  transition: all 0.4s ease;;\n",
      "}\n",
      "form .btn:hover .btn-layer{\n",
      "  left: 0;\n",
      "}\n",
      "form .btn input[type=\"submit\"]{\n",
      "  height: 100%;\n",
      "  width: 100%;\n",
      "  z-index: 1;\n",
      "  position: relative;\n",
      "  background: none;\n",
      "  border: none;\n",
      "  color: #fff;\n",
      "  padding-left: 0;\n",
      "  border-radius: 15px;\n",
      "  font-size: 20px;\n",
      "  font-weight: 500;\n",
      "  cursor: pointer;\n",
      "}\n",
      "\n",
      "--- File: ./components/app_flask/app/static/script.js ---\n",
      " const loginText = document.querySelector(\".title-text .login\");\n",
      "      const loginForm = document.querySelector(\"form.login\");\n",
      "      const loginBtn = document.querySelector(\"label.login\");\n",
      "      const signupBtn = document.querySelector(\"label.signup\");\n",
      "      const signupLink = document.querySelector(\"form .signup-link a\");\n",
      "      signupBtn.onclick = (()=>{\n",
      "        loginForm.style.marginLeft = \"-50%\";\n",
      "        loginText.style.marginLeft = \"-50%\";\n",
      "      });\n",
      "      loginBtn.onclick = (()=>{\n",
      "        loginForm.style.marginLeft = \"0%\";\n",
      "        loginText.style.marginLeft = \"0%\";\n",
      "      });\n",
      "      signupLink.onclick = (()=>{\n",
      "        signupBtn.click();\n",
      "        return false;\n",
      "      });\n",
      "\n",
      "--- File: ./components/app_flask/app/static/chat.js ---\n",
      "document.getElementById(\"send-button\").addEventListener(\"click\", function() {\n",
      "  var userInput = document.getElementById(\"user-input\").value;\n",
      "  var chatMessages = document.getElementById(\"chat-messages\");\n",
      "\n",
      "  // Display user message\n",
      "  chatMessages.innerHTML += \"<p><strong>You:</strong> \" + userInput + \"</p>\";\n",
      "\n",
      "  // Send user message to Flask server\n",
      "  fetch(\"/send_message\", {\n",
      "    method: \"POST\",\n",
      "    body: JSON.stringify({ message: userInput }),\n",
      "    headers: {\n",
      "      \"Content-Type\": \"application/json\"\n",
      "    }\n",
      "  })\n",
      "  .then(response => response.json())\n",
      "  .then(data => {\n",
      "    // Display chatbot response\n",
      "    chatMessages.innerHTML += \"<p><strong>Andres Perez:</strong> \" + data.message + \"</p>\";\n",
      "    // Scroll to bottom\n",
      "    chatMessages.scrollTop = chatMessages.scrollHeight;\n",
      "  });\n",
      "  // Clear input field\n",
      "  document.getElementById(\"user-input\").value = \"\";\n",
      "});\n",
      "--- File: ./components/app_flask/app/static/chat.css ---\n",
      "body {\n",
      "  background-image: url('andrehpereh10.png'); /* Add your background image URL */\n",
      "  /*background-size: ; 100%; /* Zoom out the background image to fit within the container */\n",
      "  /*background-size: cover; /* Cover the entire page */\n",
      "  /*background-repeat: no-repeat; /* Prevent the background image from repeating */\n",
      "  background-position: center; /* Center the background image */\n",
      "  background-attachment: fixed;\n",
      "  font-family: Arial, sans-serif;\n",
      "  color: #333;\n",
      "  margin: 0;\n",
      "  padding: 0;\n",
      "}\n",
      "h1 {\n",
      "  text-align: center;\n",
      "}\n",
      "#chat-container {\n",
      "  position: relative; /* Make container a positioning context */\n",
      "  max-width: 600px;\n",
      "  margin: 0 auto;\n",
      "  padding: 20px;\n",
      "  background-color: rgba(255, 255, 255, 0.8);\n",
      "  border-radius: 10px;\n",
      "  box-shadow: 0 0 10px rgba(0, 0, 0, 0.2);\n",
      "}\n",
      "#chat-messages {\n",
      "  max-height: 300px;\n",
      "  overflow-y: auto;\n",
      "  padding: 10px;\n",
      "  border: 1px solid #ccc;\n",
      "  border-radius: 5px;\n",
      "  margin-bottom: 10px;\n",
      "}\n",
      "#user-input {\n",
      "  width: calc(100% - 80px); /* Adjust width to leave space for the button */\n",
      "  font-size: 16px; /* Increase font size */\n",
      "  padding: 10px;\n",
      "  border: 1px solid #ccc;\n",
      "  border-radius: 5px;\n",
      "  margin-right: 10px;\n",
      "}\n",
      "#send-button {\n",
      "  font-size: 16px;\n",
      "  padding: 10px 20px;\n",
      "  background-color: #007bff;\n",
      "          color: #fff;\n",
      "          border: none;\n",
      "          border-radius: 5px;\n",
      "          cursor: pointer;\n",
      "        }\n",
      "        #send-button:hover {\n",
      "          background-color: #0056b3;\n",
      "        }\n",
      "\n",
      "#logout-button {\n",
      "  font-size: 16px; /* Adjust font size as needed */\n",
      "  padding: 10px 20px; /* Adjust padding as needed */\n",
      "  background-color: #ccc; /* Adjust background color as desired */\n",
      "  color: #333;\n",
      "  border: none;\n",
      "  border-radius: 5px;\n",
      "  cursor: pointer;\n",
      "  position: absolute; /* Absolute positioning */\n",
      "  bottom: 5px;  /* Position from the bottom edge */\n",
      "  right: 5px;  /* Position from the right edge */\n",
      "}\n",
      "#logout-button:hover {\n",
      "  background-color: #ddd; /* Adjust hover color as desired */\n",
      "}\n",
      "--- File: ./components/app_flask/app/templates/upload.html ---\n",
      "<!DOCTYPE html>\n",
      "<html>\n",
      "<head>\n",
      "    <title>Multi-Media Uploader</title>\n",
      "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='upload.css') }}\">\n",
      "</head>\n",
      "<body>\n",
      "    <div class=\"container\">\n",
      "        <h1>Create Your Chatbot </h1>\n",
      "        <input type=\"file\" id=\"fileInput\" multiple accept=\"image/*, audio/*, text/plain\"> \n",
      "        <input type=\"text\" id=\"codeVersion\" placeholder=\"Code version (optional)\">\n",
      "        <label for=\"model_name\">Model Name:</label>\n",
      "        <select id=\"model_name\">\n",
      "            <option value=\"gemma_2b_en\">gemma_2b_en</option>\n",
      "            <option value=\"gemma_7b_en\">gemma_7b_en</option>\n",
      "        </select>\n",
      "        <label for=\"epochs\">Epochs:</label>\n",
      "        <select id=\"epochs\">\n",
      "            </select>\n",
      "        <button id=\"uploadButton\">Upload and create personalized chatbot</button>\n",
      "        <div id=\"preview\"></div>  \n",
      "    </div>\n",
      "    <div id=\"upload-notification\" style=\"display: none;\">\n",
      "      Files uploaded! A personalize chatbot is on its way.\n",
      "        You'll be notified by email once it's done.\n",
      "    </div>\n",
      "    <script src=\"{{ url_for('static', filename='upload.js') }}\"> </script>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "\n",
      "--- File: ./components/app_flask/app/templates/index.html ---\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Andrehpereh Assistant</title>\n",
      "    <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='style.css') }}\">\n",
      "</head>\n",
      "<body>\n",
      "      <div class=\"wrapper\">\n",
      "      <div class=\"title-text\">\n",
      "        <div class=\"title login\">My chatbot</div>\n",
      "        <div class=\"title signup\">Mychatbot</div>\n",
      "      </div>\n",
      "      <div class=\"form-container\">\n",
      "        <div class=\"slide-controls\">\n",
      "          <input type=\"radio\" name=\"slide\" id=\"login\" checked>\n",
      "          <input type=\"radio\" name=\"slide\" id=\"signup\">\n",
      "          <label for=\"login\" class=\"slide login\">Login</label>\n",
      "          <label for=\"signup\" class=\"slide signup\">Signup</label>\n",
      "          <div class=\"slider-tab\"></div>\n",
      "        </div>\n",
      "        <div class=\"form-inner\">\n",
      "          <form action=\"/login\" class=\"login\" method=\"POST\">\n",
      "            <div class=\"field\">\n",
      "              <input type=\"text\" id=\"email\" name=\"email\" placeholder=\"Email Address\" required>\n",
      "            </div>\n",
      "            <div class=\"field\">\n",
      "              <input type=\"password\" id=\"password\" name=\"password\" placeholder=\"Password\" required>\n",
      "            </div>\n",
      "            <div class=\"pass-link\"><a href=\"#\">Forgot password?</a></div>\n",
      "            <div class=\"field btn\">\n",
      "              <div class=\"btn-layer\"></div>\n",
      "              <input type=\"submit\" value=\"Login\">\n",
      "            </div>\n",
      "            <div class=\"signup-link\">Not a member? <a href=\"\">Signup now</a></div>\n",
      "          </form>\n",
      "          <form action=\"/signup\" class=\"signup\" method=\"POST\">\n",
      "            <div class=\"field\">\n",
      "              <input type=\"text\" id=\"email\" name=\"email\" placeholder=\"Email Address\" required>\n",
      "            </div>\n",
      "            <div class=\"field\">\n",
      "              <input type=\"password\" id=\"password\" name=\"password\" placeholder=\"Password\" required>\n",
      "            </div>\n",
      "            <div class=\"field\">\n",
      "              <input type=\"password\" id=\"confirm_password\" name=\"confirm_password\" placeholder=\"Confirm password\" required>\n",
      "            </div>\n",
      "            <div class=\"field btn\">\n",
      "              <div class=\"btn-layer\"></div>\n",
      "              <input type=\"submit\" value=\"Signup\">\n",
      "            </div>\n",
      "          </form>\n",
      "        </div>\n",
      "      </div>\n",
      "    </div>\n",
      "    <script src=\"{{ url_for('static', filename='script.js') }}\"> \n",
      "    </script>\n",
      "</body>\n",
      "</html>\n",
      "--- File: ./components/app_flask/app/templates/login.html ---\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>Login</title>\n",
      "</head>\n",
      "<body>\n",
      "    <h1>My Own Chatbot Login Form</h1>\n",
      "    <form action=\"/chat\" method=\"post\">\n",
      "        <label for=\"username\">Username:</label>\n",
      "        <input type=\"text\" id=\"username\" name=\"username\" required><br><br>\n",
      "        <label for=\"password\">Password:</label>\n",
      "        <input type=\"password\" id=\"password\" name=\"password\" required><br><br>\n",
      "        <input type=\"submit\" value=\"Login\">\n",
      "    </form>\n",
      "</body>\n",
      "</html>\n",
      "--- File: ./components/app_flask/app/templates/chat.html ---\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "  <meta charset=\"UTF-8\">\n",
      "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "  <title>Chat Interface</title>\n",
      "  <link rel=\"stylesheet\" href=\"{{ url_for('static', filename='chat.css') }}\">\n",
      "</head>\n",
      "<body>\n",
      "  <h1>Welcome to gossip with Andres</h1>\n",
      "  <div id=\"chat-container\">\n",
      "    <div id=\"chat-messages\"></div>\n",
      "    <input type=\"text\" id=\"user-input\" placeholder=\"Ask me something.\">\n",
      "    <button id=\"send-button\">Send</button>\n",
      "    <button id=\"logout-button\">Logout</button>\n",
      "  </div>\n",
      "    <script src=\"{{ url_for('static', filename='chat.js') }}\"> \n",
      "    </script>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "--- File: ./components/pipeline/util.py ---\n",
      "from config import Config\n",
      "import os\n",
      "import re\n",
      "import numpy as np\n",
      "from google.cloud import storage\n",
      "\n",
      "def get_model_paths_and_config(model_name):\n",
      "    \"\"\"\n",
      "    Constructs paths, determines machine configuration, and gets the VLLM model name for a given model.\n",
      "\n",
      "    Args:\n",
      "        model_name (str): The base name of the model (e.g., \"gemma_2b_en\").\n",
      "\n",
      "    Returns:\n",
      "        dict: A dictionary containing the following keys:\n",
      "            - 'model_size': The size of the model (\"2b\" or \"7b\").\n",
      "            - 'finetuned_model_dir': Path to the finetuned model directory.\n",
      "            - 'finetuned_weights_path': Path to the finetuned model weights.\n",
      "            - 'finetuned_vocab_path': Path to the finetuned model vocabulary.\n",
      "            - 'huggingface_model_dir': Path to the Hugging Face model directory.\n",
      "            - 'deployed_model_blob': Blob name of the deployed model in Cloud Storage.\n",
      "            - 'deployed_model_uri': URI of the deployed model in Cloud Storage.\n",
      "            - 'machine_type': The appropriate machine type.\n",
      "            - 'accelerator_type': The accelerator type.\n",
      "            - 'accelerator_count': The number of accelerators.\n",
      "            - 'model_name_vllm': The VLLM-specific model name.\n",
      "    \"\"\"\n",
      "\n",
      "    allowed_models = [\n",
      "        \"gemma_2b_en\",\n",
      "        \"gemma_instruct_2b_en\",\n",
      "        \"gemma_7b_en\",\n",
      "        \"gemma_instruct_7b_en\",\n",
      "    ]\n",
      "\n",
      "    if model_name not in allowed_models:\n",
      "        raise ValueError(f\"Invalid {model_name}. Supported models are: {allowed_models}\")\n",
      "\n",
      "    # Construct paths\n",
      "    model_size = model_name.split(\"_\")[-2]\n",
      "    assert model_size in (\"2b\", \"7b\")\n",
      "\n",
      "    # When runnning local \"./\"\n",
      "    finetuned_model_dir = f\"./{model_name}\"\n",
      "    bucket_name = Config.BUCKET_NAME\n",
      "    bucket_uri = f\"gs://{Config.BUCKET_NAME}\"\n",
      "    #finetuned_model_dir = f\"{Config.BUCKET_URI}/{model_name}_raw/{model_name}\"\n",
      "    \n",
      "    print(finetuned_model_dir)\n",
      "    finetuned_weights_path = f\"{finetuned_model_dir}/model.weights.h5\"\n",
      "    finetuned_vocab_path = f\"{finetuned_model_dir}/vocabulary.spm\"\n",
      "    huggingface_model_dir = f\"{finetuned_model_dir}_huggingface\"\n",
      "    timestamp = os.path.join(os.getenv(\"USER_NAME\", \"andrehpereh1\"), Config.TIMESTAMP)\n",
      "    deployed_model_blob = os.path.join(timestamp, model_name, 'huggingface')\n",
      "    fine_tuned_keras_blob = os.path.join(timestamp, model_name, 'keras')\n",
      "    deployed_model_uri = f\"{bucket_uri}/{deployed_model_blob}\"  # Assuming BUCKET_URI is globally defined\n",
      "\n",
      "    # Determine machine configuration\n",
      "    machine_config = {\n",
      "        \"2b\": {\n",
      "            \"machine_type\": \"g2-standard-8\",\n",
      "            \"accelerator_type\": \"NVIDIA_L4\",\n",
      "            \"accelerator_count\": 1,\n",
      "            \"memory\": \"40G\",\n",
      "            \"cpu\": \"12.0\"\n",
      "        },\n",
      "        \"7b\": {\n",
      "            \"machine_type\": \"g2-standard-12\",\n",
      "            \"accelerator_type\": \"NVIDIA_L4\",\n",
      "            \"accelerator_count\": 1,\n",
      "            \"memory\": \"80G\",\n",
      "            \"cpu\": \"32.0\"\n",
      "        }\n",
      "        \n",
      "    }[model_size]  # Efficient lookup\n",
      "\n",
      "    return {\n",
      "        \"model_size\": model_size,\n",
      "        \"bucket_name\": bucket_name,\n",
      "        \"finetuned_model_dir\": finetuned_model_dir,\n",
      "        \"finetuned_weights_path\": finetuned_weights_path,\n",
      "        \"finetuned_vocab_path\": finetuned_vocab_path,\n",
      "        \"huggingface_model_dir\": huggingface_model_dir,\n",
      "        \"deployed_model_blob\": deployed_model_blob,\n",
      "        \"deployed_model_uri\": deployed_model_uri,\n",
      "        \"fine_tuned_keras_blob\": fine_tuned_keras_blob,\n",
      "        \"model_name_vllm\": f\"{model_name}-vllm\", \n",
      "        **machine_config  # Add machine config directly\n",
      "    }\n",
      "\n",
      "def upload2bs(local_directory, bucket_name, destination_subfolder=\"\"):\n",
      "    \"\"\"Uploads a local directory and its contents to a Google Cloud Storage bucket.\n",
      "\n",
      "    Args:\n",
      "        local_directory (str): Path to the local directory.\n",
      "        bucket_name (str): Name of the target Google Cloud Storage bucket.\n",
      "        destination_subfolder (str, optional): Prefix to append to the path within the bucket. \n",
      "                                        Defaults to \"\".\n",
      "    \"\"\"\n",
      "\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.bucket(bucket_name)\n",
      "\n",
      "    for root, _, files in os.walk(local_directory):\n",
      "        for file in files:\n",
      "            local_path = os.path.join(root, file)\n",
      "            # Construct the path within the bucket\n",
      "            blob_path = os.path.join(destination_subfolder, os.path.relpath(local_path, local_directory))\n",
      "            blob = bucket.blob(blob_path)\n",
      "            blob.upload_from_filename(local_path)\n",
      "            print(f\"Uploaded {local_path} to gs://{bucket_name}/{blob_path}\")\n",
      "    destination_path = os.path.dirname(f\"gs://{bucket_name}/{blob_path}\")\n",
      "    return destination_path\n",
      "\n",
      "def download_all_from_blob(bucket_name, blob_prefix, local_destination=\"\"):\n",
      "    \"\"\"Downloads all files from a Google Cloud Storage blob (with an optional prefix) to a local directory.\n",
      "\n",
      "    Args:\n",
      "        bucket_name (str): Name of the Google Cloud Storage bucket.\n",
      "        blob_prefix (str): Prefix specifying the subfolder within the bucket to download from.\n",
      "        local_destination (str, optional): Local directory to download files into. Defaults\n",
      "                                           to the current working directory.\n",
      "    \"\"\"\n",
      "\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.bucket(bucket_name)\n",
      "\n",
      "    blobs = bucket.list_blobs(prefix=blob_prefix)  # List blobs with the prefix\n",
      "    print(\"This are the blobs\", blobs)\n",
      "    for blob in blobs:\n",
      "        # Construct local download path (ensuring directories exist)\n",
      "        print(blob.name)\n",
      "        print(\"This is the file name\", os.path.basename(blob.name))\n",
      "        destination_filepath = os.path.join(local_destination, os.path.basename(blob.name))\n",
      "        os.makedirs(os.path.dirname(destination_filepath), exist_ok=True)\n",
      "\n",
      "        # Download the file \n",
      "        blob.download_to_filename(destination_filepath)\n",
      "        print(f\"Downloaded gs://{bucket_name}/{blob.name} to {destination_filepath}\")\n",
      "\n",
      "\n",
      "\n",
      "--- File: ./components/pipeline/__init__.py ---\n",
      "\n",
      "--- File: ./components/pipeline/pipeline.py ---\n",
      "from kfp import dsl\n",
      "import kfp as kfp\n",
      "from kfp.dsl import OutputPath, Artifact, InputPath, PipelineTaskFinalStatus, ExitHandler\n",
      "from kfp import compiler\n",
      "from config import Config\n",
      "from google.cloud import aiplatform as vertexai\n",
      "import os\n",
      "\n",
      "TAG_NAME = os.environ.get('TAG_NAME', 'masterv6') \n",
      "\n",
      "@dsl.component(base_image='python:3.9', packages_to_install=['google-cloud-bigquery'])\n",
      "def send_pipeline_completion_email_op(\n",
      "    project: str,\n",
      "    status: PipelineTaskFinalStatus,\n",
      "    smtp_server: str = 'smtp.gmail.com',\n",
      "    smtp_port: int = 587,\n",
      "    sender_email: str = 'andrehpereh96@gmail.com',\n",
      "    recipient_emails: str = \"andrehpereh@gmail.com\",\n",
      "    email_password: str = \"ssuy rubm kzge juid\"\n",
      "):\n",
      "    import smtplib\n",
      "    from email.mime.text import MIMEText\n",
      "    from google.cloud import bigquery\n",
      "    recipient_emails = [recipient_emails]\n",
      "    \n",
      "    DATASET_ID = 'chatbot' # This should be moved to a config file\n",
      "    USER_TRAINING_STATUS = 'user_training_status' # This should be moved to a config file\n",
      "    \"\"\"\n",
      "    Monitors for a success flag file and sends an email upon detection.\n",
      "\n",
      "    Args:\n",
      "        smtp_server (str): SMTP server address. Defaults to 'smtp.gmail.com'.\n",
      "        smtp_port (int): SMTP server port. Defaults to 587.\n",
      "        sender_email (str): Email address of the sender. Defaults to 'your_email@gmail.com'.\n",
      "        recipient_emails (list): List of recipient email addresses. Defaults to ['recipient@example.com'].\n",
      "        email_password (str): Password for the sender's email account.\n",
      "        success_flag_path (str): Path to the success flag file. Defaults to '/tmp/pipeline_success_flag.txt'.\n",
      "    \"\"\"\n",
      "    if status.state == 'SUCCEEDED':\n",
      "        msg = MIMEText(\n",
      "            f\"\"\"Chatbot Completion Status ; {status.state}:\\\n",
      "            \\nYou can start interacting with it by clicking the following link: \\\n",
      "            \\nhttps://chattingbot-gqf6v2rlha-uc.a.run.app/home \\\n",
      "            \\nPlease let us know if you have any questions or feedback. \\\n",
      "            \\n\\nBest regards, \\\n",
      "            \\nAndres Perez\n",
      "            \"\"\"\n",
      "        )\n",
      "    else:\n",
      "        msg = MIMEText(\n",
      "            f\"\"\"Chatbot Completion Status ; Unavailable:\\\n",
      "            \\nWe sincerely apologize for the unexpected delay. We've encountered a technical issue and are working to resolve it as quickly as possible. \n",
      "            \\n\\nWe'll send you an update as soon as your chatbot is available. Thank you for your patience.  \\\n",
      "            \\n\\nBest regards, \\\n",
      "            \\nAndres Perez\n",
      "            \"\"\"\n",
      "        )\n",
      "    \n",
      "    msg['Subject'] = f\"Your Chatbot has {status.state}\"\n",
      "    msg['From'] = sender_email\n",
      "    msg['To'] = ', '.join(recipient_emails)\n",
      "\n",
      "    with smtplib.SMTP(smtp_server, smtp_port) as server:\n",
      "        server.starttls()  # Enable TLS encryption\n",
      "        print(\"This is the email\", sender_email)\n",
      "        print(\"This is the password\", email_password)\n",
      "        server.login(sender_email, email_password)\n",
      "        server.sendmail(sender_email, recipient_emails, msg.as_string())\n",
      "    if status.state == \"SUCCEEDED\":\n",
      "        client = bigquery.Client(project)\n",
      "        print(\"This is the client\", client)\n",
      "        table_ref = client.dataset(DATASET_ID).table(USER_TRAINING_STATUS)\n",
      "        table = client.get_table(table_ref)\n",
      "        row_to_insert = {\n",
      "            'email': recipient_emails,\n",
      "            'training_status': 1\n",
      "        }\n",
      "        client.insert_rows(table, [row_to_insert]) \n",
      "        errors = client.insert_rows(table, [row_to_insert])\n",
      "        if errors:  # Check if there were errors\n",
      "            print(\"The model has been trained, but error updating training_status for {}: {}\".format(email_password, errors))\n",
      "        else:\n",
      "            print(\"User training has been updated\")\n",
      "\n",
      "    print('Email sent!')\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "@dsl.component(\n",
      "    base_image =f\"gcr.io/{Config.PROJECT_ID}/gemma-chatbot-data-preparation:{TAG_NAME}\"\n",
      ")\n",
      "def data_preparation_op(\n",
      "    bucket_name: str,\n",
      "    directory: str,\n",
      "    dataset_path: OutputPath('Dataset'),\n",
      "    pair_count: int=4,\n",
      "    data_augmentation_iter: int=4\n",
      "):\n",
      "    import data_ingestion\n",
      "    import json\n",
      "    from vertexai.preview.generative_models import GenerativeModel\n",
      "    gemini_pro_model = GenerativeModel(\"gemini-1.0-pro\")\n",
      "    formatted_messages = data_ingestion.data_preparation(\n",
      "        bucket_name=bucket_name, directory=directory, gemini_pro_model=gemini_pro_model,\n",
      "        pair_count=pair_count, data_augmentation_iter=data_augmentation_iter\n",
      "    )\n",
      "    with open(dataset_path, 'w') as f:\n",
      "        json.dump(formatted_messages, f)\n",
      "\n",
      "\n",
      "@dsl.component(\n",
      "    base_image = f\"gcr.io/{Config.PROJECT_ID}/gemma-chatbot-fine-tunning:{TAG_NAME}\"\n",
      ")\n",
      "def fine_tunning(\n",
      "  dataset_path: InputPath('Dataset'),\n",
      "  model_paths: dict,\n",
      "  fine_tune_flag: bool,\n",
      "  epochs: int,\n",
      "  model_name: str,\n",
      "  bucket_name: str\n",
      ") -> str:\n",
      "    import trainer\n",
      "    import json\n",
      "    import util\n",
      "    import os\n",
      "    with open(dataset_path, 'r') as f:\n",
      "        dataset = json.load(f)\n",
      "    os.makedirs(model_paths['finetuned_model_dir'], exist_ok=True)\n",
      "    finetuned_weights_path = os.path.join(model_paths['finetuned_model_dir'], 'model.weights.h5') \n",
      "    \n",
      "    model = trainer.finetune_gemma(dataset, model_paths, fine_tune_flag, epochs=epochs, model_name=model_name)\n",
      "    print(\"Its gonna save it here\", finetuned_weights_path)\n",
      "    util.upload2bs(\n",
      "        local_directory = model_paths['finetuned_model_dir'], bucket_name = bucket_name,\n",
      "        destination_subfolder = model_paths['fine_tuned_keras_blob']\n",
      "    )\n",
      "    model_gcs = \"gs://{}/{}\".format(bucket_name, model_paths['fine_tuned_keras_blob'])  \n",
      "    print(\"This is the storage bucket\", model_gcs)\n",
      "    return model_gcs\n",
      "    \n",
      "\n",
      "@dsl.component(\n",
      "    base_image = f\"gcr.io/{Config.PROJECT_ID}/gemma-chatbot-fine-tunning:{TAG_NAME}\"\n",
      ")\n",
      "def convert_checkpoints_op(\n",
      "  keras_gcs_model: str,\n",
      "  model_paths: dict,\n",
      "  bucket_name: str\n",
      ") -> str:\n",
      "    import conversion_function\n",
      "    import os\n",
      "    import util\n",
      "    print(\"This is the keras passed\", keras_gcs_model)\n",
      "    util.download_all_from_blob(bucket_name, model_paths['fine_tuned_keras_blob'], local_destination=model_paths['finetuned_model_dir'])\n",
      "    if os.path.exists(\"./model.weights.h5\"):\n",
      "        print(\"File exists!\")\n",
      "    else:\n",
      "        print(\"File does not exist.\")\n",
      "    converted_fined_tuned_path = conversion_function.convert_checkpoints(\n",
      "        weights_file=model_paths['finetuned_weights_path'],\n",
      "        size=model_paths['model_size'],\n",
      "        output_dir=model_paths['huggingface_model_dir'],\n",
      "        vocab_path=model_paths['finetuned_vocab_path']\n",
      "    )\n",
      "    util.upload2bs(\n",
      "        local_directory = converted_fined_tuned_path, bucket_name = bucket_name,\n",
      "        destination_subfolder = model_paths['deployed_model_blob']\n",
      "    )\n",
      "    return model_paths['deployed_model_uri']\n",
      "\n",
      "\n",
      "\n",
      "@dsl.component(base_image='python:3.9', packages_to_install=['google-cloud-bigquery'])\n",
      "def update_user_endpoint(\n",
      "    endpoint_resource: str,\n",
      "    email: str,\n",
      "    project: str\n",
      "):\n",
      "\n",
      "    import os\n",
      "    from google.cloud import bigquery\n",
      "    DATASET_ID = 'chatbot' # This should be moved to a config file\n",
      "    USER_TRAINING_STATUS = 'user_training_status' # This should be moved to a config file\n",
      "    #This part can be wrapped in a function\n",
      "    import json\n",
      "    data = json.loads(endpoint_resource)\n",
      "    resource_uri = data['resources'][0]['resourceUri']\n",
      "\n",
      "    print(\"This is the passed end pooint\", endpoint_resource)\n",
      "    print(dir(endpoint_resource))\n",
      "    print(type(endpoint_resource))\n",
      "    print(\"This is the project\", project)\n",
      "    \n",
      "    client = bigquery.Client(project)\n",
      "    print(\"This is the client\", client)\n",
      "    table_ref = client.dataset(DATASET_ID).table(USER_TRAINING_STATUS)\n",
      "    table = client.get_table(table_ref)\n",
      "    row_to_insert = {\n",
      "        'email': email,\n",
      "        'end_point': resource_uri,\n",
      "        'training_status': True\n",
      "    }\n",
      "    client.insert_rows(table, [row_to_insert]) \n",
      "    errors = client.insert_rows(table, [row_to_insert])\n",
      "    if errors:  # Check if there were errors\n",
      "        print(\"The model has been trained, but error updating resource_uri for {}: {}\".format(email, errors))\n",
      "    else:\n",
      "        print(\"User training has been updated\")\n",
      "    print(\"End point has been stored.\")\n",
      "\n",
      "\n",
      "@dsl.pipeline(name=\"Model deployment.\")\n",
      "def fine_tune_pipeline(\n",
      "    project: str = os.environ.get('PROJECT_ID') ,\n",
      "    bucket_name: str = \"able-analyst-416817-chatbot-v1\",\n",
      "    directory: str = \"input_data/andrehpereh\",\n",
      "    serving_image: str = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\",\n",
      "    fine_tune_flag: bool = False,\n",
      "    epochs: int = 3,\n",
      "    model_name: str = 'gemma_2b_en',\n",
      "    pair_count: int = 6,\n",
      "    data_augmentation_iter: int = 4\n",
      "):\n",
      "\n",
      "    from google_cloud_pipeline_components.types import artifact_types\n",
      "    from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp, ModelDeployOp)\n",
      "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
      "    from kfp.dsl import importer_node\n",
      "    from util import get_model_paths_and_config\n",
      "    from config import Config\n",
      "\n",
      "    model_paths = get_model_paths_and_config(Config.MODEL_NAME)\n",
      "\n",
      "    port = 7080\n",
      "    accelerator_count=1\n",
      "    max_model_len=256\n",
      "    dtype=\"bfloat16\"\n",
      "    vllm_args = [\n",
      "        \"--host=0.0.0.0\",\n",
      "        f\"--port={port}\",\n",
      "        f\"--tensor-parallel-size={accelerator_count}\",\n",
      "        \"--swap-space=16\",\n",
      "        \"--gpu-memory-utilization=0.95\",\n",
      "        f\"--max-model-len={max_model_len}\",\n",
      "        f\"--dtype={dtype}\",\n",
      "        \"--disable-log-stats\",\n",
      "    ]\n",
      "\n",
      "    metadata = {\n",
      "      \"imageUri\": serving_image,\n",
      "      \"command\": [\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
      "      \"args\": vllm_args,\n",
      "      \"ports\": [\n",
      "        {\n",
      "          \"containerPort\": port\n",
      "        }\n",
      "      ],\n",
      "      \"predictRoute\": \"/generate\",\n",
      "      \"healthRoute\": \"/ping\"\n",
      "    }\n",
      "    # This should come from a dataset instead of hardcoding it.\n",
      "    email = f'{Config.USER_NAME}@gmail.com'\n",
      "    send_email = send_pipeline_completion_email_op(recipient_emails = email, project=project)\n",
      "    with ExitHandler(send_email):\n",
      "        whatup = data_preparation_op(\n",
      "            bucket_name = bucket_name, directory = directory,\n",
      "            pair_count=pair_count, data_augmentation_iter=data_augmentation_iter\n",
      "        )\n",
      "\n",
      "        trainer = fine_tunning(\n",
      "            dataset_path=whatup.outputs['dataset_path'], model_paths=model_paths, fine_tune_flag=fine_tune_flag,\n",
      "            epochs=epochs, model_name=model_name, bucket_name = bucket_name\n",
      "        )\n",
      "        trainer.set_memory_limit(model_paths['memory']).set_cpu_limit(model_paths['cpu']).set_accelerator_limit(1).add_node_selector_constraint(model_paths['accelerator_type'])\n",
      "\n",
      "        print(\"This is the dictionary\", model_paths)\n",
      "        converted = convert_checkpoints_op(\n",
      "            keras_gcs_model=trainer.output, model_paths=model_paths, bucket_name = bucket_name\n",
      "        ).set_memory_limit(model_paths['memory']).set_cpu_limit(model_paths['cpu']).set_accelerator_limit(1).add_node_selector_constraint(model_paths['accelerator_type'])\n",
      "\n",
      "        import_unmanaged_model_task = importer_node.importer(\n",
      "            artifact_uri=converted.output,\n",
      "            artifact_class=artifact_types.UnmanagedContainerModel,\n",
      "            metadata={\n",
      "                \"containerSpec\": metadata,\n",
      "            },\n",
      "        )\n",
      "\n",
      "        model_upload_op = ModelUploadOp(\n",
      "            project=project,\n",
      "            display_name=f\"Mini {Config.USER_NAME} model uploaded.\",\n",
      "            unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
      "        )\n",
      "        model_upload_op.after(import_unmanaged_model_task)\n",
      "\n",
      "        endpoint_create_op = EndpointCreateOp(\n",
      "            project=project,\n",
      "            display_name=f\"End point created for {Config.USER_NAME}\",\n",
      "        )\n",
      "\n",
      "        model_end_point = ModelDeployOp(\n",
      "            endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
      "            model=model_upload_op.outputs[\"model\"],\n",
      "            deployed_model_display_name=f\"Model {model_paths['model_name_vllm']}, for user:{Config.USER_NAME}\",\n",
      "            dedicated_resources_machine_type=model_paths['machine_type'],\n",
      "            dedicated_resources_min_replica_count=1,\n",
      "            dedicated_resources_max_replica_count=1,\n",
      "            dedicated_resources_accelerator_type=model_paths['accelerator_type'],\n",
      "            dedicated_resources_accelerator_count=model_paths['accelerator_count']\n",
      "        )\n",
      "        print(\"This is the project\", project)\n",
      "        update_user_endpoint(endpoint_resource=model_end_point.outputs[\"gcp_resources\"], email=email, project=project)\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    \n",
      "    os.environ['TRAIN_DATA_DIR'] = 'andrehpereh/input_data' \n",
      "    os.environ['BUCKET_NAME'] = 'personalize-chatbots-v1'\n",
      "    from kfp import compiler\n",
      "    from google.cloud import aiplatform as vertexai\n",
      "    from config import Config\n",
      "    print(\"This is the model name\", Config.MODEL_NAME, \"Ahuevito\")\n",
      "    print(\"This is the directory\", Config.TRAIN_DATA_DIR, \"Ahuevito\")\n",
      "    print(\"This is the BUCKET_NAME\", Config.BUCKET_NAME, \"Ahuevito\")\n",
      "    print(\"This is the FINE_TUNE_FLAG\", Config.FINE_TUNE_FLAG, \"Ahuevito\")\n",
      "    print(\"This is the EPOCHS\", Config.EPOCHS, \"Ahuevito\")\n",
      "    pipeline_name = f\"fine_tune_pipeline{Config.USER_NAME}.json\"\n",
      "    compiler.Compiler().compile(\n",
      "        pipeline_func=fine_tune_pipeline, package_path=pipeline_name\n",
      "    )\n",
      "    vertexai.init(project=Config.PROJECT_ID, location=Config.REGION)\n",
      "    vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
      "        display_name=\"test-fine_tune_pipeline\",\n",
      "        template_path=pipeline_name,\n",
      "        parameter_values={\n",
      "            \"project\": Config.PROJECT_ID,\n",
      "            \"bucket_name\": Config.BUCKET_NAME,\n",
      "            \"directory\": Config.TRAIN_DATA_DIR,\n",
      "            \"serving_image\": Config.SERVING_IMAGE,\n",
      "            \"fine_tune_flag\": Config.FINE_TUNE_FLAG,\n",
      "            \"epochs\": Config.EPOCHS,\n",
      "            \"model_name\": Config.MODEL_NAME\n",
      "        }\n",
      "    )\n",
      "    vertex_pipelines_job.run()\n",
      "--- File: ./components/pipeline/Dockerfile ---\n",
      "FROM python:3.9-slim\n",
      "\n",
      "WORKDIR /pipeline\n",
      "RUN ls\n",
      "COPY requirements.txt .\n",
      "RUN pip install -U -r requirements.txt\n",
      "COPY . /pipeline \n",
      "WORKDIR /pipeline\n",
      "RUN ls\n",
      "RUN pwd\n",
      "RUN pip list\n",
      "--- File: ./components/pipeline/app.py ---\n",
      "from flask import Flask, request\n",
      "import base64  # For decoding Pub/Sub data\n",
      "import json\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "@app.route('/', methods=['POST'])\n",
      "def process_message():\n",
      "    if request.headers.get('Content-Type') == 'application/json':\n",
      "        json_parameters = request.get_json()\n",
      "        parameters = json.loads(json_parameters)\n",
      "    else:  # Likely base64 encoded from Pub/Sub\n",
      "        data = request.data\n",
      "        print(data)\n",
      "        json_parameters = base64.b64decode(data).decode('utf-8')\n",
      "        print(json_parameters)\n",
      "        parameters = json.loads(json_parameters)\n",
      "    print(parameters)\n",
      "\n",
      "    return 'Message processed', 200 \n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(host='0.0.0.0', port=5000, debug=True) \n",
      "\n",
      "--- File: ./components/pipeline/config.py ---\n",
      "# Copyright 2021 Google LLC. All Rights Reserved.\n",
      "#\n",
      "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "# you may not use this file except in compliance with the License.\n",
      "# You may obtain a copy of the License at\n",
      "#\n",
      "#     http://www.apache.org/licenses/LICENSE-2.0\n",
      "#\n",
      "# Unless required by applicable law or agreed to in writing, software\n",
      "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "# See the License for the specific language governing permissions and\n",
      "# limitations under the License.\n",
      "\"\"\"The pipeline configurations.\n",
      "\"\"\"\n",
      "\n",
      "import os\n",
      "import datetime\n",
      "\n",
      "class Config:\n",
      "    \"\"\"Sets configuration vars.\"\"\"\n",
      "    # Lab user environment resource settings\n",
      "    PROJECT_ID = os.getenv(\"PROJECT_ID\", \"able-analyst-416817\")  # Replace with the logic to get your project ID \n",
      "    # Other Variables\n",
      "    KAGGLE_USERNAME = os.getenv(\"KAGGLE_USERNAME\", \"andrehpereh1\")\n",
      "    USER_NAME = os.getenv(\"USER_NAME\", \"andrehpereh\")\n",
      "    KAGGLE_KEY = os.getenv(\"KAGGLE_KEY\", \"5859e39806d9456749dcbac685f04bc9\")\n",
      "    KERAS_BACKEND = os.getenv(\"KERAS_BACKEND\", \"tensorflow\")\n",
      "    REGION = os.getenv(\"REGION\", \"us-central1\")\n",
      "    BUCKET_NAME = os.getenv(\"BUCKET_NAME\", f\"{PROJECT_ID}-chatbot-v1\")\n",
      "    # BUCKET_URI = os.getenv(\"BUCKET_URI\", f\"gs://{BUCKET_NAME}\")\n",
      "    SERVICE_ACCOUNT_NAME = os.getenv(\"SERVICE_ACCOUNT_NAME\", \"gemma-vertexai-chatbot\")\n",
      "    SERVICE_ACCOUNT_DISPLAY_NAME = \"Gemma Vertex AI endpoint\"  # Not directly converted \n",
      "    SERVICE_ACCOUNT = os.getenv(\"SERVICE_ACCOUNT\", f\"{SERVICE_ACCOUNT_NAME}@{PROJECT_ID}.iam.gserviceaccount.com\")\n",
      "    TIMESTAMP = os.getenv(\"TIMESTAMP\", datetime.datetime.now().strftime('%Y%m%d%H%M%S'))\n",
      "    MODEL_NAME = os.getenv(\"MODEL_NAME\", \"gemma_2b_en\")\n",
      "    RANK_LORA = os.getenv(\"RANK_LORA\", 6)  # Default value of 6\n",
      "    SEQUENCE_LENGTH = os.getenv(\"SEQUENCE_LENGTH\", 256)\n",
      "    EPOCHS = os.getenv(\"EPOCHS\", 1)\n",
      "    BATCH_SIZE = os.getenv(\"BATCH_SIZE\", 1)\n",
      "    TRAIN_DATA_DIR = os.getenv(\"TRAIN_DATA_DIR\", \"input_data/andrehpereh\")\n",
      "    SERVING_IMAGE = os.getenv(\"SERVING_IMAGE\", \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\")\n",
      "    FINE_TUNE_FLAG = os.getenv(\"FINE_TUNE_FLAG\", False)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "--- File: ./components/cloud_functions/__init__.py ---\n",
      "\n",
      "--- File: ./components/cloud_functions/trigger_pipeline.py ---\n",
      "import os\n",
      "from kfp import dsl\n",
      "TAG_NAME = os.environ.get('TAG_NAME', 'latest')\n",
      "print(\"This is the PROD_TAG\", TAG_NAME)\n",
      "IMAGE = f\"gcr.io/{os.environ.get('PROJECT_ID')}/gemma-chatbot-pipeline-app:{TAG_NAME}\"\n",
      "\n",
      "@dsl.container_component\n",
      "def triger_pipeline_component():\n",
      "    return dsl.ContainerSpec(\n",
      "      image=IMAGE,\n",
      "      args=['python', 'pipeline.py']\n",
      "    )\n",
      "\n",
      "@dsl.pipeline(name=\"Trigger Pipeline\")\n",
      "def pipeline_trigger_pipeline_chatbot(\n",
      "):\n",
      "    task = triger_pipeline_component()    \n",
      "    task.set_env_variable('MODEL_NAME', os.environ.get('MODEL_NAME'))\n",
      "    task.set_env_variable('TAG_NAME', os.environ.get('TAG_NAME'))\n",
      "    task.set_env_variable('TRAIN_DATA_DIR', os.environ.get('TRAIN_DATA_DIR'))\n",
      "    task.set_env_variable('BUCKET_NAME', os.environ.get('BUCKET_NAME'))\n",
      "    task.set_env_variable('FINE_TUNE_FLAG', os.environ.get('FINE_TUNE_FLAG'))\n",
      "    task.set_env_variable('USER_NAME', os.environ.get('USER_NAME'))\n",
      "    task.set_env_variable('PROJECT_ID', os.environ.get('PROJECT_ID'))\n",
      "    task.set_env_variable('EPOCHS', os.environ.get('EPOCHS'))\n",
      "\n",
      "\n",
      "\n",
      "--- File: ./components/cloud_functions/main.py ---\n",
      "import base64\n",
      "import functions_framework\n",
      "import logging\n",
      "import os\n",
      "import json\n",
      "import logging\n",
      "from kfp import compiler\n",
      "from google.cloud import aiplatform as vertexai\n",
      "\n",
      "@functions_framework.cloud_event\n",
      "def trigger_pipeline_cloud_function(cloud_event):\n",
      "    try:\n",
      "        parameters = base64.b64decode(cloud_event.data[\"message\"][\"data\"])\n",
      "        parameters = json.loads(parameters.decode('utf-8'))\n",
      "        print(\"Parameters fine tunning personalized bot:\", parameters)\n",
      "        if len(parameters['tag_version']) >= 0:\n",
      "            os.environ['TAG_NAME'] = parameters['tag_version']\n",
      "        os.environ['USER_NAME'] = parameters['user_name']\n",
      "        os.environ['MODEL_NAME'] = parameters['model_name']\n",
      "        os.environ['MY_API_KEY'] = parameters['project_id']\n",
      "        os.environ['BUCKET_NAME'] = parameters['bucket_name']\n",
      "        os.environ['FINE_TUNE_FLAG'] = 'True'\n",
      "        os.environ['EPOCHS'] = parameters['epochs']\n",
      "        os.environ['PROJECT_ID'] = parameters['project_id']\n",
      "        os.environ['TRAIN_DATA_DIR'] = parameters['blob_folder']\n",
      "    \n",
      "        pipeline_name = f\"trigger_fine_tune_pipeline_{parameters['user_name']}.json\"\n",
      "\n",
      "        print(\"This is path name\", pipeline_name)\n",
      "        from trigger_pipeline import pipeline_trigger_pipeline_chatbot\n",
      "\n",
      "        compiler.Compiler().compile(\n",
      "            pipeline_func=pipeline_trigger_pipeline_chatbot, package_path=pipeline_name\n",
      "        )\n",
      "        vertexai.init(project=parameters['project_id'])\n",
      "        vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
      "            display_name=\"cloud_function_trigger_fine_tunning_pipeline\",\n",
      "            template_path=pipeline_name\n",
      "        )\n",
      "        vertex_pipelines_job.run()\n",
      "    except Exception as e: \n",
      "        logging.error(f\"Pipeline trigger failed: {e}\")\n",
      "\n",
      "--- File: ./components/data_preparation/task.py ---\n",
      "import os\n",
      "import argparse\n",
      "\n",
      "import data_ingestion\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('--bucket-name', dest='bucket-name',\n",
      "                        default='able-analyst-416817-chatbot-v1', type=str, help='GCS URI for saving model artifacts.')\n",
      "    parser.add_argument('--directory', dest='directory', \n",
      "                        default='input_data/andrehpereh', type=str, help='TF-Hub URL.')\n",
      "    args = parser.parse_args()\n",
      "    hparams = args.__dict__\n",
      "    data_ingestion.process_whatsapp_chat(hparams['bucket-name'], hparams['directory'])\n",
      "--- File: ./components/data_preparation/data_ingestion.py ---\n",
      "import re\n",
      "import os\n",
      "import sys\n",
      "import json\n",
      "import argparse\n",
      "from typing import List\n",
      "from google.cloud import storage\n",
      "from io import BytesIO\n",
      "\n",
      "def process_whatsapp_chat(bucket_name: str, directory: str, pair_count: int=6) -> List[str]:\n",
      "    print(\"Bucket Name\", bucket_name)\n",
      "    print(\"Directory\", directory)\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.get_bucket(bucket_name)\n",
      "    print(bucket_name)\n",
      "    print(directory)\n",
      "\n",
      "    current_sender = None\n",
      "    current_file = None\n",
      "    consecutive_messages = []\n",
      "    qa_pairs_all = []  # List to store question-answer pairs\n",
      "    print(bucket.list_blobs(prefix=directory))\n",
      "\n",
      "    for blob in bucket.list_blobs(prefix=directory):\n",
      "        # Extract filename from blob\n",
      "        print(blob.name)\n",
      "        filename = blob.name.split('/')[-1]\n",
      "\n",
      "        if filename.endswith('.txt') and 'WhatsApp' in filename:\n",
      "            print(filename)\n",
      "            file_content = blob.download_as_string().decode('utf-8')\n",
      "            lines = file_content.split('\\n')\n",
      "\n",
      "            for line in lines:\n",
      "                if line.startswith(('- ', '\\n')):\n",
      "                    continue\n",
      "\n",
      "                # Extract sender and message (Regex handles potential variations)\n",
      "                match = re.search(r'^.*-\\s(?P<sender>.*?):\\s(?P<message>.*)$', line)\n",
      "                if match:\n",
      "                    sender = match.group('sender').strip()\n",
      "                    if sender != 'Andres Perez':\n",
      "                        sender = 'Sender'\n",
      "                    message = match.group('message').strip()\n",
      "                    message = message.replace(\"<Media omitted>\", \"\")\n",
      "                    message = message.replace(\"Missed video call\", \"\")\n",
      "                    message = message.replace(\"null\", \"\")\n",
      "                    message = re.sub(r'http\\S+', '', message).strip()\n",
      "\n",
      "                    # Concatenate consecutive messages by the same sender\n",
      "                    if sender == current_sender:\n",
      "                        consecutive_messages.append(message)\n",
      "                    else:\n",
      "                        if consecutive_messages:\n",
      "                            qa_pairs_all.append(' '.join(consecutive_messages))\n",
      "                        current_sender = sender\n",
      "                        consecutive_messages = [f\"{sender}:\\n{message}\"]\n",
      "\n",
      "            # Add the last set of messages\n",
      "            if consecutive_messages:\n",
      "                qa_pairs_all.append(', '.join(consecutive_messages))\n",
      "\n",
      "    result = []\n",
      "    current_group = \"\"\n",
      "    for i, element in enumerate(qa_pairs_all):\n",
      "        current_group += element \n",
      "        current_group += \"\\n\\n\"  # Add double newline after every even element\n",
      "\n",
      "        if (i + 1) % pair_count == 0:\n",
      "            result.append(current_group)\n",
      "            current_group = \"\"  # Reset for the next group\n",
      "\n",
      "    # Handle a potential incomplete last group\n",
      "    if current_group:\n",
      "        result.append(current_group)\n",
      "    return result\n",
      "\n",
      "\n",
      "def process_transcripts(bucket_name, directory, gemini_pro_model, pair_count=6, data_augmentation_iter=4):\n",
      "    \"\"\"Processes all text files within a folder.\n",
      "\n",
      "    Args:\n",
      "        folder_path: Path to the input folder.\n",
      "        gemini_pro_model: The model used for generating responses.\n",
      "    \"\"\"\n",
      "\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.get_bucket(bucket_name)\n",
      "    model_response_all = \"\"\n",
      "    script_dir = os.path.dirname(__file__)  # Get the directory of the current script\n",
      "    filepath = os.path.join(script_dir, 'prompt.json')\n",
      "\n",
      "    with open(filepath, \"r\") as file:\n",
      "        data = json.load(file)\n",
      "    pre_prompt = data[\"prompt\"]\n",
      "\n",
      "    for blob in bucket.list_blobs(prefix=directory):\n",
      "        # Extract filename from blob\n",
      "        print(blob.name)\n",
      "        filename = blob.name.split('/')[-1]\n",
      "        if filename.endswith('.txt') and 'transcript' in filename:\n",
      "            # Read content of the file\n",
      "            print(filename)\n",
      "            contents = blob.download_as_string().decode('utf-8')\n",
      "            prompt = pre_prompt.format(contents)\n",
      "            for i in range(data_augmentation_iter):\n",
      "                try:\n",
      "                    model_response = gemini_pro_model.generate_content(prompt).text\n",
      "                    model_response_all += model_response.replace(\"** \", \"\\n\").replace(\"**\", \"\")\n",
      "                    print(\"-----------------------------------------------------------------------------------------------------------------------------------\")\n",
      "                except Exception as e:  # Catch any type of error\n",
      "                    print(f\"An error occurred: {e}. Skipping...\")\n",
      "\n",
      "    result = []\n",
      "    current_pair = \"\"\n",
      "    speaker_turn_count = 0\n",
      "    for line in model_response_all.splitlines():\n",
      "        if line.startswith(\"Speaker\"):  # Detect speaker changes\n",
      "            speaker_turn_count += 1\n",
      "            line = line.replace(\"Speaker 1\", \"Sender\")\n",
      "            line = line.replace(\"Speaker 2\", \"Andres Perez\")\n",
      "            if speaker_turn_count > pair_count:\n",
      "                result.append(current_pair)\n",
      "                current_pair = \"\"\n",
      "                speaker_turn_count = 1  # Reset count for a new pair\n",
      "\n",
      "        current_pair += line + \"\\n\"  \n",
      "\n",
      "    # Add the last pair (if any)\n",
      "    if current_pair:\n",
      "        result.append(current_pair)\n",
      "    return result\n",
      "\n",
      "def data_preparation(bucket_name, directory, gemini_pro_model, pair_count=6, data_augmentation_iter=4):\n",
      "    transcripts = process_transcripts(\n",
      "        bucket_name=bucket_name, directory=directory, gemini_pro_model=gemini_pro_model,\n",
      "        pair_count=pair_count, data_augmentation_iter=data_augmentation_iter\n",
      "    )\n",
      "    whatsapp = process_whatsapp_chat(bucket_name=bucket_name, directory=directory, pair_count=pair_count)\n",
      "    input_data = transcripts + whatsapp\n",
      "    print(\"Number of elements in the list\", len(input_data))\n",
      "    print(input_data[0:15])\n",
      "    return input_data\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('--bucket-name', dest='bucket-name',\n",
      "                        default='able-analyst-416817-chatbot-v1', type=str, help='GCS URI for saving model artifacts.')\n",
      "    parser.add_argument('--directory', dest='directory', \n",
      "                        default='input_data/andrehpereh', type=str, help='TF-Hub URL.')\n",
      "    args = parser.parse_args()\n",
      "    hparams = args.__dict__\n",
      "    process_whatsapp_chat(hparams['bucket-name'], hparams['directory'])\n",
      "--- File: ./components/data_preparation/Dockerfile ---\n",
      "FROM python:3.9-slim\n",
      "\n",
      "WORKDIR /app\n",
      "COPY requirements.txt .\n",
      "RUN pip install -U -r requirements.txt\n",
      "COPY . /app\n",
      "WORKDIR /app\n",
      "RUN ls\n",
      "RUN pwd\n",
      "RUN pip list\n",
      "ENTRYPOINT [\"python\"]  \n",
      "# CMD [\"data_ingestion.py\"]\n",
      "--- File: ./components/data_preparation/.ipynb_checkpoints/task-checkpoint.py ---\n",
      "import os\n",
      "import argparse\n",
      "\n",
      "import data_ingestion\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('--bucket-name', dest='bucket-name',\n",
      "                        default='able-analyst-416817-chatbot-v1', type=str, help='GCS URI for saving model artifacts.')\n",
      "    parser.add_argument('--directory', dest='directory', \n",
      "                        default='input_data/andrehpereh', type=str, help='TF-Hub URL.')\n",
      "    args = parser.parse_args()\n",
      "    hparams = args.__dict__\n",
      "    data_ingestion.process_whatsapp_chat(hparams['bucket-name'], hparams['directory'])\n",
      "--- File: ./components/data_preparation/.ipynb_checkpoints/data_ingestion-checkpoint.py ---\n",
      "import re\n",
      "import os\n",
      "import sys\n",
      "import json\n",
      "import argparse\n",
      "from typing import List\n",
      "from google.cloud import storage\n",
      "from io import BytesIO\n",
      "\n",
      "def process_whatsapp_chat(bucket_name: str, directory: str, pair_count: int=6) -> List[str]:\n",
      "    print(\"Bucket Name\", bucket_name)\n",
      "    print(\"Directory\", directory)\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.get_bucket(bucket_name)\n",
      "    print(bucket_name)\n",
      "    print(directory)\n",
      "\n",
      "    current_sender = None\n",
      "    current_file = None\n",
      "    consecutive_messages = []\n",
      "    qa_pairs_all = []  # List to store question-answer pairs\n",
      "    print(bucket.list_blobs(prefix=directory))\n",
      "\n",
      "    for blob in bucket.list_blobs(prefix=directory):\n",
      "        # Extract filename from blob\n",
      "        print(blob.name)\n",
      "        filename = blob.name.split('/')[-1]\n",
      "\n",
      "        if filename.endswith('.txt') and 'WhatsApp' in filename:\n",
      "            print(filename)\n",
      "            file_content = blob.download_as_string().decode('utf-8')\n",
      "            lines = file_content.split('\\n')\n",
      "\n",
      "            for line in lines:\n",
      "                if line.startswith(('- ', '\\n')):\n",
      "                    continue\n",
      "\n",
      "                # Extract sender and message (Regex handles potential variations)\n",
      "                match = re.search(r'^.*-\\s(?P<sender>.*?):\\s(?P<message>.*)$', line)\n",
      "                if match:\n",
      "                    sender = match.group('sender').strip()\n",
      "                    if sender != 'Andres Perez':\n",
      "                        sender = 'Sender'\n",
      "                    message = match.group('message').strip()\n",
      "                    message = message.replace(\"<Media omitted>\", \"\")\n",
      "                    message = message.replace(\"Missed video call\", \"\")\n",
      "                    message = message.replace(\"null\", \"\")\n",
      "                    message = re.sub(r'http\\S+', '', message).strip()\n",
      "\n",
      "                    # Concatenate consecutive messages by the same sender\n",
      "                    if sender == current_sender:\n",
      "                        consecutive_messages.append(message)\n",
      "                    else:\n",
      "                        if consecutive_messages:\n",
      "                            qa_pairs_all.append(' '.join(consecutive_messages))\n",
      "                        current_sender = sender\n",
      "                        consecutive_messages = [f\"{sender}:\\n{message}\"]\n",
      "\n",
      "            # Add the last set of messages\n",
      "            if consecutive_messages:\n",
      "                qa_pairs_all.append(', '.join(consecutive_messages))\n",
      "\n",
      "    result = []\n",
      "    current_group = \"\"\n",
      "    for i, element in enumerate(qa_pairs_all):\n",
      "        current_group += element \n",
      "        current_group += \"\\n\\n\"  # Add double newline after every even element\n",
      "\n",
      "        if (i + 1) % pair_count == 0:\n",
      "            result.append(current_group)\n",
      "            current_group = \"\"  # Reset for the next group\n",
      "\n",
      "    # Handle a potential incomplete last group\n",
      "    if current_group:\n",
      "        result.append(current_group)\n",
      "    return result\n",
      "\n",
      "\n",
      "def process_transcripts(bucket_name, directory, gemini_pro_model, pair_count=6, data_augmentation_iter=4):\n",
      "    \"\"\"Processes all text files within a folder.\n",
      "\n",
      "    Args:\n",
      "        folder_path: Path to the input folder.\n",
      "        gemini_pro_model: The model used for generating responses.\n",
      "    \"\"\"\n",
      "\n",
      "    storage_client = storage.Client()\n",
      "    bucket = storage_client.get_bucket(bucket_name)\n",
      "    model_response_all = \"\"\n",
      "    script_dir = os.path.dirname(__file__)  # Get the directory of the current script\n",
      "    filepath = os.path.join(script_dir, 'prompt.json')\n",
      "\n",
      "    with open(filepath, \"r\") as file:\n",
      "        data = json.load(file)\n",
      "    pre_prompt = data[\"prompt\"]\n",
      "\n",
      "    for blob in bucket.list_blobs(prefix=directory):\n",
      "        # Extract filename from blob\n",
      "        print(blob.name)\n",
      "        filename = blob.name.split('/')[-1]\n",
      "        if filename.endswith('.txt') and 'transcript' in filename:\n",
      "            # Read content of the file\n",
      "            print(filename)\n",
      "            contents = blob.download_as_string().decode('utf-8')\n",
      "            prompt = pre_prompt.format(contents)\n",
      "            for i in range(data_augmentation_iter):\n",
      "                try:\n",
      "                    model_response = gemini_pro_model.generate_content(prompt).text\n",
      "                    model_response_all += model_response.replace(\"** \", \"\\n\").replace(\"**\", \"\")\n",
      "                    print(\"-----------------------------------------------------------------------------------------------------------------------------------\")\n",
      "                except Exception as e:  # Catch any type of error\n",
      "                    print(f\"An error occurred: {e}. Skipping...\")\n",
      "\n",
      "    result = []\n",
      "    current_pair = \"\"\n",
      "    speaker_turn_count = 0\n",
      "    for line in model_response_all.splitlines():\n",
      "        if line.startswith(\"Speaker\"):  # Detect speaker changes\n",
      "            speaker_turn_count += 1\n",
      "            line = line.replace(\"Speaker 1\", \"Sender\")\n",
      "            line = line.replace(\"Speaker 2\", \"Andres Perez\")\n",
      "            if speaker_turn_count > pair_count:\n",
      "                result.append(current_pair)\n",
      "                current_pair = \"\"\n",
      "                speaker_turn_count = 1  # Reset count for a new pair\n",
      "\n",
      "        current_pair += line + \"\\n\"  \n",
      "\n",
      "    # Add the last pair (if any)\n",
      "    if current_pair:\n",
      "        result.append(current_pair)\n",
      "    return result\n",
      "\n",
      "def data_preparation(bucket_name, directory, gemini_pro_model, pair_count=6, data_augmentation_iter=4):\n",
      "    transcripts = process_transcripts(\n",
      "        bucket_name=bucket_name, directory=directory, gemini_pro_model=gemini_pro_model,\n",
      "        pair_count=pair_count, data_augmentation_iter=data_augmentation_iter\n",
      "    )\n",
      "    whatsapp = process_whatsapp_chat(bucket_name=bucket_name, directory=directory, pair_count=pair_count)\n",
      "    input_data = transcripts + whatsapp\n",
      "    print(\"Number of elements in the list\", len(input_data))\n",
      "    print(input_data[0:15])\n",
      "    return input_data\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    parser = argparse.ArgumentParser()\n",
      "    parser.add_argument('--bucket-name', dest='bucket-name',\n",
      "                        default='able-analyst-416817-chatbot-v1', type=str, help='GCS URI for saving model artifacts.')\n",
      "    parser.add_argument('--directory', dest='directory', \n",
      "                        default='input_data/andrehpereh', type=str, help='TF-Hub URL.')\n",
      "    args = parser.parse_args()\n",
      "    hparams = args.__dict__\n",
      "    process_whatsapp_chat(hparams['bucket-name'], hparams['directory'])\n"
     ]
    }
   ],
   "source": [
    "print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ad907c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_chat_response(chat, doc_prompt + all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "022c4c4c-00e9-4385-a65c-fb6cde043f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('test.txt', 'w') as f:\n",
    "    f.write(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b05b0ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('test.txt', 'r') as f:\n",
    "#     res = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fe1c897d-ac68-4018-b58d-f5ec7bd31574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res1 = get_chat_response(chat, yes + res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b301c333-c63e-4417-a1ca-d96295731c72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Building Your Own AI Chatbot: A Hands-on Tutorial with Python and Google Cloud\n",
      "\n",
      "**Introduction:**\n",
      "\n",
      "Welcome to an exciting journey beyond basic machine learning tutorials! In this guide, we'll delve into the creation of a custom chatbot platform using Python and the power of Google Cloud Platform (GCP). We'll explore advanced concepts like Large Language Models (LLMs), fine-tuning techniques, pipeline orchestration, and cloud-native technologies. \n",
      "\n",
      "**Prerequisites:**\n",
      "\n",
      "* **Advanced Python Knowledge:** A solid understanding of Python programming, including object-oriented programming, modules, and libraries.\n",
      "* **Machine Learning Fundamentals:** Familiarity with machine learning concepts like training, inference, and model evaluation. \n",
      "* **GCP Account:** Access to a GCP account for utilizing the various cloud services involved. \n",
      "\n",
      "**Project Setup:**\n",
      "\n",
      "1. **Clone the Repository:**\n",
      "```bash\n",
      "git clone https://github.com/your-username/chatbot-project.git\n",
      "cd chatbot-project\n",
      "```\n",
      "\n",
      "2. **Install Dependencies:**\n",
      "```bash\n",
      "pip install -r requirements.txt\n",
      "```\n",
      "\n",
      "3. **Configure GCP:** Set up your GCP project, enable required APIs (Vertex AI, Kubeflow Pipelines, BigQuery, Cloud Storage, etc.), and configure authentication.\n",
      "\n",
      "**Building Blocks of the Chatbot Platform:**\n",
      "\n",
      "1. **Cloud Build Configuration (`cloudbuild_compiler.py`):**\n",
      "    * This script automates the merging of multiple Cloud Build configuration files (YAML) into a single master JSON file, streamlining the build process. \n",
      "    * The `merge_cloudbuild_files()` function combines configurations, adds descriptive comments, and identifies necessary substitution variables.\n",
      "    * The `find_missing_elements()` function checks for any missing substitutions to ensure a smooth build.\n",
      "\n",
      "2. **Data Preparation (`components/data_preparation/`):**\n",
      "    * The `data_ingestion.py` script handles data ingestion and formatting. It can process both WhatsApp chat transcripts and other text data, extracting conversational exchanges and preparing them for training.\n",
      "    * The `process_whatsapp_chat()` function extracts conversation pairs from WhatsApp chat transcripts.\n",
      "    * The `process_transcripts()` function uses the Gemini API to generate responses based on provided transcripts, augmenting the training data.\n",
      "\n",
      "3. **Model Fine-tuning (`components/fine_tunning/`):**\n",
      "    * The `trainer.py` script performs the fine-tuning process using Keras-NLP and LoRA.\n",
      "    * The `finetune_gemma()` function loads a pre-trained Gemma model, optionally enables LoRA for efficient fine-tuning, and trains the model on the prepared data.\n",
      "    * The `util.py` module provides helper functions for uploading and downloading files to/from GCS, managing data and model artifacts.\n",
      "    * The `conversion_function.py` script converts the fine-tuned Keras-NLP Gemma model to a Hugging Face Transformers model for wider compatibility.\n",
      "\n",
      "4. **Kubeflow Pipelines (`components/pipeline/`):**\n",
      "    * The `pipeline.py` script defines the Kubeflow pipeline that orchestrates the entire training and deployment process.\n",
      "    * The pipeline includes components for data preparation, model fine-tuning, model conversion, model deployment to Vertex AI, and sending email notifications upon completion.\n",
      "    * The `util.py` module in this component provides functions like `get_model_paths_and_config()` to manage model configurations and paths.\n",
      "\n",
      "5. **Cloud Functions Trigger (`components/cloud_functions/`):**\n",
      "    * The `main.py` script contains a Cloud Function triggered upon user registration. \n",
      "    * This function extracts information from the Pub/Sub message (including user data and training parameters) and triggers the Kubeflow pipeline for personalized chatbot creation. \n",
      "\n",
      "6. **Flask Web Application (`components/app_flask/`):**\n",
      "    * The `app.py` script implements the Flask web application, handling user authentication, file uploads, chatbot interactions, and communication with Vertex AI for model inference. \n",
      "    * The `util.py` module provides helper functions for the application, including prediction calls to Vertex AI.\n",
      "    * The `templates/` directory contains HTML templates for the user interface (login, signup, chat interface, etc.).\n",
      "    * The `static/` directory stores CSS and JavaScript files for styling and interactivity.\n",
      "\n",
      "**Running the Chatbot Platform:**\n",
      "\n",
      "1. **Build and Push Docker Images:**\n",
      "    * Build Docker images for each component using the provided Dockerfiles. \n",
      "    * Push the images to Google Container Registry (GCR).\n",
      "\n",
      "2. **Deploy Cloud Functions:**\n",
      "    * Deploy the Cloud Function that triggers the training pipeline.\n",
      "\n",
      "3. **Deploy Flask Application:**\n",
      "    * Deploy the Flask web application to a suitable platform like App Engine or Cloud Run.\n",
      "\n",
      "4. **Set up Kubeflow Pipelines:**\n",
      "    * Configure Kubeflow Pipelines in your GCP environment.\n",
      "\n",
      "5. **Start Interacting!** \n",
      "    * Access the deployed Flask application, sign up or log in, upload your conversation data, and start chatting with your personalized AI companion! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dce9eb-38a0-43c6-8e9c-3f7215fc3262",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from vertexai.preview.generative_models import GenerativeModel\n",
    "gemini_pro_model = GenerativeModel(\"gemini-1.0-pro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6721de39-1933-4f24-9ccd-563644559125",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai import generative_models \n",
    "import vertexai\n",
    "vertexai.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf3411-a4c4-498a-83fb-267662f2288e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import os\n",
    "\n",
    "# Construct a BigQuery client object\n",
    "os.environ['PROJECT_ID'] = 'able-analyst-416817'\n",
    "client = bigquery.Client(os.environ.get('PROJECT_ID'))\n",
    "\n",
    "# Define your dataset and table information\n",
    "project_id = os.environ.get('PROJECT_ID')\n",
    "project = os.environ.get('PROJECT_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc327a85-1db9-4687-9b9e-918d7ca8e764",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from components.data_preparation import data_ingestion\n",
    "yup = data_ingestion.data_preparation(\n",
    "    bucket_name = 'personalize-chatbots-v1', directory = 'andrehpereh/input_data/',\n",
    "    gemini_pro_model= gemini_pro_model, pair_count=6, data_augmentation_iter=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b1ac9d-c6af-4fa9-a9df-ce2d420868a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218769f5-1910-4424-b1e2-8d12b88a7bf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def translate_text(target: str, text: str) -> dict:\n",
    "    \"\"\"Translates text into the target language.\n",
    "\n",
    "    Target must be an ISO 639-1 language code.\n",
    "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "    \"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    if isinstance(text, bytes):\n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "\n",
    "    print(\"Text: {}\".format(result[\"input\"]))\n",
    "    print(\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "    print(\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca20d31-1b40-4189-a0bd-63e88f7a4b99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./input_data/whatsapp_spanish.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "lines[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39eb8f08-019b-4924-bc84-27b8a1594985",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf40dd8d-8999-4236-a65f-4a88268c3e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the Google Cloud Translation library\n",
    "from google.cloud import translate\n",
    "\n",
    "\n",
    "# Initialize Translation client\n",
    "def translate_text(\n",
    "    text: str = \"YOUR_TEXT_TO_TRANSLATE\", project_id: str = \"YOUR_PROJECT_ID\"\n",
    ") -> translate.TranslationServiceClient:\n",
    "    \"\"\"Translating Text.\"\"\"\n",
    "\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    location = \"global\"\n",
    "\n",
    "    parent = f\"projects/{project_id}/locations/{location}\"\n",
    "\n",
    "    # Translate text from English to French\n",
    "    # Detail on supported types can be found here:\n",
    "    # https://cloud.google.com/translate/docs/supported-formats\n",
    "    response = client.translate_text(\n",
    "        request={\n",
    "            \"parent\": parent,\n",
    "            \"contents\": [text],\n",
    "            \"mime_type\": \"text/plain\",  # mime types: text/plain, text/html\n",
    "            \"source_language_code\": \"en-US\",\n",
    "            \"target_language_code\": \"fr\",\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Display the translation for each input text provided\n",
    "    for translation in response.translations:\n",
    "        print(f\"Translated text: {translation.translated_text}\")\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec31999c-4215-43a8-848a-f15d3ed80b47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_languages_with_target(target: str) -> dict:\n",
    "    \"\"\"Lists all available languages and localizes them to the target language.\n",
    "\n",
    "    Target must be an ISO 639-1 language code.\n",
    "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "    \"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    results = translate_client.get_languages(target_language=target)\n",
    "\n",
    "    for language in results:\n",
    "        print(\"{name} ({language})\".format(**language))\n",
    "\n",
    "    return results\n",
    "list_languages_with_target('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f63c92-6014-4306-a4d0-2b91fbbf4cc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_languages() -> dict:\n",
    "    \"\"\"Lists all available languages.\"\"\"\n",
    "    from google.cloud import translate_v2 as translate\n",
    "\n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    results = translate_client.get_languages()\n",
    "\n",
    "    for language in results:\n",
    "        print(\"{name} ({language})\".format(**language))\n",
    "\n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60e8225-3530-4d6d-b924-5adb29fdcda3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "bucket_name = 'personalize-chatbots-v1'\n",
    "blob_name = 'hermi_test/hermi_whatsapp.txt'\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket(bucket_name)\n",
    "blob = bucket.blob(blob_name)\n",
    "contents = blob.download_as_string().decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72aa7829-e1ad-4435-b514-005e71d43370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d24a6e-306b-4dc7-81aa-c1a4cb30bf30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90787233-de02-41df-843c-6a92663236dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = translate_text('en', contents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf887d-8c37-4a37-b643-304ed62d40bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(res[\"translatedText\"][350:750])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d5bcc-09c0-494d-821a-1ddf56d2625b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45159c3-0189-4f20-9f5f-0e9dce92a8d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m119"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
