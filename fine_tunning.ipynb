{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64e71634-759d-4ed1-928c-f141e4e968d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 22:21:03.295705: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-03 22:21:03.458000: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-03 22:21:03.458036: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-03 22:21:03.460252: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-03 22:21:03.472878: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "# Note: `userdata.get` is a Colab API. If you're not using Colab, set the env\n",
    "# vars as appropriate for your system.\n",
    "\n",
    "os.environ[\"KAGGLE_USERNAME\"] = 'andrehpereh1'\n",
    "os.environ[\"KAGGLE_KEY\"] = '5859e39806d9456749dcbac685f04bc9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38279b76-2622-4a3c-aafa-4c9a6e2f60cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Hugging Face Transformers\n",
    "%pip install --upgrade --quiet accelerate sentencepiece transformers\n",
    "\n",
    "# Vertex AI SDK\n",
    "#%pip install --upgrade --quiet google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f0f9a0-7695-46da-a9f2-6722442d9548",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # Or \"torch\" or \"tensorflow\".\n",
    "# Avoid memory fragmentation on JAX backend.\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]=\"1.00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241ca24-16af-49f1-a2b6-8fb5e3ece005",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# flash-district-241318\n",
    "\n",
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "\n",
    "print(f\"{PROJECT_ID=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbdc380-ccde-47ef-8092-2a4c6e2feb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List your projects\n",
    "# !gcloud projects list\n",
    "\n",
    "# Define the default project\n",
    "# PROJECT_ID = \"\"  # @param {type:\"string\"}\n",
    "# !gcloud config set core/project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6a40ad-b983-4057-ad17-21c6602b5ec0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "REGION = \"europe-west4\"  # @param {type: \"string\"}\n",
    "\n",
    "!gcloud config set ai/region $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f1b10-d799-4098-bbb1-d3c5da3da58a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a bucket related to your project\n",
    "BUCKET_URI = f\"gs://gemma-{PROJECT_ID}-unique\"\n",
    "BUCKET_URI = \"gs://gemma-andrehpereh-chatbot-unique\"\n",
    "# Or use an existing one\n",
    "# BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "\n",
    "res = !gcloud storage buckets describe $BUCKET_URI --format \"value(name)\"\n",
    "if len(res) == 1 and \"ERROR\" not in res[0]:\n",
    "    print(\"✔️ The bucket exists\")\n",
    "else:\n",
    "    print(\"⚙️ Creating the bucket…\")\n",
    "    !gcloud storage buckets create $BUCKET_URI --project $PROJECT_ID --location $REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278c096e-97a3-4279-8535-55f92dc84650",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = \"gs://gemma-andrehpereh-chatbot-unique\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522de476-26d3-444a-af03-fa43c6c5ed1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create the service account for the Vertex AI endpoint\n",
    "SERVICE_ACCOUNT_NAME = \"gemma-vertexai\"\n",
    "SERVICE_ACCOUNT_DISPLAY_NAME = \"Gemma Vertex AI endpoint\"\n",
    "SERVICE_ACCOUNT = f\"{SERVICE_ACCOUNT_NAME}@{PROJECT_ID}.iam.gserviceaccount.com\"\n",
    "# Or use an existing one\n",
    "# SERVICE_ACCOUNT = \"\"  # @param {type:\"string\"}\n",
    "assert SERVICE_ACCOUNT.endswith(f\"@{PROJECT_ID}.iam.gserviceaccount.com\")\n",
    "\n",
    "res = !gcloud iam service-accounts describe $SERVICE_ACCOUNT --format \"value(email)\"\n",
    "if len(res) == 1 and \"ERROR\" not in res[0]:\n",
    "    print(\"✔️ The service account exists\")\n",
    "else:\n",
    "    print(\"⚙️ Creating the service account…\")\n",
    "    !gcloud iam service-accounts create \"gemma-vertexai\" --display-name \"Gemma Vertex AI endpoint\"\n",
    "    # Grant \"Storage Object Admin\" role\n",
    "    !gcloud projects add-iam-policy-binding \"flash-district-241318\" --member \"serviceAccount:gemma-vertexai@flash-district-241318.iam.gserviceaccount.com\" --role \"roles/storage.objectAdmin\"\n",
    "    # Grant \"Vertex AI User\" role\n",
    "    !gcloud projects add-iam-policy-binding \"flash-district-241318\" --member \"serviceAccount:gemma-vertexai@flash-district-241318.iam.gserviceaccount.com\" --role \"roles/aiplatform\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7de0d1-a691-45ee-8c5d-27307703ac84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_NAME = \"gemma-vertexai\"\n",
    "SERVICE_ACCOUNT_DISPLAY_NAME = \"Gemma Vertex AI endpoint\"\n",
    "SERVICE_ACCOUNT = f\"{SERVICE_ACCOUNT_NAME}@{PROJECT_ID}.iam.gserviceaccount.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6fb9c-c6a0-4b45-8dc7-3164fecca978",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eb30db-1873-4cce-8300-f2ddaeec8e5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf6e36-7a57-4313-b6d7-8c6cf215d786",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcloud iam service-accounts describe gemma-vertexai@flash-district-241318.iam.gserviceaccount.com --format \"value(email)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa120e9-dc57-4ff6-ba5d-7e3b84b502e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import locale\n",
    "\n",
    "import keras\n",
    "import keras_nlp\n",
    "import torch\n",
    "import transformers\n",
    "from google.cloud import aiplatform\n",
    "from numba import cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07322114-e528-4b21-bf5f-28b7c2e43158",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5921bbf1-50c3-4f87-9d35-6b91c4d26dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a957e5-8dc7-481a-919e-ce8cac8e2110",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5205bce-893a-495b-8698-0da7b41e9086",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gemma_2b_en\"\n",
    "# MODEL_NAME = \"gemma_instruct_2b_en\"\n",
    "# MODEL_NAME = \"gemma_7b_en\"\n",
    "# MODEL_NAME = \"gemma_instruct_7b_en\"\n",
    "\n",
    "# Deduce model size from name format: \"gemma[_instruct]_{2b,7b}_en\"\n",
    "MODEL_SIZE = MODEL_NAME.split(\"_\")[-2]\n",
    "assert MODEL_SIZE in (\"2b\", \"7b\")\n",
    "\n",
    "\n",
    "# Finetuned model\n",
    "FINETUNED_MODEL_DIR = f\"./{MODEL_NAME}\"\n",
    "FINETUNED_WEIGHTS_PATH = f\"{FINETUNED_MODEL_DIR}/model.weights.h5\"\n",
    "FINETUNED_VOCAB_PATH = f\"{FINETUNED_MODEL_DIR}/vocabulary.spm\"\n",
    "\n",
    "# Converted model\n",
    "HUGGINGFACE_MODEL_DIR = f\"./{MODEL_NAME}_huggingface\"\n",
    "\n",
    "# Deployed model\n",
    "DEPLOYED_MODEL_URI = f\"{BUCKET_URI}/{MODEL_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ebf70f1-09ba-4de0-a864-e755e96786c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f800468-2899-4138-b20d-7a967cbab029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./CHAT/WhatsApp Chat with Mike Haarlem.txt\n",
      "./CHAT/WhatsApp Chat with Ilse Flatmate.txt\n",
      "./CHAT/WhatsApp Chat with Rosa Rosa Rosa.txt\n",
      "./CHAT/WhatsApp Chat with Ruben Ewald Puijker.txt\n",
      "./CHAT/WhatsApp Chat with Anki.txt\n",
      "./CHAT/WhatsApp Chat with Michael.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def process_whatsapp_chat(directory):\n",
    "\n",
    "\n",
    "    current_sender = None\n",
    "    current_file = None\n",
    "    consecutive_messages = []\n",
    "    qa_pairs_all = []  # List to store question-answer pairs\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        consecutive_messages = []\n",
    "        qa_pairs = []  # List to store question-answer pairs\n",
    "        # Check if the current item is a file and ends with \".txt\"\n",
    "        if os.path.isfile(filepath) and filename.endswith('.txt'):\n",
    "            print(filepath)\n",
    "             \n",
    "            with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                lines = file.readlines()\n",
    "\n",
    "            for line in lines:\n",
    "                if line.startswith(('- ', '\\n')):\n",
    "                    continue\n",
    "\n",
    "                # Extract sender and message (Regex handles potential variations)\n",
    "                match = re.search(r'^.*-\\s(?P<sender>.*?):\\s(?P<message>.*)$', line)\n",
    "                if match:\n",
    "                    sender = match.group('sender').strip()\n",
    "                    # if filepath !=current_file and sender == 'Andres Perez':\n",
    "                    #    current_file = filepath \n",
    "                    #    continue\n",
    "                    if sender != 'Andres Perez':\n",
    "                        sender = 'Sender'\n",
    "                    message = match.group('message').strip()\n",
    "                    message = message.replace(\"<Media omitted>\", \"\")\n",
    "                    message = message.replace(\"Missed video call\", \"\")\n",
    "                    message = message.replace(\"null\", \"\")\n",
    "                    message = re.sub(r'http\\S+', '', message).strip()\n",
    "                    # if len(message) <=1:\n",
    "                    #    continue\n",
    "\n",
    "                    # Concatenate consecutive messages by the same sender\n",
    "                    if sender == current_sender:\n",
    "                        consecutive_messages.append(message)\n",
    "                    else:\n",
    "                        if consecutive_messages:\n",
    "                            qa_pairs.append(' '.join(consecutive_messages))\n",
    "                        current_sender = sender\n",
    "                        consecutive_messages = [f\"{sender}:\\n{message}\"]\n",
    "\n",
    "            # Add the last set of messages\n",
    "            if consecutive_messages:\n",
    "                qa_pairs.append(', '.join(consecutive_messages))\n",
    "        \n",
    "        qa_pairs_all.extend(qa_pairs)\n",
    "    if len(qa_pairs_all) % 2 != 0:\n",
    "        qa_pairs_all = qa_pairs_all[:-1]\n",
    "\n",
    "    res = np.array(qa_pairs_all).reshape(len(qa_pairs_all) // 2, 2)\n",
    "    formatted_messages = [f\"{message_pair[0]}\\n\\n{message_pair[1]}\" for message_pair in res]\n",
    "    return formatted_messages \n",
    "\n",
    "# Example Usage:\n",
    "directory = \"./CHAT\" \n",
    "data = process_whatsapp_chat(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78501005-57a9-42e0-814b-48e41ef493c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sender:\\nI just did my nails😂 maybe tomorrow after work?\\n\\nAndres Perez:\\nHaha Yeah better tomorrow',\n",
       " 'Sender:\\nCool\\n\\nAndres Perez:\\nThanks 🙏',\n",
       " 'Sender:\\nHave fun at your rave😆 Don’t forget tomorrow morning maybe the guy from Coolblue is coming😂😅✌🏻 Goodnight!😂😂\\n\\nAndres Perez:\\nNo worries What time is he supposed to come? Do you think they could let us know when they are on their way?',\n",
       " 'Sender:\\nThe time frame was between 9 and 15\\U0001fae0 Jesper will receive a message probably when they arrive\\n\\nAndres Perez:\\nHaha I am on the football pitch So I can be there in 4 mins',\n",
       " 'Sender:\\nHmm okay I’m going to call the client service of coolblue, but if they can deliver it tomorrow or tuesday are you home during the day?\\n\\nAndres Perez:\\nYes, I will be Excuse to not to go to the office :)']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1254:1259]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfb6ca83-edc4-4bc7-a95f-476e758d0708",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n",
      "0.8.2\n",
      "3.0.5\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "import keras_nlp\n",
    "print(keras_nlp.__version__)\n",
    "import keras\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7f563997-84a5-4545-bb0d-d509e34774c3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-03 22:21:41.014930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20758 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma_2b_en\")\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7180ad-6ece-4271-9de8-0f25962a0450",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"What is the plan for tonight?\",\n",
    "    \"What would you like to drink?\",\n",
    "    \"Are you coming tonight?\",\n",
    "    \"So, what happened?\",\n",
    "    \"Bro\",\n",
    "    'Was she hot?'\n",
    "]\n",
    "\n",
    "# Prompt template for the training data and the finetuning tests\n",
    "PROMPT_TEMPLATE = \"Sender:\\n{instruction}\\n\\nAndres Perez:\\n{response}\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    PROMPT_TEMPLATE.format(instruction=example, response=\"\")\n",
    "    for example in TEST_EXAMPLES\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772af605-49b8-41f2-b5c5-c0d9fbaba3a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler(k=3)\n",
    "gemma_lm.compile(sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b1d919-d6c2-4381-899c-7496f93e2a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for test_example in TEST_EXAMPLES:\n",
    "    response = gemma_lm.generate(test_example, max_length=48)\n",
    "    output = response[len(test_example) :]\n",
    "    print(f\"{test_example}\\n{output!r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1ac8ab4-3452-4da0-8eaf-fc712de5bbcc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,508,218,368</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,508,218,368\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,508,218,368</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,508,218,368\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,045,952</span> (7.80 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,045,952\u001b[0m (7.80 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Enable LoRA for the model and set the LoRA rank to 4.\n",
    "gemma_lm.backbone.enable_lora(rank=6)\n",
    "gemma_lm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa620293-0515-47c9-88be-29c5249052c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 09:12:25.492411: W tensorflow/compiler/tf2xla/kernels/assert_op.cc:38] Ignoring Assert operator compile_loss/sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/4495\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m47:11:45\u001b[0m 38s/step - loss: 0.3324 - sparse_categorical_accuracy: 0.3871"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1709543556.657157  365026 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4495/4495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m774s\u001b[0m 164ms/step - loss: 0.3174 - sparse_categorical_accuracy: 0.5469\n",
      "Epoch 2/2\n",
      "\u001b[1m4495/4495\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m736s\u001b[0m 164ms/step - loss: 0.3095 - sparse_categorical_accuracy: 0.5544\n"
     ]
    }
   ],
   "source": [
    "def finetune_gemma(model: keras_nlp.models.GemmaCausalLM, data: list[str]):\n",
    "    # Reduce the input sequence length to limit memory usage\n",
    "    model.preprocessor.sequence_length = 256\n",
    "\n",
    "    # Use AdamW (a common optimizer for transformer models)\n",
    "    optimizer = keras.optimizers.AdamW(\n",
    "        learning_rate=5e-5,\n",
    "        weight_decay=0.01,\n",
    "    )\n",
    "\n",
    "    # Exclude layernorm and bias terms from decay\n",
    "    optimizer.exclude_from_weight_decay(var_names=[\"bias\", \"scale\"])\n",
    "\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=optimizer,\n",
    "        weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "        sampler=\"greedy\",\n",
    "    )\n",
    "    model.fit(data, epochs=2, batch_size=1)\n",
    "\n",
    "\n",
    "finetune_gemma(gemma_lm, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8c406b0-3357-4cb7-ba9f-4cd6646f109e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58263c63-4cd7-4cc2-a7f5-e901129cd8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sampler = keras_nlp.samplers.TopKSampler()\n",
    "gemma_lm.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d19d2e-213c-40fa-885d-8c2055145a84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for test_example in TEST_EXAMPLES:\n",
    "    print(test_example)\n",
    "    # response = gemma_lm.generate(test_example, max_length=48)\n",
    "    #print(response)\n",
    "    #output = response[len(test_example) :]\n",
    "    #print(f\"{test_example}\\n{output!r}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b80d1b2-5bdb-4a03-9774-df94404f2c18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5a8df3-91ce-49e1-9d7f-77fe4b222e8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9637c9b9-e19e-4fed-a8dd-a714a2faf6e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"😂 bro whats your salary?\",\n",
    "    \"Are you home?\",\n",
    "    \"Did you manage to finish?\",\n",
    "    \"What is the plan for tonight?\",\n",
    "    \"Do you have siblings?\",\n",
    "    \"Are you gay?\",\n",
    "    \"What's your name\",\n",
    "    \"Where are you from?\",\n",
    "    'Would you like to have kids?',\n",
    "    \"Who is your best friend?\",\n",
    "    \"Are we going out tonight?\",\n",
    "    \"Where should we go next?\",\n",
    "    \"Where do you wanna go on holidays?\",\n",
    "    \"What would you like to drink?\",\n",
    "    \"Are you coming tonight?\",\n",
    "    \"So, what happened?\",\n",
    "    \"Bro\",\n",
    "    'Was she hot?',\n",
    "    'It was nice to see you',\n",
    "    'Tell me a crazy romantic story',\n",
    "    'Have you talked to Veronique?',\n",
    "    'Are you working tomorrow?',\n",
    "    'Tell me a joke',\n",
    "    'Tell me a racist joke',\n",
    "    'Btw, I know  you are working out so hard lately, why do you do it? what motivates you?',\n",
    "    'How would you describe Haarlem?',\n",
    "    'What do you love the most about being alive?',\n",
    "    \"What are your dreams?\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10842b58-163f-414d-9765-ad237b52171a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TEST_EXAMPLES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m instruction \u001b[38;5;129;01min\u001b[39;00m \u001b[43mTEST_EXAMPLES\u001b[49m:\n\u001b[1;32m      2\u001b[0m     template \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSender:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{instruction}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAndres Perez:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{response}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m     prompt \u001b[38;5;241m=\u001b[39m template\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m      4\u001b[0m         instruction\u001b[38;5;241m=\u001b[39minstruction,\n\u001b[1;32m      5\u001b[0m         response\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      6\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TEST_EXAMPLES' is not defined"
     ]
    }
   ],
   "source": [
    "for instruction in TEST_EXAMPLES:\n",
    "    template = \"Sender:\\n{instruction}\\n\\nAndres Perez:\\n{response}\"\n",
    "    prompt = template.format(\n",
    "        instruction=instruction,\n",
    "        response=\"\",\n",
    "    )\n",
    "    sampler = keras_nlp.samplers.TopKSampler(k=3)\n",
    "    gemma_lm.compile(sampler=sampler)\n",
    "    print(gemma_lm.generate(prompt, max_length=256), \"\\n\\n\\n\")\n",
    "\n",
    "   # template = \"Andres Perez:\\n{instruction}\\n\\nSender:\\n{response}\"\n",
    "  #  prompt = template.format(\n",
    "#        instruction=instruction,\n",
    " #       response=\"\",\n",
    "    #)\n",
    "    #sampler = keras_nlp.samplers.TopKSampler(k=2)\n",
    "    #gemma_lm.compile(sampler=sampler)\n",
    "    #print(gemma_lm.generate(prompt, max_length=64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26228da-3101-49e6-b490-35d57d3dba18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FINETUNED_MODEL_DIR_8_EPOCHS = './gemma_2b_en_12_Epochs_v2/'\n",
    "FINETUNED_WEIGHTS_PATH_8_EPOCHS = f\"{FINETUNED_MODEL_DIR_8_EPOCHS}/model.weights.h5\"\n",
    "FINETUNED_VOCAB_PATH_8_EPOCHS = f\"{FINETUNED_MODEL_DIR_8_EPOCHS}/vocabulary.spm\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac976e7-2487-4d26-8850-39dc032e7060",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make sure the directory exists\n",
    "%mkdir -p $FINETUNED_MODEL_DIR_8_EPOCHS\n",
    "\n",
    "gemma_lm.save_weights(FINETUNED_WEIGHTS_PATH_8_EPOCHS)\n",
    "\n",
    "gemma_lm.preprocessor.tokenizer.save_assets(FINETUNED_MODEL_DIR_8_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6d9ef1-cdc5-44b6-993d-87fad0bf139f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff5e2a9-6c48-4aa2-beb0-571d9b12138d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!du -shc $FINETUNED_MODEL_DIR_8_EPOCHS/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee10036a-93a8-44cd-b241-9dc67e371bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!du -shc $FINETUNED_MODEL_DIR/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4ecad9-5c42-451c-a058-480113ac0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del gemma_lm\n",
    "\n",
    "device = cuda.get_current_device()\n",
    "cuda.select_device(device.id)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b094ec-4f3c-4016-9741-400dd5f95743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffef750-0c6c-4446-a626-298362057d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the conversion script from KerasNLP tools\n",
    "!wget -nv -nc https://raw.githubusercontent.com/keras-team/keras-nlp/master/tools/gemma/export_gemma_to_hf.py\n",
    "\n",
    "# Run the conversion script\n",
    "# Note: it uses the PyTorch backend of Keras (hence the KERAS_BACKEND env variable)\n",
    "!KERAS_BACKEND=torch python export_gemma_to_hf.py \\\n",
    "    --weights_file $FINETUNED_WEIGHTS_PATH \\\n",
    "    --size $MODEL_SIZE \\\n",
    "    --vocab_path $FINETUNED_VOCAB_PATH \\\n",
    "    --output_dir $HUGGINGFACE_MODEL_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba264256-338c-4182-99e7-0587b49ebbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd8ce41-5973-4400-b771-f5155db7e2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cefd49c-4d89-4c6e-a75a-a755189e4450",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.GemmaForCausalLM.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_DIR,\n",
    "    local_files_only=True,\n",
    "    device_map=\"auto\",  # Library \"accelerate\" to auto-select GPU\n",
    ")\n",
    "tokenizer = transformers.GemmaTokenizer.from_pretrained(\n",
    "    HUGGINGFACE_MODEL_DIR,\n",
    "    local_files_only=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557efb78-e3c4-4250-a3e3-5f59eb8d0c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_transformers_model(\n",
    "    model: transformers.GemmaForCausalLM,\n",
    "    tokenizer: transformers.GemmaTokenizer,\n",
    ") -> None:\n",
    "    for prompt in TEST_PROMPTS:\n",
    "        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n",
    "        outputs = model.generate(**inputs, max_length=30)\n",
    "\n",
    "        output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_transformers_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ff420d-8cf8-406e-b826-081e8bb9b9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release resources\n",
    "del model, tokenizer\n",
    "\n",
    "# Free GPU RAM\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Restore the default encoding (current issue with the transformers library)\n",
    "locale.getpreferredencoding = lambda: \"UTF-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ac7719-72e0-40a5-9187-0bcf000ac890",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad94ea8b-5455-4e34-b369-91b40a2e6b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model upload\n",
    "# Upload the model to the Cloud Storage bucket:\n",
    "!gcloud storage rsync --recursive --verbosity error $HUGGINGFACE_MODEL_DIR $DEPLOYED_MODEL_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def1d14-70da-4401-86e2-1c4650300ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud storage du $DEPLOYED_MODEL_URI --readable-sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a5d3b-1370-4d67-b3c9-87c5bbdc006b",
   "metadata": {},
   "outputs": [],
   "source": [
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    suffix = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0db21d-b396-4140-87ce-3f944319359d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fb126d-11ed-4730-a32e-dc78d0ab587a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME_VLLM = f\"{MODEL_NAME}-vllm\"\n",
    "\n",
    "# Start with a G2 Series cost-effective configuration\n",
    "match MODEL_SIZE:\n",
    "    case \"2b\":\n",
    "        machine_type = \"g2-standard-8\"\n",
    "        accelerator_type = \"NVIDIA_L4\"\n",
    "        accelerator_count = 1\n",
    "    case \"7b\":\n",
    "        machine_type = \"g2-standard-12\"\n",
    "        accelerator_type = \"NVIDIA_L4\"\n",
    "        accelerator_count = 1\n",
    "    case _:\n",
    "        assert MODEL_SIZE in (\"2b\", \"7b\")\n",
    "\n",
    "# See supported machine/GPU configurations in chosen region:\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/configure-compute\n",
    "\n",
    "# For even more performance, consider V100 and A100 GPUs\n",
    "# > Nvidia Tesla V100\n",
    "# machine_type = \"n1-standard-8\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_V100\"\n",
    "# > Nvidia Tesla A100\n",
    "# machine_type = \"a2-highgpu-1g\"\n",
    "# accelerator_type = \"NVIDIA_TESLA_A100\"\n",
    "\n",
    "# Larger `max_model_len` values will require more GPU memory\n",
    "max_model_len = 2048\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    MODEL_NAME_VLLM,\n",
    "    DEPLOYED_MODEL_URI,\n",
    "    SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c15431-3532-4f1d-97f1-009bbcf7fa45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vertexai_endpoint(endpoint: aiplatform.Endpoint):\n",
    "    for question, prompt in zip(TEST_EXAMPLES, TEST_PROMPTS):\n",
    "        instance = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 10,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 1,\n",
    "            \"raw_response\": True,\n",
    "        }\n",
    "        response = endpoint.predict(instances=[instance])\n",
    "        output = response.predictions[0]\n",
    "        print(f\"{question}\\n{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_vertexai_endpoint(endpoint)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m117",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m117"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
