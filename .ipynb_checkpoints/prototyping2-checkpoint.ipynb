{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee61f0-6718-4878-b513-cebfa470d485",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d4859b-0461-4a5b-a369-b0bf8849c39a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "from kfp import dsl\n",
    "import kfp as kfp\n",
    "from kfp.dsl import OutputPath, InputPath\n",
    "from kfp import compiler\n",
    "from google.cloud import aiplatform as vertexai\n",
    "\n",
    "@dsl.component(\n",
    "  base_image ='gcr.io/able-analyst-416817/gemma-chatbot-data-preparation:latest'\n",
    ")\n",
    "def process_whatsapp_chat_op(\n",
    "  bucket_name: str,\n",
    "  directory: str,\n",
    "  dataset_path: OutputPath('Dataset')\n",
    "):\n",
    "    import data_ingestion\n",
    "    import json\n",
    "    formatted_messages = data_ingestion.process_whatsapp_chat(bucket_name, directory)\n",
    "    with open(dataset_path, 'w') as f:\n",
    "        json.dump(formatted_messages, f)\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "  base_image = 'gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest'\n",
    ")\n",
    "def fine_tunning(\n",
    "  dataset_path: InputPath('Dataset'),\n",
    "  model_paths: dict\n",
    ") -> str:\n",
    "    import trainer\n",
    "    import json\n",
    "    import util\n",
    "    import os\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    os.makedirs(model_paths['finetuned_model_dir'], exist_ok=True)\n",
    "    finetuned_weights_path = os.path.join(model_paths['finetuned_model_dir'], 'model.weights.h5') \n",
    "    \n",
    "    model = trainer.finetune_gemma(dataset, model_paths, False)\n",
    "    print(\"Its gonna save it here\", finetuned_weights_path)\n",
    "    bucket_name = 'able-analyst-416817-chatbot-v1' # move to parameter.\n",
    "    util.upload2bs(\n",
    "        local_directory = model_paths['finetuned_model_dir'], bucket_name = bucket_name,\n",
    "        destination_subfolder = model_paths['fine_tuned_keras_blob']\n",
    "    )\n",
    "    model_gcs = \"gs://{}/{}\".format(bucket_name, model_paths['fine_tuned_keras_blob'])  \n",
    "    print(\"This is the storage bucket\", model_gcs)\n",
    "    return model_gcs\n",
    "    \n",
    "\n",
    "@dsl.component(\n",
    "  base_image = 'gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest'\n",
    ")\n",
    "def convert_checkpoints_op(\n",
    "  keras_gcs_model: str,\n",
    "  model_paths: dict\n",
    ") -> str:\n",
    "    import conversion_function\n",
    "    import os\n",
    "    import util\n",
    "    bucket_name, blob_name = os.path.dirname(keras_gcs_model).lstrip(\"gs://\").split(\"/\", 1) \n",
    "    print(\"This is the keras passed\", keras_gcs_model)\n",
    "    util.download_all_from_blob(bucket_name, model_paths['fine_tuned_keras_blob'], local_destination=model_paths['finetuned_model_dir'])\n",
    "    if os.path.exists(\"./model.weights.h5\"):\n",
    "        print(\"File exists!\")\n",
    "    else:\n",
    "        print(\"File does not exist.\")\n",
    "    converted_fined_tuned_path = conversion_function.convert_checkpoints(\n",
    "        weights_file=model_paths['finetuned_weights_path'],\n",
    "        size=model_paths['model_size'],\n",
    "        output_dir=model_paths['huggingface_model_dir'],\n",
    "        vocab_path=model_paths['finetuned_vocab_path']\n",
    "    )\n",
    "    util.upload2bs(\n",
    "        local_directory = converted_fined_tuned_path, bucket_name = bucket_name,\n",
    "        destination_subfolder = model_paths['deployed_model_blob']\n",
    "    )\n",
    "    return model_paths['deployed_model_uri']\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(name=\"Model deployment.\")\n",
    "def model_deployment_pipeline(\n",
    "    project: str = PROJECT_ID,\n",
    "    bucket_name: str = \"able-analyst-416817-chatbot-v1\",\n",
    "    directory: str = \"input_data/andrehpereh\",\n",
    "    serving_image: str = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\",\n",
    "):\n",
    "\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp, ModelDeployOp)\n",
    "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    from kfp.dsl import importer_node\n",
    "    from util import get_model_paths_and_config\n",
    "    from config import Config\n",
    "    \n",
    "    model_paths = get_model_paths_and_config(Config.MODEL_NAME)\n",
    "\n",
    "    port = 7080\n",
    "    accelerator_count=1\n",
    "    max_model_len=256\n",
    "    dtype=\"bfloat16\"\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        f\"--port={port}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    metadata = {\n",
    "      \"imageUri\": serving_image,\n",
    "      \"command\": [\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "      \"args\": vllm_args,\n",
    "      \"ports\": [\n",
    "        {\n",
    "          \"containerPort\": port\n",
    "        }\n",
    "      ],\n",
    "      \"predictRoute\": \"/generate\",\n",
    "      \"healthRoute\": \"/ping\"\n",
    "    }\n",
    "\n",
    "    whatup = process_whatsapp_chat_op(bucket_name = bucket_name, directory = directory)\n",
    "\n",
    "    trainer = fine_tunning(dataset_path=whatup.outputs['dataset_path'], model_paths=model_paths)\n",
    "    trainer.set_memory_limit(\"36G\").set_cpu_limit('12.0m').set_accelerator_limit(1).add_node_selector_constraint(model_paths['accelerator_type'])\n",
    "\n",
    "    print(\"This is the dictionary\", model_paths)\n",
    "    converted = convert_checkpoints_op(\n",
    "        keras_gcs_model=trainer.output, model_paths=model_paths\n",
    "    ).set_memory_limit(\"36G\").set_cpu_limit('8.0m').set_accelerator_limit(1).add_node_selector_constraint(model_paths['accelerator_type'])\n",
    "\n",
    "    import_unmanaged_model_task = importer_node.importer(\n",
    "        artifact_uri=converted.output,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": metadata,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=\"Mini Andres, first version automated\",\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    )\n",
    "    model_upload_op.after(import_unmanaged_model_task)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"pipelines-created-endpoint\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_paths['model_name_vllm'],\n",
    "        dedicated_resources_machine_type=model_paths['machine_type'],\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        dedicated_resources_accelerator_type=model_paths['accelerator_type'],\n",
    "        dedicated_resources_accelerator_count=model_paths['accelerator_count']\n",
    "    )\n",
    "\n",
    "    \n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=model_deployment_pipeline, package_path=\"model_deployment_pipeline.json\"\n",
    ")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "\n",
    "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
    "    display_name=\"test-model_deployment_pipeline\",\n",
    "    template_path=\"model_deployment_pipeline.json\"\n",
    ")\n",
    "vertex_pipelines_job.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8cf6e95-9d70-413f-a3e0-0da33128533f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbba336-c748-4e47-95e6-8d2e0cfb0040",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dsl.component(\n",
    "  base_image ='gcr.io/able-analyst-416817/gemma-chatbot-data-preparation:latest'\n",
    ")\n",
    "def process_whatsapp_chat_op(\n",
    "  bucket_name: str,\n",
    "  directory: str,\n",
    "  dataset_path: OutputPath('Dataset')\n",
    "):\n",
    "    import data_ingestion\n",
    "    import json\n",
    "    formatted_messages = data_ingestion.process_whatsapp_chat(bucket_name, directory)\n",
    "    with open(dataset_path, 'w') as f:\n",
    "        json.dump(formatted_messages, f)\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "  base_image = 'gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest'\n",
    ")\n",
    "def fine_tunning(\n",
    "  dataset_path: InputPath('Dataset'),\n",
    "  model_paths: dict,\n",
    "  #finetuned_weights_dir: OutputPath('Model'),\n",
    ") -> str:\n",
    "    # import test_container\n",
    "    import trainer\n",
    "    import json\n",
    "    import util\n",
    "    import os\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    os.makedirs(model_paths['finetuned_model_dir'], exist_ok=True)\n",
    "    finetuned_weights_path = os.path.join(model_paths['finetuned_model_dir'], 'model.weights.h5') \n",
    "    \n",
    "    model = trainer.finetune_gemma(dataset, model_paths, False)\n",
    "    print(\"Its gonna save it here\", finetuned_weights_path)\n",
    "    #model.save_weights(finetuned_weights_path)\n",
    "    #model.preprocessor.tokenizer.save_assets(model_paths['finetuned_model_dir'])\n",
    "    bucket_name = 'able-analyst-416817-chatbot-v1' # move to parameter.\n",
    "    util.upload2bs(\n",
    "        local_directory = model_paths['finetuned_model_dir'], bucket_name = bucket_name,\n",
    "        destination_subfolder = model_paths['fine_tuned_keras_blob']\n",
    "    )\n",
    "    model_gcs = \"gs://{}/{}\".format(bucket_name, model_paths['fine_tuned_keras_blob'])  \n",
    "    print(\"This is the storage bucket\", model_gcs)\n",
    "    return model_gcs\n",
    "    \n",
    "\n",
    "@dsl.component(\n",
    "  base_image = 'gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest'\n",
    ")\n",
    "def convert_checkpoints_op(\n",
    "  keras_gcs_model: str,\n",
    "  model_paths: dict\n",
    ") -> str:\n",
    "    import conversion_function\n",
    "    import os\n",
    "    import util\n",
    "    bucket_name, blob_name = os.path.dirname(keras_gcs_model).lstrip(\"gs://\").split(\"/\", 1) \n",
    "    print(\"This is the keras passed\", keras_gcs_model)\n",
    "    util.download_all_from_blob(bucket_name, model_paths['fine_tuned_keras_blob'], local_destination=model_paths['finetuned_model_dir'])\n",
    "    if os.path.exists(\"./model.weights.h5\"):\n",
    "        print(\"File exists!\")\n",
    "    else:\n",
    "        print(\"File does not exist.\")\n",
    "    converted_fined_tuned_path = conversion_function.convert_checkpoints(\n",
    "        weights_file=model_paths['finetuned_weights_path'],\n",
    "        size=model_paths['model_size'],\n",
    "        output_dir=model_paths['huggingface_model_dir'],\n",
    "        vocab_path=model_paths['finetuned_vocab_path']\n",
    "    )\n",
    "    util.upload2bs(\n",
    "        local_directory = converted_fined_tuned_path, bucket_name = bucket_name,\n",
    "        destination_subfolder = model_paths['deployed_model_blob']\n",
    "    )\n",
    "    return model_paths['deployed_model_uri']\n",
    "\n",
    "\n",
    "\n",
    "@kfp.dsl.pipeline(name=\"Model deployment.\")\n",
    "def model_deployment_pipeline(\n",
    "    project: str = PROJECT_ID, bucket_name: str = \"able-analyst-416817-chatbot-v1\", directory: str = \"input_data/andrehpereh\", \n",
    "    model_paths: dict=get_model_paths_and_config(Config.MODEL_NAME)\n",
    "):\n",
    "    WORKING_DIR = 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240322091040/'\n",
    "    VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "    print(Config.MODEL_NAME)\n",
    "    model_paths_and_config = get_model_paths_and_config(Config.MODEL_NAME)\n",
    "\n",
    "    from google_cloud_pipeline_components.types import artifact_types\n",
    "    from google_cloud_pipeline_components.v1.endpoint import (EndpointCreateOp,\n",
    "                                                              ModelDeployOp)\n",
    "    from google_cloud_pipeline_components.v1.model import ModelUploadOp\n",
    "    from kfp.dsl import importer_node\n",
    "\n",
    "\n",
    "    port = 7080\n",
    "    accelerator_count=1\n",
    "    max_model_len=256\n",
    "    dtype=\"bfloat16\"\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        f\"--port={port}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    metadata = {\n",
    "      \"imageUri\": VLLM_DOCKER_URI,\n",
    "      \"command\": [\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "      \"args\": vllm_args,\n",
    "      \"ports\": [\n",
    "        {\n",
    "          \"containerPort\": port\n",
    "        }\n",
    "      ],\n",
    "      \"predictRoute\": \"/generate\",\n",
    "      \"healthRoute\": \"/ping\"\n",
    "    }\n",
    "\n",
    "    # from google_cloud_pipeline_components import ModelUploadOp\n",
    "    model_paths = get_model_paths_and_config(Config.MODEL_NAME)\n",
    "    whatup = process_whatsapp_chat_op(bucket_name = bucket_name, directory = directory)\n",
    "\n",
    "    trainer = fine_tunning(dataset_path=whatup.outputs['dataset_path'], model_paths=model_paths)\n",
    "    trainer.set_memory_limit(\"50G\").set_cpu_limit('12.0m').set_accelerator_limit(1).add_node_selector_constraint(\"NVIDIA_L4\")\n",
    "\n",
    "    print(\"This is the dictionary\", model_paths)\n",
    "    converted = convert_checkpoints_op(\n",
    "        keras_gcs_model=trainer.output, model_paths=model_paths\n",
    "    ).set_memory_limit(\"50G\").set_cpu_limit('8.0m').set_accelerator_limit(1).add_node_selector_constraint(\"NVIDIA_L4\")\n",
    "\n",
    "    import_unmanaged_model_task = importer_node.importer(\n",
    "        artifact_uri=converted.output,\n",
    "        artifact_class=artifact_types.UnmanagedContainerModel,\n",
    "        metadata={\n",
    "            \"containerSpec\": metadata,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    model_upload_op = ModelUploadOp(\n",
    "        project=project,\n",
    "        display_name=\"Mini Andres, first version automated\",\n",
    "        unmanaged_container_model=import_unmanaged_model_task.outputs[\"artifact\"],\n",
    "    )\n",
    "    model_upload_op.after(import_unmanaged_model_task)\n",
    "\n",
    "    endpoint_create_op = EndpointCreateOp(\n",
    "        project=project,\n",
    "        display_name=\"pipelines-created-endpoint\",\n",
    "    )\n",
    "\n",
    "    ModelDeployOp(\n",
    "        endpoint=endpoint_create_op.outputs[\"endpoint\"],\n",
    "        model=model_upload_op.outputs[\"model\"],\n",
    "        deployed_model_display_name=model_paths_and_config['model_name_vllm'],\n",
    "        dedicated_resources_machine_type=model_paths_and_config['machine_type'],\n",
    "        dedicated_resources_min_replica_count=1,\n",
    "        dedicated_resources_max_replica_count=1,\n",
    "        dedicated_resources_accelerator_type=model_paths_and_config['accelerator_type'],\n",
    "        dedicated_resources_accelerator_count=model_paths_and_config['accelerator_count']\n",
    "    )\n",
    "\n",
    "    \n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=model_deployment_pipeline, package_path=\"model_deployment_pipeline.json\"\n",
    ")\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
    "    display_name=\"test-model_deployment_pipeline\",\n",
    "    template_path=\"model_deployment_pipeline.json\"\n",
    ")\n",
    "vertex_pipelines_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691e6c51-5230-4c16-ba9a-df128e4541b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf5ede-36a7-4412-9ec5-8877dfa6f597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m118"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
