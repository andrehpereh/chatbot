{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f178cfef-2782-4a77-b4f8-7d372a7c61f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import keras\n",
    "#import keras_nlp\n",
    "from util import get_model_paths_and_config, upload2bs\n",
    "from config import Config\n",
    "from components.data_preparation.data_ingestion import process_whatsapp_chat\n",
    "from components.fine_tunning.trainer import finetune_gemma\n",
    "from components.fine_tunning.conversion_function import convert_checkpoints\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6da46322-3139-4427-b552-1f444750a378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KAGGLE_USERNAME'] = \"andrehpereh1\"\n",
    "os.environ['KAGGLE_KEY'] = \"5859e39806d9456749dcbac685f04bc9\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d508aa69-1810-4fc2-9a3d-735e55e4ccf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gemma_2b_en\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_size': '2b',\n",
       " 'finetuned_model_dir': './gemma_2b_en',\n",
       " 'finetuned_weights_path': './gemma_2b_en/model.weights.h5',\n",
       " 'finetuned_vocab_path': './gemma_2b_en/vocabulary.spm',\n",
       " 'huggingface_model_dir': './gemma_2b_en_huggingface',\n",
       " 'deployed_model_blob': 'gemma_2b_en/20240321122146',\n",
       " 'deployed_model_uri': 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240321122146',\n",
       " 'fine_tuned_keras_blob': 'gemma_2b_en/keras/20240321122146',\n",
       " 'model_name_vllm': 'gemma_2b_en-vllm',\n",
       " 'machine_type': 'g2-standard-8',\n",
       " 'accelerator_type': 'NVIDIA_L4',\n",
       " 'accelerator_count': 1}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import util\n",
    "importlib.reload(util)\n",
    "model_paths_and_config = util.get_model_paths_and_config(Config.MODEL_NAME)\n",
    "model_paths_and_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69027026-fb4f-4de3-9e5c-84f7ec881d29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('input_data/andrehpereh', 'able-analyst-416817-chatbot-v1')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.TRAIN_DATA_DIR, Config.BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b737395-4d42-4a7e-becf-9fbc07a2580d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket Name able-analyst-416817-chatbot-v1\n",
      "Directory input_data/andrehpereh\n",
      "able-analyst-416817-chatbot-v1\n",
      "input_data/andrehpereh\n",
      "<google.api_core.page_iterator.HTTPIterator object at 0x7f0159c2e500>\n",
      "input_data/andrehpereh/\n",
      "\n",
      "input_data/andrehpereh/WhatsApp Chat with Anki.txt\n",
      "WhatsApp Chat with Anki.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Anki.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Ilse Flatmate.txt\n",
      "WhatsApp Chat with Ilse Flatmate.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Ilse Flatmate.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Michael.txt\n",
      "WhatsApp Chat with Michael.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Michael.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Mike Haarlem.txt\n",
      "WhatsApp Chat with Mike Haarlem.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Mike Haarlem.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Rosa Rosa Rosa.txt\n",
      "WhatsApp Chat with Rosa Rosa Rosa.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Rosa Rosa Rosa.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Ruben Ewald Puijker.txt\n",
      "WhatsApp Chat with Ruben Ewald Puijker.txt\n",
      "input_data/andrehpereh/WhatsApp Chat with Ruben Ewald Puijker.txt\n"
     ]
    }
   ],
   "source": [
    "data = process_whatsapp_chat(Config.BUCKET_NAME, Config.TRAIN_DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bc1897-d11a-4e8c-852c-af78ad7656cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab04ebef-6bad-4c2d-aced-34083491afce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_size': '2b',\n",
       " 'finetuned_model_dir': './gemma_2b_en',\n",
       " 'finetuned_weights_path': './gemma_2b_en/model.weights.h5',\n",
       " 'finetuned_vocab_path': './gemma_2b_en/vocabulary.spm',\n",
       " 'huggingface_model_dir': './gemma_2b_en_huggingface',\n",
       " 'deployed_model_blob': 'gemma_2b_en/20240321122146',\n",
       " 'deployed_model_uri': 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240321122146',\n",
       " 'model_name_vllm': 'gemma_2b_en-vllm',\n",
       " 'machine_type': 'g2-standard-8',\n",
       " 'accelerator_type': 'NVIDIA_L4',\n",
       " 'accelerator_count': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths_and_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67913c94-353d-43f3-8026-827aafdbbdbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Print a few examples of the input data\n",
      "[\"Sender:\\nHola! I also enjoyed our conversations! How are you today? Did you learn new things about ai? I finished the podcast the line, its an end worth listing to And yes, let’s meet again Y no me recuerdo el nombre de bebida que es más o menos como tequila 😅\\n\\nAndres Perez:\\nI am doing good, last night I compensated and slept 10 hours 😅 how about you? I'll give it a shot this weekend and let you know what I think after Not really, I was studying the basics this time, I forgot a few important things 😂 Se llama mezcal, it soo good When will it work for you? How was your poetry class btw?\"]\n",
      "Fine tune Function section has ben called and started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-21 12:23:17.039911: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20758 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Tokenizer (type)                                   </span>┃<span style=\"font-weight: bold\">                                             Vocab # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                   │                                             <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                            Vocab #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                   │                                             \u001b[38;5;34m256,000\u001b[0m │\n",
       "└────────────────────────────────────────────────────┴─────────────────────────────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">524,288,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)        │   \u001b[38;5;34m2,506,172,416\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
       "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
       "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
       "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m524,288,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
       "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,506,172,416</span> (9.34 GB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,506,172,416\u001b[0m (9.34 GB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model is not fine tuned due to google cloud lacking support\n",
      "Saving the weights  ./gemma_2b_en/model.weights.h5\n",
      "Saving model in  ./gemma_2b_en\n"
     ]
    }
   ],
   "source": [
    "finetuned_weights_path = finetune_gemma(data=data[:50], model_paths=model_paths_and_config, fine_tune_flag=False, model_name=Config.MODEL_NAME, rank_lora=Config.SEQUENCE_LENGTH, sequence_length=Config.SEQUENCE_LENGTH, epochs=Config.EPOCHS, batch_size=Config.BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b6d2d38-4f7b-4eca-bf72-31027d060ac6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "device = cuda.get_current_device()\n",
    "cuda.select_device(device.id)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "762d8ec4-0a5c-4718-a599-34b74305715c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5\n",
      "gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en\n"
     ]
    }
   ],
   "source": [
    "type(finetuned_weights_path)\n",
    "print(model_paths_and_config['finetuned_weights_path'])\n",
    "print(model_paths_and_config['finetuned_model_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b8d15a7-6344-4faf-8a22-69f203a74676",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5...\n",
      "\\ [1 files][  9.3 GiB/  9.3 GiB]   74.8 MiB/s                                   \n",
      "Operation completed over 1 objects/9.3 GiB.                                      \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5 ./test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6fca7dfb-6fde-4a9f-b879-9be138a35564",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Unable to synchronously create file (unable to open file: name = 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/test/model.weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mfinetuned_weights_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/test/model.weights.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m finetuned_weights_path\u001b[38;5;241m.\u001b[39mpreprocessor\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39msave_assets(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/test\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:562\u001b[0m, in \u001b[0;36mFile.__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, fs_strategy, fs_persist, fs_threshold, fs_page_size, page_buf_size, min_meta_keep, min_raw_keep, locking, alignment_threshold, alignment_interval, meta_block_size, **kwds)\u001b[0m\n\u001b[1;32m    553\u001b[0m     fapl \u001b[38;5;241m=\u001b[39m make_fapl(driver, libver, rdcc_nslots, rdcc_nbytes, rdcc_w0,\n\u001b[1;32m    554\u001b[0m                      locking, page_buf_size, min_meta_keep, min_raw_keep,\n\u001b[1;32m    555\u001b[0m                      alignment_threshold\u001b[38;5;241m=\u001b[39malignment_threshold,\n\u001b[1;32m    556\u001b[0m                      alignment_interval\u001b[38;5;241m=\u001b[39malignment_interval,\n\u001b[1;32m    557\u001b[0m                      meta_block_size\u001b[38;5;241m=\u001b[39mmeta_block_size,\n\u001b[1;32m    558\u001b[0m                      \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    559\u001b[0m     fcpl \u001b[38;5;241m=\u001b[39m make_fcpl(track_order\u001b[38;5;241m=\u001b[39mtrack_order, fs_strategy\u001b[38;5;241m=\u001b[39mfs_strategy,\n\u001b[1;32m    560\u001b[0m                      fs_persist\u001b[38;5;241m=\u001b[39mfs_persist, fs_threshold\u001b[38;5;241m=\u001b[39mfs_threshold,\n\u001b[1;32m    561\u001b[0m                      fs_page_size\u001b[38;5;241m=\u001b[39mfs_page_size)\n\u001b[0;32m--> 562\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mmake_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muserblock_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mswmr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mswmr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(libver, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_libver \u001b[38;5;241m=\u001b[39m libver\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/h5py/_hl/files.py:241\u001b[0m, in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    239\u001b[0m     fid \u001b[38;5;241m=\u001b[39m h5f\u001b[38;5;241m.\u001b[39mcreate(name, h5f\u001b[38;5;241m.\u001b[39mACC_EXCL, fapl\u001b[38;5;241m=\u001b[39mfapl, fcpl\u001b[38;5;241m=\u001b[39mfcpl)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 241\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh5f\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mACC_TRUNC\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfapl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfapl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfcpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfcpl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# Open in append mode (read/write).\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# If that fails, create a new file only if it won't clobber an\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# existing one (ACC_EXCL)\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5f.pyx:122\u001b[0m, in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Unable to synchronously create file (unable to open file: name = 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/test/model.weights.h5', errno = 2, error message = 'No such file or directory', flags = 13, o_flags = 242)"
     ]
    }
   ],
   "source": [
    "finetuned_weights_path.save_weights(\"gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/test/model.weights.h5\")\n",
    "finetuned_weights_path.preprocessor.tokenizer.save_assets(\"gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "511c2ed7-d7fc-4c1b-a73f-e44940701a88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-20 23:26:11.267470: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 23:26:11.333412: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-20 23:26:26.527151: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-20 23:26:26.528887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9838 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-> Loading Keras weights from file `./gemma_2b_en/model.weights.h5`...\n",
      "\n",
      "-> Loading HuggingFace Gemma `2B` model...\n",
      "\n",
      "✅ Model loading complete.\n",
      "\n",
      "-> Converting weights from KerasNLP Gemma to HuggingFace Gemma...\n",
      "\n",
      "✅ Weights converted successfully.\n",
      "\n",
      "-> Saving HuggingFace model to `./gemma_2b_en_huggingface`...\n",
      "\n",
      "✅ Saving complete. Model saved at `./gemma_2b_en_huggingface`.\n",
      "\n",
      "-> Saving HuggingFace Gemma tokenizer to `./gemma_2b_en_huggingface`...\n",
      "\n",
      "✅ Saving complete. Tokenizer saved at `./gemma_2b_en_huggingface`.\n"
     ]
    }
   ],
   "source": [
    "output_dir = convert_checkpoints(\n",
    "    weights_file=model_paths_and_config['finetuned_weights_path'],\n",
    "    size=model_paths_and_config['model_size'],\n",
    "    output_dir=model_paths_and_config['huggingface_model_dir'],\n",
    "    vocab_path=model_paths_and_config['finetuned_vocab_path'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202aae46-9fdc-40f6-9d3e-d4f7cb50278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir=model_paths_and_config['huggingface_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4221ec4a-8da6-463b-943d-c4d66b9615df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('able-analyst-416817-chatbot-v1', 'gemma_2b_en_raw/gemma_2b_en')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths_and_config, Config.BUCKET_NAME\n",
    "bucket_name, blob_name = os.path.dirname(\"gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5\").lstrip(\"gs://\").split(\"/\", 1) \n",
    "bucket_name, blob_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ea9b7e0c-e4bd-4ed8-95a8-16eac527e81d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model.weights.h5'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(\"gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad8c09-c59c-4424-8165-24e5716740e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_path = upload2bs(local_directory = output_dir, bucket_name = Config.BUCKET_NAME, destination_subfolder = model_paths_and_config['deployed_model_blob'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98ac8551-f497-4b79-b6bd-41b5034c3011",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This are the blobs <google.api_core.page_iterator.HTTPIterator object at 0x7f9721e6b250>\n",
      "gemma_2b_en_raw/gemma_2b_en/model.weights.h5\n",
      "This is the file name odel.weights.h5\n",
      "Downloaded gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/model.weights.h5 to ./model.weights.h5\n",
      "gemma_2b_en_raw/gemma_2b_en/vocabulary.spm\n",
      "This is the file name vocabulary.spm\n",
      "Downloaded gs://able-analyst-416817-chatbot-v1/gemma_2b_en_raw/gemma_2b_en/vocabulary.spm to ./vocabulary.spm\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import util\n",
    "importlib.reload(util)\n",
    "\n",
    "util.download_all_from_blob(Config.BUCKET_NAME, \"gemma_2b_en_raw/gemma_2b_en\", './')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16877b-31ae-44c2-8b07-d90e5ad9e849",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Config.BUCKET_NAME\n",
    "model_paths_and_config['huggingface_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "86b9a6a6-9cf8-4fb4-a9e2-69f867cc31c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma_2b_en/20240321122146'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths_and_config['deployed_model_blob']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acea5a28-8c3c-44d7-87aa-1952de6c3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=Config.PROJECT_ID, location=Config.REGION, staging_bucket=Config.BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b32639-f0ac-459a-924b-db590770a837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240220_0936_RC01\"\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    suffix = datetime.datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "    return f\"{prefix}{suffix}\"\n",
    "\n",
    "\n",
    "def deploy_model_vllm(\n",
    "    model_name: str,\n",
    "    model_uri: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    max_model_len: int = 8192,\n",
    "    dtype: str = \"bfloat16\",\n",
    ") -> tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    # Upload the model to \"Model Registry\"\n",
    "    job_name = get_job_name_with_datetime(model_name)\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        \"--gpu-memory-utilization=0.95\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        f\"--dtype={dtype}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=job_name,\n",
    "        artifact_uri=model_uri,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "    )\n",
    "\n",
    "    # Deploy the model to an endpoint to serve \"Online predictions\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c07dfde0-7192-48af-9730-d3b692cf878a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma_2b_en-vllm'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up artificially since the model was already in a bucket\n",
    "model_paths_and_config['deployed_model_uri'] = 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240314162107'\n",
    "model_paths_and_config['model_name_vllm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "251fc433-3657-434e-a64c-6b351bf143ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/24796876098/locations/us-central1/models/6563293868962349056/operations/1798728489633841152\n",
      "Model created. Resource name: projects/24796876098/locations/us-central1/models/6563293868962349056@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/24796876098/locations/us-central1/models/6563293868962349056@1')\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/24796876098/locations/us-central1/endpoints/2459943961893011456/operations/8683465682488131584\n",
      "Endpoint created. Resource name: projects/24796876098/locations/us-central1/endpoints/2459943961893011456\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/24796876098/locations/us-central1/endpoints/2459943961893011456')\n",
      "Deploying model to Endpoint : projects/24796876098/locations/us-central1/endpoints/2459943961893011456\n",
      "Deploy Endpoint model backing LRO: projects/24796876098/locations/us-central1/endpoints/2459943961893011456/operations/7799071305663250432\n",
      "Endpoint model deployed. Resource name: projects/24796876098/locations/us-central1/endpoints/2459943961893011456\n"
     ]
    }
   ],
   "source": [
    "max_model_len = 2048\n",
    "\n",
    "model, endpoint = deploy_model_vllm(\n",
    "    model_name=model_paths_and_config['model_name_vllm'],\n",
    "    model_uri=model_paths_and_config['deployed_model_uri'],\n",
    "    service_account=Config.SERVICE_ACCOUNT,\n",
    "    machine_type=model_paths_and_config['machine_type'],\n",
    "    accelerator_type=model_paths_and_config['accelerator_type'],\n",
    "    accelerator_count=model_paths_and_config['accelerator_count'],\n",
    "    max_model_len=max_model_len,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69dc40ff-d622-423b-8c43-f066e93907e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88520f0-174a-4cb7-97c4-41a1fbcaca81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b0e2a926-dac3-4dfd-a184-515685b4b657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f6f329e-9925-42fe-8362-3ca5c97f1cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the plan for tonight?\n",
      "I don’t know, I’m not sure. I’ll let you know when I know.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "What would you like to drink?\n",
      "I’ll take a coffee, please.\n",
      "\n",
      "Andres Perez:\n",
      "I’ll take a coffee, please.\n",
      "\n",
      "Andres Perez:\n",
      "I’ll take a coffee, please.\n",
      "\n",
      "Andres Perez:\n",
      "I’ll take a coffee, please.\n",
      "\n",
      "Andres Perez:\n",
      "\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "Are you coming tonight?\n",
      "I’m not sure. I’ll ask my mom.\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    }
   ],
   "source": [
    "TEST_EXAMPLES = [\n",
    "    \"What is the plan for tonight?\",\n",
    "    \"What would you like to drink?\",\n",
    "    \"Are you coming tonight?\"\n",
    "]\n",
    "\n",
    "# Prompt template for the training data and the finetuning tests\n",
    "PROMPT_TEMPLATE = \"Sender:\\n{instruction}\\n\\nAndres Perez:\\n{response}\"\n",
    "\n",
    "TEST_PROMPTS = [\n",
    "    PROMPT_TEMPLATE.format(instruction=example, response=\"\")\n",
    "    for example in TEST_EXAMPLES\n",
    "]\n",
    "\n",
    "def test_vertexai_endpoint(endpoint: aiplatform.Endpoint):\n",
    "    for question, prompt in zip(TEST_EXAMPLES, TEST_PROMPTS):\n",
    "        instance = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": 56,\n",
    "            \"temperature\": 0.0,\n",
    "            \"top_p\": 1.0,\n",
    "            \"top_k\": 1,\n",
    "            \"raw_response\": True,\n",
    "        }\n",
    "        response = endpoint.predict(instances=[instance])\n",
    "        output = response.predictions[0]\n",
    "        print(f\"{question}\\n{output}\\n{'- '*40}\")\n",
    "\n",
    "\n",
    "test_vertexai_endpoint(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "518bd694-aa5b-45f4-934a-894c33b2bf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "SERVICE_ACCOUNT = 'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'\n",
    "from datetime import datetime\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "GCP_REGION='us-central1'\n",
    "IMAGE_NAME=\"gemma-chatbot\"\n",
    "TAG_NAME = 'latest'\n",
    "KAGGLE_USERNAME='andrehpereh1'\n",
    "KAGGLE_KEY='5859e39806d9456749dcbac685f04bc9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d98f42-6a85-4324-81a7-dd1c6b5bab4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-data-preparation,TAG_NAME=latest\n"
     ]
    }
   ],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-data-preparation\",\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dfd38dea-1f7f-486d-bf3e-1b3a1f69582c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Runs the data_preparation component image. (Development, when tested should be moved to the main cloudbuild in the project folder)\n",
    "# Pay attention to the \".\" after summit. Might need some changes when move to the master pipeline.\n",
    "!gcloud builds submit . --timeout=15m --config \"components/data_preparation/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "# DO not forget the tag\n",
    "#!docker run gcr.io/able-analyst-416817/gemma-chatbot-data-preparation:latest data_ingestion.py --bucket-name 'able-analyst-416817-chatbot-v1' --directory 'input_data/andrehpereh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a0d0f14-b406-4a5a-bc6b-fa4e8da5cc62",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9250f9bf-5bf0-4bd2-8f5f-3f10daf9dc8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-fine-tunning,_KAGGLE_USERNAME=andrehpereh1,_KAGGLE_KEY=5859e39806d9456749dcbac685f04bc9,TAG_NAME=latest\n",
      "Creating temporary tarball archive of 85 file(s) totalling 1.8 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://able-analyst-416817_cloudbuild/source/1711059994.229577-44e2712079f546a1bacb1abef8eb53c3.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/able-analyst-416817/locations/us-central1/builds/755a0621-803c-4629-a651-35cb98e212fe].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/755a0621-803c-4629-a651-35cb98e212fe?project=24796876098 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"755a0621-803c-4629-a651-35cb98e212fe\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://able-analyst-416817_cloudbuild/source/1711059994.229577-44e2712079f546a1bacb1abef8eb53c3.tgz#1711059994928785\n",
      "Copying gs://able-analyst-416817_cloudbuild/source/1711059994.229577-44e2712079f546a1bacb1abef8eb53c3.tgz#1711059994928785...\n",
      "/ [1 files][706.6 KiB/706.6 KiB]                                                \n",
      "Operation completed over 1 objects/706.6 KiB.\n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  72.19kB\n",
      "Step #0: Step 1/12 : FROM us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121.py310\n",
      "Step #0: latest: Pulling from deeplearning-platform-release/gcr.io/base-cu121.py310\n",
      "Step #0: 96d54c3075c9: Already exists\n",
      "Step #0: 755e535b54a3: Pulling fs layer\n",
      "Step #0: 24ff69e0a1e4: Pulling fs layer\n",
      "Step #0: 76a627ca5e65: Pulling fs layer\n",
      "Step #0: 35817692a87e: Pulling fs layer\n",
      "Step #0: 84b6a42e847a: Pulling fs layer\n",
      "Step #0: 4639b7cd68e5: Pulling fs layer\n",
      "Step #0: a7bc10701a5b: Pulling fs layer\n",
      "Step #0: 7d32a9230f8f: Pulling fs layer\n",
      "Step #0: 8c35a1861813: Pulling fs layer\n",
      "Step #0: e3f63d7242f5: Pulling fs layer\n",
      "Step #0: afa75cca3a04: Pulling fs layer\n",
      "Step #0: c1f9e802b8d1: Pulling fs layer\n",
      "Step #0: c6b53aacfed4: Pulling fs layer\n",
      "Step #0: 4f4fb700ef54: Pulling fs layer\n",
      "Step #0: 10fcf6d9a8b1: Pulling fs layer\n",
      "Step #0: bd5512e53c5d: Pulling fs layer\n",
      "Step #0: 50d7191d66bf: Pulling fs layer\n",
      "Step #0: 180a65867fa7: Pulling fs layer\n",
      "Step #0: 7a1daf630b7f: Pulling fs layer\n",
      "Step #0: 20208c43ea0e: Pulling fs layer\n",
      "Step #0: ea960cf75e3a: Pulling fs layer\n",
      "Step #0: dfc4f5a5a8e5: Pulling fs layer\n",
      "Step #0: 3ebdd0bf63e9: Pulling fs layer\n",
      "Step #0: b19c2d4b7a08: Pulling fs layer\n",
      "Step #0: f9c0f83d4666: Pulling fs layer\n",
      "Step #0: d8cb5f186edc: Pulling fs layer\n",
      "Step #0: 7195b2f69d30: Pulling fs layer\n",
      "Step #0: 84f4ed49af3b: Pulling fs layer\n",
      "Step #0: 754279d31ba2: Pulling fs layer\n",
      "Step #0: e5fd8154ec1e: Pulling fs layer\n",
      "Step #0: 064dd4dfd950: Pulling fs layer\n",
      "Step #0: 7ce4912a0f9a: Pulling fs layer\n",
      "Step #0: 2873af0a7328: Pulling fs layer\n",
      "Step #0: 06676754b8b3: Pulling fs layer\n",
      "Step #0: d625375a5d42: Pulling fs layer\n",
      "Step #0: e7c45501f24b: Pulling fs layer\n",
      "Step #0: b4614282a05d: Pulling fs layer\n",
      "Step #0: 35817692a87e: Waiting\n",
      "Step #0: 84b6a42e847a: Waiting\n",
      "Step #0: 4639b7cd68e5: Waiting\n",
      "Step #0: a7bc10701a5b: Waiting\n",
      "Step #0: 7d32a9230f8f: Waiting\n",
      "Step #0: 8c35a1861813: Waiting\n",
      "Step #0: e3f63d7242f5: Waiting\n",
      "Step #0: afa75cca3a04: Waiting\n",
      "Step #0: c1f9e802b8d1: Waiting\n",
      "Step #0: c6b53aacfed4: Waiting\n",
      "Step #0: 4f4fb700ef54: Waiting\n",
      "Step #0: 10fcf6d9a8b1: Waiting\n",
      "Step #0: bd5512e53c5d: Waiting\n",
      "Step #0: 50d7191d66bf: Waiting\n",
      "Step #0: 180a65867fa7: Waiting\n",
      "Step #0: 7a1daf630b7f: Waiting\n",
      "Step #0: 20208c43ea0e: Waiting\n",
      "Step #0: ea960cf75e3a: Waiting\n",
      "Step #0: dfc4f5a5a8e5: Waiting\n",
      "Step #0: 3ebdd0bf63e9: Waiting\n",
      "Step #0: b19c2d4b7a08: Waiting\n",
      "Step #0: f9c0f83d4666: Waiting\n",
      "Step #0: d8cb5f186edc: Waiting\n",
      "Step #0: 7195b2f69d30: Waiting\n",
      "Step #0: 84f4ed49af3b: Waiting\n",
      "Step #0: 754279d31ba2: Waiting\n",
      "Step #0: e5fd8154ec1e: Waiting\n",
      "Step #0: 064dd4dfd950: Waiting\n",
      "Step #0: 7ce4912a0f9a: Waiting\n",
      "Step #0: 2873af0a7328: Waiting\n",
      "Step #0: 06676754b8b3: Waiting\n",
      "Step #0: d625375a5d42: Waiting\n",
      "Step #0: e7c45501f24b: Waiting\n",
      "Step #0: b4614282a05d: Waiting\n",
      "Step #0: 76a627ca5e65: Download complete\n",
      "Step #0: 755e535b54a3: Verifying Checksum\n",
      "Step #0: 755e535b54a3: Download complete\n",
      "Step #0: 35817692a87e: Verifying Checksum\n",
      "Step #0: 35817692a87e: Download complete\n",
      "Step #0: 4639b7cd68e5: Verifying Checksum\n",
      "Step #0: 4639b7cd68e5: Download complete\n",
      "Step #0: a7bc10701a5b: Verifying Checksum\n",
      "Step #0: a7bc10701a5b: Download complete\n",
      "Step #0: 24ff69e0a1e4: Verifying Checksum\n",
      "Step #0: 24ff69e0a1e4: Download complete\n",
      "Step #0: 7d32a9230f8f: Verifying Checksum\n",
      "Step #0: 7d32a9230f8f: Download complete\n",
      "Step #0: e3f63d7242f5: Verifying Checksum\n",
      "Step #0: e3f63d7242f5: Download complete\n",
      "Step #0: 755e535b54a3: Pull complete\n",
      "Step #0: 84b6a42e847a: Download complete\n",
      "Step #0: afa75cca3a04: Verifying Checksum\n",
      "Step #0: afa75cca3a04: Download complete\n",
      "Step #0: c6b53aacfed4: Verifying Checksum\n",
      "Step #0: c6b53aacfed4: Download complete\n",
      "Step #0: 4f4fb700ef54: Verifying Checksum\n",
      "Step #0: 4f4fb700ef54: Download complete\n",
      "Step #0: 24ff69e0a1e4: Pull complete\n",
      "Step #0: 76a627ca5e65: Pull complete\n",
      "Step #0: 10fcf6d9a8b1: Download complete\n",
      "Step #0: 35817692a87e: Pull complete\n",
      "Step #0: bd5512e53c5d: Download complete\n",
      "Step #0: 50d7191d66bf: Verifying Checksum\n",
      "Step #0: 50d7191d66bf: Download complete\n",
      "Step #0: 180a65867fa7: Download complete\n",
      "Step #0: 7a1daf630b7f: Verifying Checksum\n",
      "Step #0: 7a1daf630b7f: Download complete\n",
      "Step #0: 20208c43ea0e: Verifying Checksum\n",
      "Step #0: 20208c43ea0e: Download complete\n",
      "Step #0: ea960cf75e3a: Verifying Checksum\n",
      "Step #0: ea960cf75e3a: Download complete\n",
      "Step #0: dfc4f5a5a8e5: Verifying Checksum\n",
      "Step #0: dfc4f5a5a8e5: Download complete\n",
      "Step #0: 8c35a1861813: Verifying Checksum\n",
      "Step #0: 8c35a1861813: Download complete\n",
      "Step #0: b19c2d4b7a08: Verifying Checksum\n",
      "Step #0: b19c2d4b7a08: Download complete\n",
      "Step #0: f9c0f83d4666: Verifying Checksum\n",
      "Step #0: f9c0f83d4666: Download complete\n",
      "Step #0: 3ebdd0bf63e9: Verifying Checksum\n",
      "Step #0: 3ebdd0bf63e9: Download complete\n",
      "Step #0: d8cb5f186edc: Verifying Checksum\n",
      "Step #0: d8cb5f186edc: Download complete\n",
      "Step #0: 7195b2f69d30: Verifying Checksum\n",
      "Step #0: 7195b2f69d30: Download complete\n",
      "Step #0: 84f4ed49af3b: Verifying Checksum\n",
      "Step #0: 84f4ed49af3b: Download complete\n",
      "Step #0: 754279d31ba2: Verifying Checksum\n",
      "Step #0: 754279d31ba2: Download complete\n",
      "Step #0: e5fd8154ec1e: Verifying Checksum\n",
      "Step #0: e5fd8154ec1e: Download complete\n",
      "Step #0: 064dd4dfd950: Verifying Checksum\n",
      "Step #0: 064dd4dfd950: Download complete\n",
      "Step #0: 7ce4912a0f9a: Verifying Checksum\n",
      "Step #0: 7ce4912a0f9a: Download complete\n",
      "Step #0: 2873af0a7328: Verifying Checksum\n",
      "Step #0: 2873af0a7328: Download complete\n",
      "Step #0: d625375a5d42: Verifying Checksum\n",
      "Step #0: d625375a5d42: Download complete\n",
      "Step #0: e7c45501f24b: Download complete\n",
      "Step #0: b4614282a05d: Verifying Checksum\n",
      "Step #0: b4614282a05d: Download complete\n",
      "Step #0: c1f9e802b8d1: Verifying Checksum\n",
      "Step #0: c1f9e802b8d1: Download complete\n",
      "Step #0: 06676754b8b3: Verifying Checksum\n",
      "Step #0: 06676754b8b3: Download complete\n",
      "Step #0: 84b6a42e847a: Pull complete\n",
      "Step #0: 4639b7cd68e5: Pull complete\n",
      "Step #0: a7bc10701a5b: Pull complete\n",
      "Step #0: 7d32a9230f8f: Pull complete\n",
      "Step #0: 8c35a1861813: Pull complete\n",
      "Step #0: e3f63d7242f5: Pull complete\n",
      "Step #0: afa75cca3a04: Pull complete\n",
      "Step #0: c1f9e802b8d1: Pull complete\n",
      "Step #0: c6b53aacfed4: Pull complete\n",
      "Step #0: 4f4fb700ef54: Pull complete\n",
      "Step #0: 10fcf6d9a8b1: Pull complete\n",
      "Step #0: bd5512e53c5d: Pull complete\n",
      "Step #0: 50d7191d66bf: Pull complete\n",
      "Step #0: 180a65867fa7: Pull complete\n",
      "Step #0: 7a1daf630b7f: Pull complete\n",
      "Step #0: 20208c43ea0e: Pull complete\n",
      "Step #0: ea960cf75e3a: Pull complete\n",
      "Step #0: dfc4f5a5a8e5: Pull complete\n",
      "Step #0: 3ebdd0bf63e9: Pull complete\n",
      "Step #0: b19c2d4b7a08: Pull complete\n",
      "Step #0: f9c0f83d4666: Pull complete\n",
      "Step #0: d8cb5f186edc: Pull complete\n",
      "Step #0: 7195b2f69d30: Pull complete\n",
      "Step #0: 84f4ed49af3b: Pull complete\n",
      "Step #0: 754279d31ba2: Pull complete\n",
      "Step #0: e5fd8154ec1e: Pull complete\n",
      "Step #0: 064dd4dfd950: Pull complete\n",
      "Step #0: 7ce4912a0f9a: Pull complete\n",
      "Step #0: 2873af0a7328: Pull complete\n",
      "Step #0: 06676754b8b3: Pull complete\n",
      "Step #0: d625375a5d42: Pull complete\n",
      "Step #0: e7c45501f24b: Pull complete\n",
      "Step #0: b4614282a05d: Pull complete\n",
      "Step #0: Digest: sha256:a0d3c16c924fdda8134fb4a29a3f491208189d99590a04643abb34e72108752a\n",
      "Step #0: Status: Downloaded newer image for us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu121.py310:latest\n",
      "Step #0:  ---> f04ebe26fc76\n",
      "Step #0: Step 2/12 : WORKDIR /trainer\n",
      "Step #0:  ---> Running in b08949d20c95\n",
      "Step #0: Removing intermediate container b08949d20c95\n",
      "Step #0:  ---> 87f56408fdd6\n",
      "Step #0: Step 3/12 : COPY requirements.txt .\n",
      "Step #0:  ---> 104faaf6393b\n",
      "Step #0: Step 4/12 : RUN pip install -U -r requirements.txt\n",
      "Step #0:  ---> Running in 9a40cc26bbd9\n",
      "Step #0: Collecting keras-nlp==0.8.2 (from -r requirements.txt (line 3))\n",
      "Step #0:   Downloading keras_nlp-0.8.2-py3-none-any.whl.metadata (7.0 kB)\n",
      "Step #0: Collecting keras==3.0.5 (from -r requirements.txt (line 4))\n",
      "Step #0:   Downloading keras-3.0.5-py3-none-any.whl.metadata (4.8 kB)\n",
      "Step #0: Collecting accelerate (from -r requirements.txt (line 5))\n",
      "Step #0:   Downloading accelerate-0.28.0-py3-none-any.whl.metadata (18 kB)\n",
      "Step #0: Collecting sentencepiece (from -r requirements.txt (line 6))\n",
      "Step #0:   Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Step #0: Collecting transformers==4.38 (from -r requirements.txt (line 7))\n",
      "Step #0:   Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 131.1/131.1 kB 1.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting fire==0.6.0 (from -r requirements.txt (line 8))\n",
      "Step #0:   Downloading fire-0.6.0.tar.gz (88 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 88.4/88.4 kB 5.4 MB/s eta 0:00:00\n",
      "Step #0:   Preparing metadata (setup.py): started\n",
      "Step #0:   Preparing metadata (setup.py): finished with status 'done'\n",
      "Step #0: Requirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.14.0)\n",
      "Step #0: Collecting google-cloud-storage (from -r requirements.txt (line 9))\n",
      "Step #0:   Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl.metadata (6.1 kB)\n",
      "Step #0: Collecting keras-core (from keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading keras_core-0.1.7-py3-none-any.whl.metadata (4.3 kB)\n",
      "Step #0: Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.8.2->-r requirements.txt (line 3)) (2.1.0)\n",
      "Step #0: Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.8.2->-r requirements.txt (line 3)) (1.25.2)\n",
      "Step #0: Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.8.2->-r requirements.txt (line 3)) (23.2)\n",
      "Step #0: Collecting regex (from keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.9/40.9 kB 3.4 MB/s eta 0:00:00\n",
      "Step #0: Requirement already satisfied: rich in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.8.2->-r requirements.txt (line 3)) (13.7.1)\n",
      "Step #0: Requirement already satisfied: dm-tree in /opt/conda/lib/python3.10/site-packages (from keras-nlp==0.8.2->-r requirements.txt (line 3)) (0.1.8)\n",
      "Step #0: Collecting kagglehub (from keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading kagglehub-0.2.1-py3-none-any.whl.metadata (18 kB)\n",
      "Step #0: Collecting tensorflow-text (from keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Step #0: Collecting namex (from keras==3.0.5->-r requirements.txt (line 4))\n",
      "Step #0:   Downloading namex-0.0.7-py3-none-any.whl.metadata (246 bytes)\n",
      "Step #0: Collecting h5py (from keras==3.0.5->-r requirements.txt (line 4))\n",
      "Step #0:   Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Step #0: Collecting ml-dtypes (from keras==3.0.5->-r requirements.txt (line 4))\n",
      "Step #0:   Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Step #0: Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers==4.38->-r requirements.txt (line 7)) (3.13.1)\n",
      "Step #0: Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38->-r requirements.txt (line 7))\n",
      "Step #0:   Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Step #0: Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38->-r requirements.txt (line 7)) (6.0.1)\n",
      "Step #0: Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers==4.38->-r requirements.txt (line 7)) (2.31.0)\n",
      "Step #0: Collecting tokenizers<0.19,>=0.14 (from transformers==4.38->-r requirements.txt (line 7))\n",
      "Step #0:   Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Step #0: Collecting safetensors>=0.4.1 (from transformers==4.38->-r requirements.txt (line 7))\n",
      "Step #0:   Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Step #0: Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers==4.38->-r requirements.txt (line 7)) (4.66.2)\n",
      "Step #0: Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from fire==0.6.0->-r requirements.txt (line 8)) (1.16.0)\n",
      "Step #0: Collecting termcolor (from fire==0.6.0->-r requirements.txt (line 8))\n",
      "Step #0:   Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Step #0: Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->-r requirements.txt (line 5)) (5.9.3)\n",
      "Step #0: Collecting torch>=1.10.0 (from accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Step #0: Requirement already satisfied: google-auth<3.0dev,>=2.26.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->-r requirements.txt (line 9)) (2.28.1)\n",
      "Step #0: Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage->-r requirements.txt (line 9))\n",
      "Step #0:   Downloading google_api_core-2.18.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Step #0: Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->-r requirements.txt (line 9)) (2.4.1)\n",
      "Step #0: Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->-r requirements.txt (line 9)) (2.7.0)\n",
      "Step #0: Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage->-r requirements.txt (line 9)) (1.5.0)\n",
      "Step #0: Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->-r requirements.txt (line 9)) (1.62.0)\n",
      "Step #0: Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0.dev0,>=3.19.5 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->-r requirements.txt (line 9)) (3.20.3)\n",
      "Step #0: Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage->-r requirements.txt (line 9)) (1.23.0)\n",
      "Step #0: Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r requirements.txt (line 9)) (5.3.3)\n",
      "Step #0: Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r requirements.txt (line 9)) (0.3.0)\n",
      "Step #0: Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r requirements.txt (line 9)) (4.9)\n",
      "Step #0: Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38->-r requirements.txt (line 7)) (2024.2.0)\n",
      "Step #0: Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38->-r requirements.txt (line 7)) (4.10.0)\n",
      "Step #0: Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38->-r requirements.txt (line 7)) (3.3.2)\n",
      "Step #0: Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38->-r requirements.txt (line 7)) (3.6)\n",
      "Step #0: Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38->-r requirements.txt (line 7)) (1.26.18)\n",
      "Step #0: Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers==4.38->-r requirements.txt (line 7)) (2024.2.2)\n",
      "Step #0: Collecting sympy (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
      "Step #0: Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 5)) (3.2.1)\n",
      "Step #0: Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate->-r requirements.txt (line 5)) (3.1.3)\n",
      "Step #0: Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Step #0: Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Step #0: Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Step #0: Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Step #0: Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Step #0: Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Step #0: Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Step #0: Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Step #0: Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Step #0: Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
      "Step #0: Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Step #0: Collecting triton==2.2.0 (from torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Step #0: Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Step #0: Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp==0.8.2->-r requirements.txt (line 3)) (3.0.0)\n",
      "Step #0: Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich->keras-nlp==0.8.2->-r requirements.txt (line 3)) (2.17.2)\n",
      "Step #0: Collecting tensorflow<2.17,>=2.16.1 (from tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Step #0: Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras-nlp==0.8.2->-r requirements.txt (line 3)) (0.1.2)\n",
      "Step #0: Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage->-r requirements.txt (line 9)) (0.5.1)\n",
      "Step #0: Collecting astunparse>=1.6.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Step #0: Collecting flatbuffers>=23.5.26 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading flatbuffers-24.3.7-py2.py3-none-any.whl.metadata (849 bytes)\n",
      "Step #0: Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Step #0: Collecting google-pasta>=0.1.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Step #0: Collecting libclang>=13.0.0 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Step #0: Collecting opt-einsum>=2.3.2 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Step #0: Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3)) (69.1.1)\n",
      "Step #0: Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3)) (1.16.0)\n",
      "Step #0: Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3)) (1.62.0)\n",
      "Step #0: Collecting tensorboard<2.17,>=2.16 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Step #0: Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Step #0: Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate->-r requirements.txt (line 5)) (2.1.5)\n",
      "Step #0: Collecting mpmath>=0.19 (from sympy->torch>=1.10.0->accelerate->-r requirements.txt (line 5))\n",
      "Step #0:   Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Step #0: Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3)) (0.42.0)\n",
      "Step #0: Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Step #0: Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Step #0: Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow<2.17,>=2.16.1->tensorflow-text->keras-nlp==0.8.2->-r requirements.txt (line 3))\n",
      "Step #0:   Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Step #0: Downloading keras_nlp-0.8.2-py3-none-any.whl (465 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 465.3/465.3 kB 8.2 MB/s eta 0:00:00\n",
      "Step #0: Downloading keras-3.0.5-py3-none-any.whl (1.0 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.0/1.0 MB 21.1 MB/s eta 0:00:00\n",
      "Step #0: Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 64.3 MB/s eta 0:00:00\n",
      "Step #0: Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 290.1/290.1 kB 27.5 MB/s eta 0:00:00\n",
      "Step #0: Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 75.1 MB/s eta 0:00:00\n",
      "Step #0: Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.6/125.6 kB 12.2 MB/s eta 0:00:00\n",
      "Step #0: Downloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.3/138.3 kB 10.9 MB/s eta 0:00:00\n",
      "Step #0: Downloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 346.4/346.4 kB 28.3 MB/s eta 0:00:00\n",
      "Step #0: Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 774.0/774.0 kB 52.2 MB/s eta 0:00:00\n",
      "Step #0: Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 71.5 MB/s eta 0:00:00\n",
      "Step #0: Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 69.8 MB/s eta 0:00:00\n",
      "Step #0: Downloading torch-2.2.1-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.5/755.5 MB 2.4 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 4.1 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 57.1 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 50.7 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 50.3 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 2.4 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 11.2 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 14.5 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 10.6 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 4.9 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 7.8 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 9.3 MB/s eta 0:00:00\n",
      "Step #0: Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 MB 3.5 MB/s eta 0:00:00\n",
      "Step #0: Downloading h5py-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.8/4.8 MB 4.4 MB/s eta 0:00:00\n",
      "Step #0: Downloading kagglehub-0.2.1-py3-none-any.whl (32 kB)\n",
      "Step #0: Downloading keras_core-0.1.7-py3-none-any.whl (950 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 950.8/950.8 kB 4.3 MB/s eta 0:00:00\n",
      "Step #0: Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.2/2.2 MB 3.8 MB/s eta 0:00:00\n",
      "Step #0: Downloading namex-0.0.7-py3-none-any.whl (5.8 kB)\n",
      "Step #0: Downloading tensorflow_text-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.2/5.2 MB 5.0 MB/s eta 0:00:00\n",
      "Step #0: Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Step #0: Downloading tensorflow-2.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (589.8 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 589.8/589.8 MB 1.8 MB/s eta 0:00:00\n",
      "Step #0: Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 2.8 MB/s eta 0:00:00\n",
      "Step #0: Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Step #0: Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Step #0: Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Step #0: Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.5/57.5 kB 2.3 MB/s eta 0:00:00\n",
      "Step #0: Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.5/24.5 MB 3.4 MB/s eta 0:00:00\n",
      "Step #0: Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 3.6 MB/s eta 0:00:00\n",
      "Step #0: Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 65.5/65.5 kB 2.8 MB/s eta 0:00:00\n",
      "Step #0: Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 3.5 MB/s eta 0:00:00\n",
      "Step #0: Downloading tensorflow_io_gcs_filesystem-0.36.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 3.2 MB/s eta 0:00:00\n",
      "Step #0: Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 4.9 MB/s eta 0:00:00\n",
      "Step #0: Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.4/105.4 kB 3.9 MB/s eta 0:00:00\n",
      "Step #0: Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.6/6.6 MB 5.4 MB/s eta 0:00:00\n",
      "Step #0: Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Step #0:    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 4.7 MB/s eta 0:00:00\n",
      "Step #0: Building wheels for collected packages: fire\n",
      "Step #0:   Building wheel for fire (setup.py): started\n",
      "Step #0:   Building wheel for fire (setup.py): finished with status 'done'\n",
      "Step #0:   Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117031 sha256=ebebd035c538a1870300341fe25e020e4d417dc0b582c93bba73225736fae47c\n",
      "Step #0:   Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
      "Step #0: Successfully built fire\n",
      "Step #0: Installing collected packages: sentencepiece, namex, mpmath, libclang, flatbuffers, werkzeug, triton, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, sympy, safetensors, regex, opt-einsum, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ml-dtypes, markdown, h5py, google-pasta, gast, astunparse, tensorboard, nvidia-cusparse-cu12, nvidia-cudnn-cu12, kagglehub, huggingface-hub, fire, tokenizers, nvidia-cusolver-cu12, keras-core, keras, google-api-core, transformers, torch, tensorflow, tensorflow-text, google-cloud-storage, accelerate, keras-nlp\n",
      "Step #0:   Attempting uninstall: google-api-core\n",
      "Step #0:     Found existing installation: google-api-core 1.34.1\n",
      "Step #0:     Uninstalling google-api-core-1.34.1:\n",
      "Step #0:       Successfully uninstalled google-api-core-1.34.1\n",
      "Step #0:   Attempting uninstall: google-cloud-storage\n",
      "Step #0:     Found existing installation: google-cloud-storage 2.14.0\n",
      "Step #0:     Uninstalling google-cloud-storage-2.14.0:\n",
      "Step #0:       Successfully uninstalled google-cloud-storage-2.14.0\n",
      "Step #0: \u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "Step #0: google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.18.0 which is incompatible.\n",
      "Step #0: \u001b[0mSuccessfully installed accelerate-0.28.0 astunparse-1.6.3 fire-0.6.0 flatbuffers-24.3.7 gast-0.5.4 google-api-core-2.18.0 google-cloud-storage-2.16.0 google-pasta-0.2.0 h5py-3.10.0 huggingface-hub-0.21.4 kagglehub-0.2.1 keras-3.0.5 keras-core-0.1.7 keras-nlp-0.8.2 libclang-18.1.1 markdown-3.6 ml-dtypes-0.3.2 mpmath-1.3.0 namex-0.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 opt-einsum-3.3.0 regex-2023.12.25 safetensors-0.4.2 sentencepiece-0.2.0 sympy-1.12 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-io-gcs-filesystem-0.36.0 tensorflow-text-2.16.1 termcolor-2.4.0 tokenizers-0.15.2 torch-2.2.1 transformers-4.38.0 triton-2.2.0 werkzeug-3.0.1\n",
      "Step #0: \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0: \u001b[0mRemoving intermediate container 9a40cc26bbd9\n",
      "Step #0:  ---> 405609c245ba\n",
      "Step #0: Step 5/12 : ARG KAGGLE_USERNAME\n",
      "Step #0:  ---> Running in b6296f383569\n",
      "Step #0: Removing intermediate container b6296f383569\n",
      "Step #0:  ---> c031f73e36c8\n",
      "Step #0: Step 6/12 : ENV KAGGLE_USERNAME=$KAGGLE_USERNAME\n",
      "Step #0:  ---> Running in d9f60db3d10c\n",
      "Step #0: Removing intermediate container d9f60db3d10c\n",
      "Step #0:  ---> 7a5cbf15ee06\n",
      "Step #0: Step 7/12 : ARG KAGGLE_KEY\n",
      "Step #0:  ---> Running in e4b5bd186ceb\n",
      "Step #0: Removing intermediate container e4b5bd186ceb\n",
      "Step #0:  ---> f3d3b005384b\n",
      "Step #0: Step 8/12 : ENV KAGGLE_KEY=$KAGGLE_KEY\n",
      "Step #0:  ---> Running in cb56266fceb1\n",
      "Step #0: Removing intermediate container cb56266fceb1\n",
      "Step #0:  ---> 2386acf953fc\n",
      "Step #0: Step 9/12 : COPY . /trainer\n",
      "Step #0:  ---> f155532fade6\n",
      "Step #0: Step 10/12 : WORKDIR /trainer\n",
      "Step #0:  ---> Running in f873b908469c\n",
      "Step #0: Removing intermediate container f873b908469c\n",
      "Step #0:  ---> a2f0dedee877\n",
      "Step #0: Step 11/12 : RUN ls\n",
      "Step #0:  ---> Running in 67630e137c03\n",
      "Step #0: Dockerfile\n",
      "Step #0: Untitled.ipynb\n",
      "Step #0: __pycache__\n",
      "Step #0: cloudbuild.yaml\n",
      "Step #0: conversion_function.py\n",
      "Step #0: export_gemma_to_hf.py\n",
      "Step #0: requirements.txt\n",
      "Step #0: trainer.py\n",
      "Step #0: util.py\n",
      "Step #0: Removing intermediate container 67630e137c03\n",
      "Step #0:  ---> 86440068aa97\n",
      "Step #0: Step 12/12 : ENTRYPOINT [\"python\"]\n",
      "Step #0:  ---> Running in 46148a98162b\n",
      "Step #0: Removing intermediate container 46148a98162b\n",
      "Step #0:  ---> 0b577cfa7bf8\n",
      "Step #0: Successfully built 0b577cfa7bf8\n",
      "Step #0: Successfully tagged gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1: The push refers to repository [gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning]\n",
      "Step #1: 5c1571a296d6: Preparing\n",
      "Step #1: 7819410bcce0: Preparing\n",
      "Step #1: 58721aae01a9: Preparing\n",
      "Step #1: 56b388f91e39: Preparing\n",
      "Step #1: 9deb6fc54da9: Preparing\n",
      "Step #1: 6931085b550f: Preparing\n",
      "Step #1: c230b525aedf: Preparing\n",
      "Step #1: 5f70bf18a086: Preparing\n",
      "Step #1: c57ef954d51d: Preparing\n",
      "Step #1: b831d1fa39bf: Preparing\n",
      "Step #1: 4520767ffc08: Preparing\n",
      "Step #1: 09d837e2554d: Preparing\n",
      "Step #1: 6e3d09f63d7a: Preparing\n",
      "Step #1: d3135376200a: Preparing\n",
      "Step #1: e76fa06cba23: Preparing\n",
      "Step #1: 46b83560dec5: Preparing\n",
      "Step #1: 51029eb3efd6: Preparing\n",
      "Step #1: b32920786550: Preparing\n",
      "Step #1: 0430b0b45ba7: Preparing\n",
      "Step #1: cd26331ad5b3: Preparing\n",
      "Step #1: 8e74dfc7859c: Preparing\n",
      "Step #1: b81e99c9fcc3: Preparing\n",
      "Step #1: 4e3c3f15a9b6: Preparing\n",
      "Step #1: 457986848246: Preparing\n",
      "Step #1: cbf3c905e2e1: Preparing\n",
      "Step #1: da817efd0bfb: Preparing\n",
      "Step #1: 2f41ef0e83a0: Preparing\n",
      "Step #1: 52248bbcc0bc: Preparing\n",
      "Step #1: 5f70bf18a086: Preparing\n",
      "Step #1: 537816d7f4e1: Preparing\n",
      "Step #1: f158d4b2b4b4: Preparing\n",
      "Step #1: e7bf000641e2: Preparing\n",
      "Step #1: d7d705e1decf: Preparing\n",
      "Step #1: 2227317d988c: Preparing\n",
      "Step #1: 50bceba2b2b7: Preparing\n",
      "Step #1: 40fc5e6cc198: Preparing\n",
      "Step #1: 889402d51413: Preparing\n",
      "Step #1: 284c466ee6ce: Preparing\n",
      "Step #1: 1ff8f721b9db: Preparing\n",
      "Step #1: 1eeecbd4dbae: Preparing\n",
      "Step #1: 35d40f4df845: Preparing\n",
      "Step #1: 2651516ff8de: Preparing\n",
      "Step #1: 6c3e7df31590: Preparing\n",
      "Step #1: 0430b0b45ba7: Waiting\n",
      "Step #1: cd26331ad5b3: Waiting\n",
      "Step #1: 8e74dfc7859c: Waiting\n",
      "Step #1: b81e99c9fcc3: Waiting\n",
      "Step #1: 4e3c3f15a9b6: Waiting\n",
      "Step #1: 457986848246: Waiting\n",
      "Step #1: cbf3c905e2e1: Waiting\n",
      "Step #1: da817efd0bfb: Waiting\n",
      "Step #1: 52248bbcc0bc: Waiting\n",
      "Step #1: 537816d7f4e1: Waiting\n",
      "Step #1: f158d4b2b4b4: Waiting\n",
      "Step #1: e7bf000641e2: Waiting\n",
      "Step #1: d7d705e1decf: Waiting\n",
      "Step #1: 2227317d988c: Waiting\n",
      "Step #1: 50bceba2b2b7: Waiting\n",
      "Step #1: 40fc5e6cc198: Waiting\n",
      "Step #1: 889402d51413: Waiting\n",
      "Step #1: 284c466ee6ce: Waiting\n",
      "Step #1: 1ff8f721b9db: Waiting\n",
      "Step #1: 1eeecbd4dbae: Waiting\n",
      "Step #1: 35d40f4df845: Waiting\n",
      "Step #1: 6931085b550f: Waiting\n",
      "Step #1: c230b525aedf: Waiting\n",
      "Step #1: 5f70bf18a086: Waiting\n",
      "Step #1: c57ef954d51d: Waiting\n",
      "Step #1: b831d1fa39bf: Waiting\n",
      "Step #1: 4520767ffc08: Waiting\n",
      "Step #1: 09d837e2554d: Waiting\n",
      "Step #1: 6e3d09f63d7a: Waiting\n",
      "Step #1: d3135376200a: Waiting\n",
      "Step #1: e76fa06cba23: Waiting\n",
      "Step #1: 46b83560dec5: Waiting\n",
      "Step #1: 51029eb3efd6: Waiting\n",
      "Step #1: b32920786550: Waiting\n",
      "Step #1: 2651516ff8de: Waiting\n",
      "Step #1: 6c3e7df31590: Waiting\n",
      "Step #1: 2f41ef0e83a0: Waiting\n",
      "Step #1: 9deb6fc54da9: Layer already exists\n",
      "Step #1: 6931085b550f: Layer already exists\n",
      "Step #1: c230b525aedf: Layer already exists\n",
      "Step #1: 5f70bf18a086: Layer already exists\n",
      "Step #1: c57ef954d51d: Layer already exists\n",
      "Step #1: b831d1fa39bf: Layer already exists\n",
      "Step #1: 5c1571a296d6: Pushed\n",
      "Step #1: 4520767ffc08: Layer already exists\n",
      "Step #1: 56b388f91e39: Pushed\n",
      "Step #1: 58721aae01a9: Pushed\n",
      "Step #1: 09d837e2554d: Layer already exists\n",
      "Step #1: 6e3d09f63d7a: Layer already exists\n",
      "Step #1: d3135376200a: Layer already exists\n",
      "Step #1: e76fa06cba23: Layer already exists\n",
      "Step #1: 46b83560dec5: Layer already exists\n",
      "Step #1: 0430b0b45ba7: Layer already exists\n",
      "Step #1: 51029eb3efd6: Layer already exists\n",
      "Step #1: cd26331ad5b3: Layer already exists\n",
      "Step #1: 8e74dfc7859c: Layer already exists\n",
      "Step #1: b81e99c9fcc3: Layer already exists\n",
      "Step #1: 4e3c3f15a9b6: Layer already exists\n",
      "Step #1: b32920786550: Layer already exists\n",
      "Step #1: 457986848246: Layer already exists\n",
      "Step #1: cbf3c905e2e1: Layer already exists\n",
      "Step #1: da817efd0bfb: Layer already exists\n",
      "Step #1: 2f41ef0e83a0: Layer already exists\n",
      "Step #1: 52248bbcc0bc: Layer already exists\n",
      "Step #1: 537816d7f4e1: Layer already exists\n",
      "Step #1: f158d4b2b4b4: Layer already exists\n",
      "Step #1: e7bf000641e2: Layer already exists\n",
      "Step #1: d7d705e1decf: Layer already exists\n",
      "Step #1: 2227317d988c: Layer already exists\n",
      "Step #1: 50bceba2b2b7: Layer already exists\n",
      "Step #1: 40fc5e6cc198: Layer already exists\n",
      "Step #1: 284c466ee6ce: Layer already exists\n",
      "Step #1: 889402d51413: Layer already exists\n",
      "Step #1: 1eeecbd4dbae: Layer already exists\n",
      "Step #1: 1ff8f721b9db: Layer already exists\n",
      "Step #1: 2651516ff8de: Layer already exists\n",
      "Step #1: 35d40f4df845: Layer already exists\n",
      "Step #1: 6c3e7df31590: Layer already exists\n",
      "Step #1: 7819410bcce0: Pushed\n",
      "Step #1: latest: digest: sha256:c0f4b0f3df27040c1c8088adf1e9322dfcad7190d4e15c9ddbdc34ca72768d24 size: 9326\n",
      "Finished Step #1\n",
      "PUSH\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                             IMAGES  STATUS\n",
      "755a0621-803c-4629-a651-35cb98e212fe  2024-03-21T22:26:35+00:00  18M40S    gs://able-analyst-416817_cloudbuild/source/1711059994.229577-44e2712079f546a1bacb1abef8eb53c3.tgz  -       SUCCESS\n"
     ]
    }
   ],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_KAGGLE_USERNAME={},\\\n",
    "_KAGGLE_KEY={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-fine-tunning\",\n",
    "           KAGGLE_USERNAME,\n",
    "           KAGGLE_KEY,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "\n",
    "# Builds image\n",
    "!gcloud builds submit . --config \"components/fine_tunning/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef60c94-45b0-4cf2-a7d1-cf96b3f6ad6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0452fcec-bd44-4ab8-801b-e6f804b5a432",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e961b567-3239-4c10-915f-67fd98515c95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Sender: FoooodddAndres Perez: Coming :)\n",
      "Sender: Can I maybe borrow your iron? Andres Perez: It's not my iron But yeah haha Or is it?\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "data = [\"Sender: FoooodddAndres Perez: Coming :)\", \"Sender: Can I maybe borrow your iron? Andres Perez: It\\'s not my iron But yeah haha Or is it?\"]\n",
    "model_paths = \"\"\"{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"}\"\"\"\n",
    "print(len(data))\n",
    "model_paths = json.dumps(model_paths)\n",
    "#!docker run gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest trainer.py --data {data} --model-paths {model_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "090fa27a-121a-4954-8ddc-973415226bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-20 23:02:44.449256: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 23:02:44.520889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "<class 'list'> ['[Sender: FoooodddAndres Perez: Coming :),', \"Sender: Can I maybe borrow your iron? Andres Perez: It's not my iron But yeah haha Or is it?]\"]\n",
      "<class 'str'> {'finetuned_model_dir': './gemma_2b_en', 'finetuned_weights_path': './gemma_2b_en/model.weights.h5'}\n",
      "False\n",
      "<class 'bool'>\n",
      "Print a few examples of the input data\n",
      "['[Sender: FoooodddAndres Perez: Coming :),']\n",
      "Fine tune Function section has ben called and started.\n",
      "2024-03-20 23:02:47.086699: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 20758 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "┃\u001b[1m \u001b[0m\u001b[1mTokenizer (type)                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                         \u001b[0m\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "│ gemma_tokenizer (\u001b[94mGemmaTokenizer\u001b[0m)                   │                          \n",
      "└────────────────────────────────────────────────────┴──────────────────────────\n",
      "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━\n",
      "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━\n",
      "│ padding_mask (\u001b[94mInputLayer\u001b[0m)     │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m)              │               \u001b[32m0\u001b[0m │ \n",
      "├───────────────────────────────┼───────────────────────────┼─────────────────┼─\n",
      "│ token_ids (\u001b[94mInputLayer\u001b[0m)        │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m)              │               \u001b[32m0\u001b[0m │ \n",
      "├───────────────────────────────┼───────────────────────────┼─────────────────┼─\n",
      "│ gemma_backbone                │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m2048\u001b[0m)        │   \u001b[32m2,506,172,416\u001b[0m │ \n",
      "│ (\u001b[94mGemmaBackbone\u001b[0m)               │                           │                 │ \n",
      "├───────────────────────────────┼───────────────────────────┼─────────────────┼─\n",
      "│ token_embedding               │ (\u001b[96mNone\u001b[0m, \u001b[96mNone\u001b[0m, \u001b[32m256000\u001b[0m)      │     \u001b[32m524,288,000\u001b[0m │ \n",
      "│ (\u001b[94mReversibleEmbedding\u001b[0m)         │                           │                 │ \n",
      "└───────────────────────────────┴───────────────────────────┴─────────────────┴─\n",
      "\u001b[1m Total params: \u001b[0m\u001b[32m2,506,172,416\u001b[0m (9.34 GB)\n",
      "\u001b[1m Trainable params: \u001b[0m\u001b[32m2,506,172,416\u001b[0m (9.34 GB)\n",
      "\u001b[1m Non-trainable params: \u001b[0m\u001b[32m0\u001b[0m (0.00 B)\n",
      "The model is not fine tuned due to google cloud lacking support\n",
      "Saving the weights  ./gemma_2b_en/model.weights.h5\n",
      "Saving model in  ./gemma_2b_en\n"
     ]
    }
   ],
   "source": [
    "!python ./components/fine_tunning/trainer.py --data {data} --model-paths {model_paths}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eb2aa69-a793-4a4f-b8e4-92731c92b6ac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2b'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_paths_and_config['huggingface_model_dir']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d84c44-c65c-4ae1-9a4c-6d293f139495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-03-20 23:46:08.056820: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-03-20 23:46:08.124720: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\n",
      "-> Loading Keras weights from file `./gemma_2b_en/model.weights.h5`...\n",
      "2024-03-20 23:46:24.031167: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2024-03-20 23:46:24.032462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9838 MB memory:  -> device: 0, name: NVIDIA L4, pci bus id: 0000:00:03.0, compute capability: 8.9\n",
      "normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.\n",
      "\n",
      "-> Loading HuggingFace Gemma `2B` model...\n",
      "\n",
      "✅ Model loading complete.\n",
      "\n",
      "-> Converting weights from KerasNLP Gemma to HuggingFace Gemma...\n",
      "\n",
      "✅ Weights converted successfully.\n",
      "\n",
      "-> Saving HuggingFace model to `./gemma_2b_en_huggingface`...\n",
      "\n",
      "✅ Saving complete. Model saved at `./gemma_2b_en_huggingface`.\n",
      "\n",
      "-> Saving HuggingFace Gemma tokenizer to `./gemma_2b_en_huggingface`...\n",
      "\n",
      "✅ Saving complete. Tokenizer saved at `./gemma_2b_en_huggingface`.\n"
     ]
    }
   ],
   "source": [
    "!python ./components/fine_tunning/conversion_function.py --weights-file {model_paths_and_config['finetuned_weights_path']} --size {model_paths_and_config['model_size']} --vocab-path {model_paths_and_config['finetuned_vocab_path']} --output-dir {model_paths_and_config['huggingface_model_dir']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f6c134-bd9a-4fda-8e29-31ef6c8aea4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = convert_checkpoints(\n",
    "    weights_file=model_paths_and_config['finetuned_weights_path'],\n",
    "    size=model_paths_and_config['model_size'],\n",
    "    output_dir=model_paths_and_config['huggingface_model_dir'],\n",
    "    vocab_path=model_paths_and_config['finetuned_vocab_path'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "e6404c6c-d998-4253-bbf3-3f83b29abe94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-experimental,TAG_NAME=latest\n",
      "3\n",
      "experimental.py <class 'str'>\n",
      "\"[\\\"Sender: FoooodddAndres Perez: Coming :)\\\", \\\"Sender: Can I maybe borrow your iron? Andres Perez: It's not my iron But yeah haha Or is it?\\\"]\" <class 'str'>\n",
      "{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"} <class 'str'>\n",
      "This is type param1 <class 'str'>\n",
      "<class 'dict'>\n",
      "<class 'str'>\n",
      "<class 'str'>\n",
      "Jalo todo bien\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-experimental\",\n",
    "           TAG_NAME,\n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "data = '\"[\\\\\"Sender: FoooodddAndres Perez: Coming :)\\\\\", \\\\\"Sender: Can I maybe borrow your iron? Andres Perez: It\\'s not my iron But yeah haha Or is it?\\\\\"]\"'\n",
    "model_paths = \"\"\"{\"finetuned_model_dir\": \"./gemma_2b_en\", \"finetuned_weights_path\": \"./gemma_2b_en/model.weights.h5\"}\"\"\"\n",
    "data_json = json.dumps(data)\n",
    "model_paths_json = json.dumps(model_paths)\n",
    "#!gcloud builds submit . --timeout=15m --config \"components/experimental/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "!docker run gcr.io/able-analyst-416817/gemma-chatbot-experimental:latest experimental.py {data_json} {model_paths_json}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bff6c518-79e1-4ec6-a832-39ae6f364ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-fine-tunning,_GCP_REGION=us-central1,TAG_NAME=latest\n"
     ]
    }
   ],
   "source": [
    "#Re-runs the image to restart the website, service account might be needed with this one.  (Development, when tested should be moved to the main cloudbuild in the project folder)\n",
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_GCP_REGION={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-running-app\",\n",
    "           GCP_REGION,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "!gcloud builds submit . --timeout=15m --config \"components/app_flask/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "#!gcloud builds submit . --timeout=15m --config \"cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e1cae7-c300-4cd1-aa97-c7fef2efaa5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b05aa3f-9a40-41db-8553-b65d7dee4352",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_CONTAINER_IMAGE_NAME=gemma-chatbot-running-app,_GCP_REGION=us-central1,TAG_NAME=latest\n",
      "Creating temporary tarball archive of 87 file(s) totalling 1.9 MiB before compression.\n",
      "Uploading tarball of [.] to [gs://able-analyst-416817_cloudbuild/source/1711107490.154039-879fa64159f04fe781ec80ab4e260da6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/able-analyst-416817/locations/us-central1/builds/8ddcea06-cf63-4b9a-b3d5-fc709bf40d77].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds;region=us-central1/8ddcea06-cf63-4b9a-b3d5-fc709bf40d77?project=24796876098 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"8ddcea06-cf63-4b9a-b3d5-fc709bf40d77\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://able-analyst-416817_cloudbuild/source/1711107490.154039-879fa64159f04fe781ec80ab4e260da6.tgz#1711107490818934\n",
      "Copying gs://able-analyst-416817_cloudbuild/source/1711107490.154039-879fa64159f04fe781ec80ab4e260da6.tgz#1711107490818934...\n",
      "/ [1 files][720.9 KiB/720.9 KiB]                                                \n",
      "Operation completed over 1 objects/720.9 KiB.\n",
      "BUILD\n",
      "Starting Step #0\n",
      "Step #0: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0: Sending build context to Docker daemon  369.2kB\n",
      "Step #0: Step 1/13 : FROM python:3.9-slim\n",
      "Step #0: 3.9-slim: Pulling from library/python\n",
      "Step #0: 8a1e25ce7c4f: Pulling fs layer\n",
      "Step #0: 1103112ebfc4: Pulling fs layer\n",
      "Step #0: 6e52db3290c0: Pulling fs layer\n",
      "Step #0: 937bce5dbc70: Pulling fs layer\n",
      "Step #0: 05e63546fee1: Pulling fs layer\n",
      "Step #0: 937bce5dbc70: Waiting\n",
      "Step #0: 05e63546fee1: Waiting\n",
      "Step #0: 1103112ebfc4: Verifying Checksum\n",
      "Step #0: 1103112ebfc4: Download complete\n",
      "Step #0: 937bce5dbc70: Verifying Checksum\n",
      "Step #0: 937bce5dbc70: Download complete\n",
      "Step #0: 6e52db3290c0: Verifying Checksum\n",
      "Step #0: 6e52db3290c0: Download complete\n",
      "Step #0: 8a1e25ce7c4f: Verifying Checksum\n",
      "Step #0: 8a1e25ce7c4f: Download complete\n",
      "Step #0: 05e63546fee1: Verifying Checksum\n",
      "Step #0: 05e63546fee1: Download complete\n",
      "Step #0: 8a1e25ce7c4f: Pull complete\n",
      "Step #0: 1103112ebfc4: Pull complete\n",
      "Step #0: 6e52db3290c0: Pull complete\n",
      "Step #0: 937bce5dbc70: Pull complete\n",
      "Step #0: 05e63546fee1: Pull complete\n",
      "Step #0: Digest: sha256:df78d66895cd3b12ec57c451f9776192a535688e27ab8a72c03896b17dbb4b98\n",
      "Step #0: Status: Downloaded newer image for python:3.9-slim\n",
      "Step #0:  ---> 500c1b793e9d\n",
      "Step #0: Step 2/13 : WORKDIR /root\n",
      "Step #0:  ---> Running in 75388cfa4a75\n",
      "Step #0: Removing intermediate container 75388cfa4a75\n",
      "Step #0:  ---> 79198c27e776\n",
      "Step #0: Step 3/13 : RUN pwd\n",
      "Step #0:  ---> Running in e0ba019d8113\n",
      "Step #0: /root\n",
      "Step #0: Removing intermediate container e0ba019d8113\n",
      "Step #0:  ---> 75ae58de2f9a\n",
      "Step #0: Step 4/13 : RUN ls\n",
      "Step #0:  ---> Running in 995d953d973f\n",
      "Step #0: Removing intermediate container 995d953d973f\n",
      "Step #0:  ---> d3290ac963c1\n",
      "Step #0: Step 5/13 : COPY requirements.txt .\n",
      "Step #0:  ---> dd804cf124fb\n",
      "Step #0: Step 6/13 : RUN pip install -U -r requirements.txt\n",
      "Step #0:  ---> Running in 02fb799195a4\n",
      "Step #0: Collecting flask\n",
      "Step #0:   Downloading flask-3.0.2-py3-none-any.whl (101 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 101.3/101.3 kB 1.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-aiplatform\n",
      "Step #0:   Downloading google_cloud_aiplatform-1.44.0-py2.py3-none-any.whl (4.2 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.2/4.2 MB 21.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting protobuf\n",
      "Step #0:   Downloading protobuf-5.26.0-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 302.8/302.8 kB 36.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting click>=8.1.3\n",
      "Step #0:   Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 97.9/97.9 kB 14.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting Jinja2>=3.1.2\n",
      "Step #0:   Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.2/133.2 kB 22.5 MB/s eta 0:00:00\n",
      "Step #0: Collecting Werkzeug>=3.0.0\n",
      "Step #0:   Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 226.7/226.7 kB 32.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting blinker>=1.6.2\n",
      "Step #0:   Downloading blinker-1.7.0-py3-none-any.whl (13 kB)\n",
      "Step #0: Collecting itsdangerous>=2.1.2\n",
      "Step #0:   Downloading itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Step #0: Collecting importlib-metadata>=3.6.0\n",
      "Step #0:   Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\n",
      "Step #0: Collecting google-cloud-storage<3.0.0dev,>=1.32.0\n",
      "Step #0:   Downloading google_cloud_storage-2.16.0-py2.py3-none-any.whl (125 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 125.6/125.6 kB 20.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting proto-plus<2.0.0dev,>=1.22.0\n",
      "Step #0:   Downloading proto_plus-1.23.0-py3-none-any.whl (48 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.8/48.8 kB 7.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-auth<3.0.0dev,>=2.14.1\n",
      "Step #0:   Downloading google_auth-2.29.0-py2.py3-none-any.whl (189 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 189.2/189.2 kB 28.7 MB/s eta 0:00:00\n",
      "Step #0: Collecting packaging>=14.3\n",
      "Step #0:   Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 9.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "Step #0:   Downloading google_cloud_resource_manager-1.12.3-py2.py3-none-any.whl (333 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 333.7/333.7 kB 39.2 MB/s eta 0:00:00\n",
      "Step #0: Collecting protobuf\n",
      "Step #0:   Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 294.6/294.6 kB 33.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting shapely<3.0.0dev\n",
      "Step #0:   Downloading shapely-2.0.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.5/2.5 MB 76.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-bigquery<4.0.0dev,>=1.15.0\n",
      "Step #0:   Downloading google_cloud_bigquery-3.19.0-py2.py3-none-any.whl (232 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 232.6/232.6 kB 32.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1\n",
      "Step #0:   Downloading google_api_core-2.18.0-py3-none-any.whl (138 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 138.3/138.3 kB 22.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting requests<3.0.0.dev0,>=2.18.0\n",
      "Step #0:   Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 10.4 MB/s eta 0:00:00\n",
      "Step #0: Collecting googleapis-common-protos<2.0.dev0,>=1.56.2\n",
      "Step #0:   Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.1/229.1 kB 29.3 MB/s eta 0:00:00\n",
      "Step #0: Collecting grpcio-status<2.0.dev0,>=1.33.2\n",
      "Step #0:   Downloading grpcio_status-1.62.1-py3-none-any.whl (14 kB)\n",
      "Step #0: Collecting grpcio<2.0dev,>=1.33.2\n",
      "Step #0:   Downloading grpcio-1.62.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.6/5.6 MB 52.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyasn1-modules>=0.2.1\n",
      "Step #0:   Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.3/181.3 kB 26.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting cachetools<6.0,>=2.0.0\n",
      "Step #0:   Downloading cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Step #0: Collecting rsa<5,>=3.1.4\n",
      "Step #0:   Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Step #0: Collecting google-resumable-media<3.0dev,>=0.6.0\n",
      "Step #0:   Downloading google_resumable_media-2.7.0-py2.py3-none-any.whl (80 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 80.6/80.6 kB 12.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting python-dateutil<3.0dev,>=2.7.2\n",
      "Step #0:   Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 229.9/229.9 kB 31.6 MB/s eta 0:00:00\n",
      "Step #0: Collecting google-cloud-core<3.0.0dev,>=1.6.0\n",
      "Step #0:   Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Step #0: Collecting grpc-google-iam-v1<1.0.0dev,>=0.12.4\n",
      "Step #0:   Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Step #0: Collecting google-crc32c<2.0dev,>=1.0\n",
      "Step #0:   Downloading google_crc32c-1.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Step #0: Collecting zipp>=0.5\n",
      "Step #0:   Downloading zipp-3.18.1-py3-none-any.whl (8.2 kB)\n",
      "Step #0: Collecting MarkupSafe>=2.0\n",
      "Step #0:   Downloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Step #0: Collecting numpy<2,>=1.14\n",
      "Step #0:   Downloading numpy-1.26.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.2/18.2 MB 42.9 MB/s eta 0:00:00\n",
      "Step #0: Collecting pyasn1<0.6.0,>=0.4.6\n",
      "Step #0:   Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 84.9/84.9 kB 12.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting six>=1.5\n",
      "Step #0:   Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Step #0: Collecting urllib3<3,>=1.21.1\n",
      "Step #0:   Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 21.8 MB/s eta 0:00:00\n",
      "Step #0: Collecting certifi>=2017.4.17\n",
      "Step #0:   Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.8/163.8 kB 23.1 MB/s eta 0:00:00\n",
      "Step #0: Collecting charset-normalizer<4,>=2\n",
      "Step #0:   Downloading charset_normalizer-3.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 142.3/142.3 kB 20.0 MB/s eta 0:00:00\n",
      "Step #0: Collecting idna<4,>=2.5\n",
      "Step #0:   Downloading idna-3.6-py3-none-any.whl (61 kB)\n",
      "Step #0:      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.6/61.6 kB 9.5 MB/s eta 0:00:00\n",
      "Step #0: Installing collected packages: zipp, urllib3, six, pyasn1, protobuf, packaging, numpy, MarkupSafe, itsdangerous, idna, grpcio, google-crc32c, click, charset-normalizer, certifi, cachetools, blinker, Werkzeug, shapely, rsa, requests, python-dateutil, pyasn1-modules, proto-plus, Jinja2, importlib-metadata, googleapis-common-protos, google-resumable-media, grpcio-status, google-auth, flask, grpc-google-iam-v1, google-api-core, google-cloud-core, google-cloud-storage, google-cloud-resource-manager, google-cloud-bigquery, google-cloud-aiplatform\n",
      "Step #0: Successfully installed Jinja2-3.1.3 MarkupSafe-2.1.5 Werkzeug-3.0.1 blinker-1.7.0 cachetools-5.3.3 certifi-2024.2.2 charset-normalizer-3.3.2 click-8.1.7 flask-3.0.2 google-api-core-2.18.0 google-auth-2.29.0 google-cloud-aiplatform-1.44.0 google-cloud-bigquery-3.19.0 google-cloud-core-2.4.1 google-cloud-resource-manager-1.12.3 google-cloud-storage-2.16.0 google-crc32c-1.5.0 google-resumable-media-2.7.0 googleapis-common-protos-1.63.0 grpc-google-iam-v1-0.13.0 grpcio-1.62.1 grpcio-status-1.62.1 idna-3.6 importlib-metadata-7.1.0 itsdangerous-2.1.2 numpy-1.26.4 packaging-24.0 proto-plus-1.23.0 protobuf-4.25.3 pyasn1-0.5.1 pyasn1-modules-0.3.0 python-dateutil-2.9.0.post0 requests-2.31.0 rsa-4.9 shapely-2.0.3 six-1.16.0 urllib3-2.2.1 zipp-3.18.1\n",
      "Step #0: \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0: \u001b[0m\u001b[91m\n",
      "Step #0: [notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "Step #0: [notice] To update, run: pip install --upgrade pip\n",
      "Step #0: \u001b[0mRemoving intermediate container 02fb799195a4\n",
      "Step #0:  ---> a0bc9c7467bc\n",
      "Step #0: Step 7/13 : RUN ls\n",
      "Step #0:  ---> Running in 46da18ed2bbe\n",
      "Step #0: requirements.txt\n",
      "Step #0: Removing intermediate container 46da18ed2bbe\n",
      "Step #0:  ---> 506ee3c0a511\n",
      "Step #0: Step 8/13 : COPY . /app\n",
      "Step #0:  ---> 3e399f1c897f\n",
      "Step #0: Step 9/13 : WORKDIR /app\n",
      "Step #0:  ---> Running in 5851c3f47d22\n",
      "Step #0: Removing intermediate container 5851c3f47d22\n",
      "Step #0:  ---> 7e42497996f1\n",
      "Step #0: Step 10/13 : EXPOSE 5000\n",
      "Step #0:  ---> Running in 6171a71a0415\n",
      "Step #0: Removing intermediate container 6171a71a0415\n",
      "Step #0:  ---> 777f0f3c436c\n",
      "Step #0: Step 11/13 : RUN pwd\n",
      "Step #0:  ---> Running in 113ef861ad6a\n",
      "Step #0: /app\n",
      "Step #0: Removing intermediate container 113ef861ad6a\n",
      "Step #0:  ---> 5411c53b19e7\n",
      "Step #0: Step 12/13 : RUN ls\n",
      "Step #0:  ---> Running in b18e3518bab4\n",
      "Step #0: Dockerfile\n",
      "Step #0: app\n",
      "Step #0: cloudbuild.yaml\n",
      "Step #0: requirements.txt\n",
      "Step #0: Removing intermediate container b18e3518bab4\n",
      "Step #0:  ---> e8d3c284e9b3\n",
      "Step #0: Step 13/13 : CMD [\"python\", \"-m\", \"app.app\"]\n",
      "Step #0:  ---> Running in 7380b3e04b85\n",
      "Step #0: Removing intermediate container 7380b3e04b85\n",
      "Step #0:  ---> ad1e38d4b57d\n",
      "Step #0: Successfully built ad1e38d4b57d\n",
      "Step #0: Successfully tagged gcr.io/able-analyst-416817/gemma-chatbot-running-app:latest\n",
      "Finished Step #0\n",
      "Starting Step #1\n",
      "Step #1: Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1: The push refers to repository [gcr.io/able-analyst-416817/gemma-chatbot-running-app]\n",
      "Step #1: 53f9aeb76c64: Preparing\n",
      "Step #1: f887744ab2c5: Preparing\n",
      "Step #1: 62d1c6d5bd4e: Preparing\n",
      "Step #1: 5d78e4c3c132: Preparing\n",
      "Step #1: 9138b29cde77: Preparing\n",
      "Step #1: 4c8474755d1b: Preparing\n",
      "Step #1: c8f253aef560: Preparing\n",
      "Step #1: a483da8ab3e9: Preparing\n",
      "Step #1: 4c8474755d1b: Waiting\n",
      "Step #1: c8f253aef560: Waiting\n",
      "Step #1: a483da8ab3e9: Waiting\n",
      "Step #1: 9138b29cde77: Layer already exists\n",
      "Step #1: 5d78e4c3c132: Layer already exists\n",
      "Step #1: c8f253aef560: Layer already exists\n",
      "Step #1: a483da8ab3e9: Layer already exists\n",
      "Step #1: 62d1c6d5bd4e: Pushed\n",
      "Step #1: 53f9aeb76c64: Pushed\n",
      "Step #1: 4c8474755d1b: Layer already exists\n",
      "Step #1: f887744ab2c5: Pushed\n",
      "Step #1: latest: digest: sha256:577fe11b12f5f142575143e4b2ae05b631fb499e4f4814c4ac9dad9f0cddc4de size: 1999\n",
      "Finished Step #1\n",
      "Starting Step #2\n",
      "Step #2: Already have image (with digest): gcr.io/cloud-builders/gcloud\n",
      "Step #2: Deploying container to Cloud Run service [chattingwithandreh] in project [able-analyst-416817] region [us-central1]\n",
      "Step #2: Deploying new service...\n",
      "Step #2: Setting IAM Policy.........done\n",
      "Step #2: Creating Revision................................................................................................................................................................................................................................................................................................................................done\n",
      "Step #2: Routing traffic.....done\n",
      "Step #2: Done.\n",
      "Step #2: Service [chattingwithandreh] revision [chattingwithandreh-00001-bhg] has been deployed and is serving 100 percent of traffic.\n",
      "Step #2: Service URL: https://chattingwithandreh-gqf6v2rlha-uc.a.run.app\n",
      "Finished Step #2\n",
      "PUSH\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                             IMAGES  STATUS\n",
      "8ddcea06-cf63-4b9a-b3d5-fc709bf40d77  2024-03-22T11:38:10+00:00  1M36S     gs://able-analyst-416817_cloudbuild/source/1711107490.154039-879fa64159f04fe781ec80ab4e260da6.tgz  -       SUCCESS\n",
      "\u001b[1;31mERROR:\u001b[0m (gcloud.builds.submit) Unable to read file [cloudbuild.yaml]: [Errno 2] No such file or directory: 'cloudbuild.yaml'\n"
     ]
    }
   ],
   "source": [
    "SUBSTITUTIONS=\"\"\"\n",
    "_CONTAINER_IMAGE_NAME={},\\\n",
    "_GCP_REGION={},\\\n",
    "TAG_NAME={}\\\n",
    "\"\"\".format(\n",
    "           f\"{CONTAINER_IMAGE_NAME}-running-app\",\n",
    "           GCP_REGION,\n",
    "           TAG_NAME, \n",
    "           ).strip()\n",
    "print(SUBSTITUTIONS)\n",
    "!gcloud builds submit . --timeout=15m --config \"components/app_flask/cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}\n",
    "#!gcloud builds submit . --timeout=15m --config \"cloudbuild.yaml\" --substitutions {SUBSTITUTIONS} --region={GCP_REGION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb10f3c-b135-470b-a74b-0c825e0f0761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aa1979-e2f1-41ce-a3f4-bab8465866e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53b974a3-cab2-425e-bfc5-dcb01d45b5ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from config import Config\n",
    "from util import get_model_paths_and_config, upload2bs\n",
    "\n",
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "from google.cloud import compute_v1\n",
    "\n",
    "project = PROJECT_ID  # Replace with your project ID\n",
    "\n",
    "# Regions to consider\n",
    "regions = [\"us-central1\", \"europe-west4\", \"asia-east1\"]  \n",
    "\n",
    "client = compute_v1.AcceleratorTypesClient()\n",
    "\n",
    "for region in regions:\n",
    "    zone_client = compute_v1.ZonesClient()\n",
    "    all_zones = zone_client.list(project=project)\n",
    "\n",
    "    zone_list = [zone for zone in all_zones if zone.region == f\"regions/{region}\"]\n",
    "\n",
    "    for zone in zone_list:\n",
    "        result = client.describe(\n",
    "            project=project, zone=zone.name, accelerator_type=\"nvidia-l4\"\n",
    "        )\n",
    "        # If the result isn't an error, the 'nvidia-l4' type is available\n",
    "        GCP_REGION = region  # Update your region variable\n",
    "        break "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11402c19-d1b8-4b8c-a959-932ca09fcf18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:24:11.294 - INFO - Executing task \u001b[96m'add'\u001b[0m\n",
      "12:24:11.295 - INFO - Streamed logs:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/kfp/dsl/component_decorator.py:119: FutureWarning: Python 3.7 has reached end-of-life. The default base_image used by the @dsl.component decorator will switch from 'python:3.7' to 'python:3.8' on April 23, 2024. To ensure your existing components work with versions of the KFP SDK released after that date, you should provide an explicit base_image argument and ensure your component works as intended on Python 3.8.\n",
      "  return component_factory.create_component_from_func(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Pulling image 'python:3.7'\n",
      "    Image pull complete\n",
      "\n",
      "    WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "    [KFP Executor 2024-03-19 12:24:40,053 INFO]: Looking for component `add` in --component_module_path `/tmp/tmp.EOYBBcdz35/ephemeral_component.py`\n",
      "    [KFP Executor 2024-03-19 12:24:40,053 INFO]: Loading KFP component \"add\" from /tmp/tmp.EOYBBcdz35/ephemeral_component.py (directory \"/tmp/tmp.EOYBBcdz35\" and module name \"ephemeral_component\")\n",
      "    [KFP Executor 2024-03-19 12:24:40,054 INFO]: Got executor_input:\n",
      "    {\n",
      "        \"inputs\": {\n",
      "            \"parameterValues\": {\n",
      "                \"a\": 1,\n",
      "                \"b\": 2\n",
      "            }\n",
      "        },\n",
      "        \"outputs\": {\n",
      "            \"parameters\": {\n",
      "                \"Output\": {\n",
      "                    \"outputFile\": \"/home/jupyter/chatbot/local_outputs/add-2024-03-19-12-24-11-294086/add/Output\"\n",
      "                }\n",
      "            },\n",
      "            \"outputFile\": \"/home/jupyter/chatbot/local_outputs/add-2024-03-19-12-24-11-294086/add/executor_output.json\"\n",
      "        }\n",
      "    }\n",
      "    /usr/local/lib/python3.7/runpy.py:109: FutureWarning: Python 3.7 has reached end-of-life. KFP will drop support for Python 3.7 on April 23, 2024. To use new versions of the KFP SDK after that date, you will need to upgrade to Python >= 3.8. See https://devguide.python.org/versions/ for more details.\n",
      "      __import__(pkg_name)\n",
      "    [KFP Executor 2024-03-19 12:24:40,074 INFO]: Wrote executor output file to /home/jupyter/chatbot/local_outputs/add-2024-03-19-12-24-11-294086/add/executor_output.json.\n",
      "12:24:40.282 - INFO - Task \u001b[96m'add'\u001b[0m finished with status \u001b[92mSUCCESS\u001b[0m\n",
      "12:24:40.285 - INFO - Task \u001b[96m'add'\u001b[0m outputs:\n",
      "    Output: 3\n",
      "--------------------------------------------------------------------------------\n",
      "12:24:40.290 - INFO - Running pipeline: \u001b[95m'math-pipeline'\u001b[0m\n",
      "--------------------------------------------------------------------------------\n",
      "12:24:40.292 - INFO - Executing task \u001b[96m'add'\u001b[0m\n",
      "12:24:40.293 - INFO - Streamed logs:\n",
      "\n",
      "    Found image 'python:3.7'\n",
      "\n",
      "    WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "    [KFP Executor 2024-03-19 12:24:45,739 INFO]: Looking for component `add` in --component_module_path `/tmp/tmp.WTI6AJHdMW/ephemeral_component.py`\n",
      "    [KFP Executor 2024-03-19 12:24:45,739 INFO]: Loading KFP component \"add\" from /tmp/tmp.WTI6AJHdMW/ephemeral_component.py (directory \"/tmp/tmp.WTI6AJHdMW\" and module name \"ephemeral_component\")\n",
      "    [KFP Executor 2024-03-19 12:24:45,740 INFO]: Got executor_input:\n",
      "    {\n",
      "        \"inputs\": {\n",
      "            \"parameterValues\": {\n",
      "                \"b\": 2,\n",
      "                \"a\": 1\n",
      "            }\n",
      "        },\n",
      "        \"outputs\": {\n",
      "            \"parameters\": {\n",
      "                \"Output\": {\n",
      "                    \"outputFile\": \"/home/jupyter/chatbot/local_outputs/math-pipeline-2024-03-19-12-24-40-289852/add/Output\"\n",
      "                }\n",
      "            },\n",
      "            \"outputFile\": \"/home/jupyter/chatbot/local_outputs/math-pipeline-2024-03-19-12-24-40-289852/add/executor_output.json\"\n",
      "        }\n",
      "    }\n",
      "    [KFP Executor 2024-03-19 12:24:45,741 INFO]: Wrote executor output file to /home/jupyter/chatbot/local_outputs/math-pipeline-2024-03-19-12-24-40-289852/add/executor_output.json.\n",
      "    /usr/local/lib/python3.7/runpy.py:109: FutureWarning: Python 3.7 has reached end-of-life. KFP will drop support for Python 3.7 on April 23, 2024. To use new versions of the KFP SDK after that date, you will need to upgrade to Python >= 3.8. See https://devguide.python.org/versions/ for more details.\n",
      "      __import__(pkg_name)\n",
      "12:24:45.960 - INFO - Task \u001b[96m'add'\u001b[0m finished with status \u001b[92mSUCCESS\u001b[0m\n",
      "12:24:45.962 - INFO - Task \u001b[96m'add'\u001b[0m outputs:\n",
      "    Output: 3\n",
      "--------------------------------------------------------------------------------\n",
      "12:24:45.964 - INFO - Executing task \u001b[96m'add-2'\u001b[0m\n",
      "12:24:45.965 - INFO - Streamed logs:\n",
      "\n",
      "    Found image 'python:3.7'\n",
      "\n",
      "    WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "    [KFP Executor 2024-03-19 12:24:51,419 INFO]: Looking for component `add` in --component_module_path `/tmp/tmp.krxiNg32tG/ephemeral_component.py`\n",
      "    [KFP Executor 2024-03-19 12:24:51,419 INFO]: Loading KFP component \"add\" from /tmp/tmp.krxiNg32tG/ephemeral_component.py (directory \"/tmp/tmp.krxiNg32tG\" and module name \"ephemeral_component\")\n",
      "    [KFP Executor 2024-03-19 12:24:51,420 INFO]: Got executor_input:\n",
      "    {\n",
      "        \"inputs\": {\n",
      "            \"parameterValues\": {\n",
      "                \"a\": 3,\n",
      "                \"b\": 3\n",
      "            }\n",
      "        },\n",
      "        \"outputs\": {\n",
      "            \"parameters\": {\n",
      "                \"Output\": {\n",
      "                    \"outputFile\": \"/home/jupyter/chatbot/local_outputs/math-pipeline-2024-03-19-12-24-40-289852/add-2/Output\"\n",
      "                }\n",
      "            },\n",
      "            \"outputFile\": \"/home/jupyter/chatbot/local_outputs/math-pipeline-2024-03-19-12-24-40-289852/add-2/executor_output.json\"\n",
      "        }\n",
      "    }\n",
      "    [KFP Executor 2024-03-19 12:24:51,458 INFO]: Wrote executor output file to /home/jupyter/chatbot/local_outputs/math-pipeline-2024-03-19-12-24-40-289852/add-2/executor_output.json.\n",
      "    /usr/local/lib/python3.7/runpy.py:109: FutureWarning: Python 3.7 has reached end-of-life. KFP will drop support for Python 3.7 on April 23, 2024. To use new versions of the KFP SDK after that date, you will need to upgrade to Python >= 3.8. See https://devguide.python.org/versions/ for more details.\n",
      "      __import__(pkg_name)\n",
      "12:24:51.735 - INFO - Task \u001b[96m'add-2'\u001b[0m finished with status \u001b[92mSUCCESS\u001b[0m\n",
      "12:24:51.737 - INFO - Task \u001b[96m'add-2'\u001b[0m outputs:\n",
      "    Output: 6\n",
      "--------------------------------------------------------------------------------\n",
      "12:24:51.739 - INFO - Pipeline \u001b[95m'math-pipeline'\u001b[0m finished with status \u001b[92mSUCCESS\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from kfp import local\n",
    "from kfp import dsl\n",
    "\n",
    "local.init(runner=local.DockerRunner())\n",
    "\n",
    "@dsl.component\n",
    "def add(a: int, b: int) -> int:\n",
    "    return a + b\n",
    "\n",
    "# run a single component\n",
    "task = add(a=1, b=2)\n",
    "assert task.output == 3\n",
    "\n",
    "# or run it in a pipeline\n",
    "@dsl.pipeline\n",
    "def math_pipeline(x: int, y: int, z: int) -> int:\n",
    "    t1 = add(a=x, b=y)\n",
    "    t2 = add(a=t1.output, b=z)\n",
    "    return t2.output\n",
    "\n",
    "pipeline_task = math_pipeline(x=1, y=2, z=3)\n",
    "assert pipeline_task.output == 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cf7ac36-bc0e-4e00-928f-1c7247b9685e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./gemma_2b_en\n",
      "{'model_size': '2b', 'finetuned_model_dir': './gemma_2b_en', 'finetuned_weights_path': './gemma_2b_en/model.weights.h5', 'finetuned_vocab_path': './gemma_2b_en/vocabulary.spm', 'huggingface_model_dir': './gemma_2b_en_huggingface', 'deployed_model_blob': 'gemma_2b_en/20240321092813', 'deployed_model_uri': 'gs://able-analyst-416817-chatbot-v1/gemma_2b_en/20240321092813', 'model_name_vllm': 'gemma_2b_en-vllm', 'machine_type': 'g2-standard-8', 'accelerator_type': 'NVIDIA_L4', 'accelerator_count': 1}\n",
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/pythagorean-20240321092821?project=24796876098\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/24796876098/locations/us-central1/pipelineJobs/pythagorean-20240321092821 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [convert-checkpoints-op].; Job (project_id = able-analyst-416817, job_id = 7434508674782986240) is failed due to the above error.; Failed to handle the job: {project_number = 24796876098, job_id = 7434508674782986240}\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 136\u001b[0m\n\u001b[1;32m    131\u001b[0m vertex_pipelines_job \u001b[38;5;241m=\u001b[39m vertexai\u001b[38;5;241m.\u001b[39mpipeline_jobs\u001b[38;5;241m.\u001b[39mPipelineJob(\n\u001b[1;32m    132\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-whatsapp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    133\u001b[0m     template_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest-whatsapp.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    134\u001b[0m )\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m#vertex_pipelines_job.worker_pool_specs = worker_pool_specs \u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[43mvertex_pipelines_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:323\u001b[0m, in \u001b[0;36mPipelineJob.run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Run this configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    302\u001b[0m \n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[0;32m--> 323\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreserved_ip_ranges\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:850\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    849\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    853\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:366\u001b[0m, in \u001b[0;36mPipelineJob._run\u001b[0;34m(self, service_account, network, reserved_ip_ranges, sync, create_request_timeout)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to ensure network synchronization and to run\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03mthe configured PipelineJob and monitor the job until completion.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m        Optional. The timeout for the create request in seconds.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubmit(\n\u001b[1;32m    360\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[1;32m    361\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[1;32m    362\u001b[0m     reserved_ip_ranges\u001b[38;5;241m=\u001b[39mreserved_ip_ranges,\n\u001b[1;32m    363\u001b[0m     create_request_timeout\u001b[38;5;241m=\u001b[39mcreate_request_timeout,\n\u001b[1;32m    364\u001b[0m )\n\u001b[0;32m--> 366\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/google/cloud/aiplatform/pipeline_jobs.py:615\u001b[0m, in \u001b[0;36mPipelineJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;66;03m# Error is only populated when the job state is\u001b[39;00m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;66;03m# JOB_STATE_FAILED or JOB_STATE_CANCELLED.\u001b[39;00m\n\u001b[1;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m _PIPELINE_ERROR_STATES:\n\u001b[0;32m--> 615\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJob failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    617\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Job failed with:\ncode: 9\nmessage: \"The DAG failed because some tasks failed. The failed tasks are: [convert-checkpoints-op].; Job (project_id = able-analyst-416817, job_id = 7434508674782986240) is failed due to the above error.; Failed to handle the job: {project_number = 24796876098, job_id = 7434508674782986240}\"\n"
     ]
    }
   ],
   "source": [
    "from config import Config\n",
    "from util import get_model_paths_and_config, upload2bs\n",
    "\n",
    "res = !gcloud config get core/project\n",
    "PROJECT_ID = res[0]\n",
    "SERVICE_ACCOUNT = 'gemma-vertexai-chatbot@able-analyst-416817.iam.gserviceaccount.com'\n",
    "from datetime import datetime\n",
    "CONTAINER_IMAGE_NAME=\"gemma-chatbot\"\n",
    "GCP_REGION='us-central1'\n",
    "IMAGE_NAME=\"gemma-chatbot\"\n",
    "TAG_NAME = 'latest'\n",
    "KAGGLE_USERNAME='andrehpereh1'\n",
    "KAGGLE_KEY='5859e39806d9456749dcbac685f04bc9'\n",
    "model_paths_and_config = get_model_paths_and_config(Config.MODEL_NAME)\n",
    "print(model_paths_and_config)\n",
    "from google.cloud import aiplatform as vertexai\n",
    "from kfp import dsl\n",
    "from kfp.dsl import OutputPath, Artifact, InputPath\n",
    "from kfp import compiler\n",
    "# from google_cloud_pipeline_components.v1.custom_job import create_custom_training_job_from_component\n",
    "# from google_cloud_pipeline_components.v1.custom_job import CustomTrainingJobOp\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "  base_image ='gcr.io/able-analyst-416817/gemma-chatbot-data-preparation:latest'\n",
    ")\n",
    "def process_whatsapp_chat_op(\n",
    "  bucket_name: str,\n",
    "  directory: str,\n",
    "  dataset_path: OutputPath('Dataset')\n",
    "):\n",
    "    import data_ingestion\n",
    "    import json\n",
    "    formatted_messages = data_ingestion.process_whatsapp_chat(bucket_name, directory)\n",
    "    with open(dataset_path, 'w') as f:\n",
    "        json.dump(formatted_messages, f)\n",
    "\n",
    "\n",
    "@dsl.component(\n",
    "  base_image = 'gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest'\n",
    ")\n",
    "def fine_tunning(\n",
    "  dataset_path: InputPath('Dataset'),\n",
    "  model_paths: dict,\n",
    "  finetuned_weights_path: OutputPath('Model'),\n",
    "):\n",
    "    # import test_container\n",
    "    import trainer\n",
    "    import json\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        dataset = json.load(f)\n",
    "    # finetuned_weights_path = test_container.add_test(dataset, model_paths)\n",
    "    finetuned_weights_path = trainer.finetune_gemma(dataset, model_paths, False)\n",
    "    print(\"Si funciono este pedo\", finetuned_weights_path)\n",
    "    # return finetuned_weights_path\n",
    "\n",
    "@dsl.component(\n",
    "  base_image = 'gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest'\n",
    ")\n",
    "def convert_checkpoints_op(\n",
    "  finetuned_weights_path: InputPath('Model'),\n",
    "  size: str,\n",
    "  output_dir: str,\n",
    "  vocab_path: str,\n",
    "  converted_fined_tuned_path: OutputPath('Model')\n",
    "):\n",
    "    import conversion_function\n",
    "    converted_fined_tuned_path = conversion_function.convert_checkpoints(\n",
    "        weights_file=finetuned_weights_path,\n",
    "        size=size,\n",
    "        output_dir=output_dir,\n",
    "        vocab_path=vocab_path,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# TRAINER_ARGS = [\"trainer.finetune_gemma.py\"] + [ \"--data-dir\", dataset_path, \"--hptune-dict\", model_paths]\n",
    "# custom_job_task = CustomTrainingJobOp(\n",
    "#     display_name=\"model-training-CustomTrainingJobOp\",\n",
    "#     worker_pool_specs=[\n",
    "#         {\n",
    "#             \"containerSpec\": {\n",
    "#                 \"args\": \"trainer.finetune_gemma.py\",\n",
    "#                 \"imageUri\": \"gcr.io/able-analyst-416817/gemma-chatbot-fine-tunning:latest\",\n",
    "#             },\n",
    "#             \"replicaCount\": \"1\",\n",
    "#             \"machineSpec\": {\n",
    "#                 \"machineType\": \"n1-standard-16\",\n",
    "#                 \"accelerator_type\": aip.gapic.AcceleratorType.NVIDIA_TESLA_K80,\n",
    "#                 \"accelerator_count\": 2,\n",
    "#             },\n",
    "#         }\n",
    "#     ],\n",
    "# )\n",
    "\n",
    "\n",
    "# Convert the above component into a custom training job, if set_memory_limit and add are added in the pipeline, this is not needed.\n",
    "# custom_training_job_fine_tune = create_custom_training_job_from_component(\n",
    "#    fine_tunning,\n",
    "#    display_name = 'DISPLAY_NAME',\n",
    "#    machine_type = 'g2-standard-12',\n",
    "#    accelerator_type='NVIDIA_L4',\n",
    "#    accelerator_count=1,\n",
    "#    replica_count=1\n",
    "#)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dsl.pipeline\n",
    "def pythagorean(\n",
    "    bucket_name: str = \"able-analyst-416817-chatbot-v1\", directory: str = \"input_data/andrehpereh\", \n",
    "    model_paths: dict=model_paths_and_config\n",
    "):\n",
    "    whatup = process_whatsapp_chat_op(bucket_name = bucket_name, directory = directory)\n",
    "    # trainer = custom_training_job_fine_tune(dataset_path=whatup.outputs['dataset_path'], model_paths=model_paths)\n",
    "    # print(trainer.outputs['gcp_resources'])\n",
    "    trainer = fine_tunning(dataset_path=whatup.outputs['dataset_path'], model_paths=model_paths)\n",
    "    trainer.set_memory_limit(\"28G\").set_cpu_limit('4.0m') # .set_accelerator_limit(1).add_node_selector_constraint(\"NVIDIA_TESLA_T4\")\n",
    "    converted = convert_checkpoints_op(\n",
    "        finetuned_weights_path=trainer.outputs['finetuned_weights_path'], size=model_paths_and_config['model_size'],\n",
    "        output_dir=model_paths_and_config['huggingface_model_dir'],\n",
    "        vocab_path=model_paths_and_config['finetuned_vocab_path']\n",
    "    )\n",
    "converted.outputs['converted_fined_tuned_path']\n",
    "\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=pythagorean, package_path=\"test-whatsapp.json\"\n",
    ")\n",
    "vertexai.init(project=PROJECT_ID, location=\"us-central1\")\n",
    "vertex_pipelines_job = vertexai.pipeline_jobs.PipelineJob(\n",
    "    display_name=\"test-whatsapp\",\n",
    "    template_path=\"test-whatsapp.json\"\n",
    ")\n",
    "#vertex_pipelines_job.worker_pool_specs = worker_pool_specs \n",
    "vertex_pipelines_job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "09398956-e97e-411b-a1fe-9e05ff61e6d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafdfdea-c57b-4a26-ac5f-2775ff291bbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed69c43-8407-460b-bd2d-1ac0ed39a610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-15.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-gpu.2-15:m118"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
