Cloudbuild components, check the folder/subfolder
Allow cloudbuild access to everything, (cloud run) (artifact registry admin)
Enable AI Platform Training & Prediction API 
Enable Cloud Build API for triggering runs with Github.
Gran cloud permission to run vertex ai pipeline 


# 1. @dsl.container_component would is platform agnostic and could run any programming language (depends on the container image used)
# 2. Resource also can be specified within the pipeline directly and it is platform agnostic.


# 3. CustomTrainingJobOp and create_custom_training_job_from_component is vertex specific and one can specify resources and command like in container_component (1 and two combined). This deploys te model and retrieves metadata related to it.

# The pipeline should be adapted with the precise resources based on model_name "set_memory_limit("70G").set_cpu_limit("13.0")"

# We need to create the tables in Bigquery to store the users data.

# We need to create the bucket to store the users files.

# We need to create a PUB/SUB topic to get the messages.
# Give Pub/Sub admin to the service account.

#Default variables should be stored in a reachable location for all the components/containers (Blob storage). The models should be stored in the users folder.

# Check how to include model_name as a parameters instead of always declearing it as ENV 


gcloud builds triggers create github \
    --repo-name=<YOUR-REPO-NAME> \ 
    --repo-owner=<YOUR-REPO-OWNER> \
    --branch-pattern="main" \ 
    --build-config="cloudbuild.yaml" 



find . -type f \( -not -path '*/\.ipynb_checkpoints/*' -and -not -path '*/\__pycache__/*' \) -exec git add {} +



